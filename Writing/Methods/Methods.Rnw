<<set-parent, echo=FALSE, cache=FALSE>>=
knitr::set_parent(parent = "../Main/Main.Rnw")
@

\chapter{Methods}
\label{ch:methods}

Our aim is to detect hostile connections within a network with minimal false alarm occurrence through an unsupervised anomaly detection technique. As introduced in chapter [REF intro], the domain of unsupervised anomaly detection is, especially, attractive to the field of network intrusion detection as it allows the detection of novel threats. In order to secure a network, it is of the essence to be able to identify previously unknown threats before any harm can be caused. As with any forms of crime, new trends of cyber-crime are constantly emerging for a need to fortify networks and systems with the ability to cope with any possible novelty. This has given us a sounding reason to pursue the idea of implementing an unsupervised anomaly-based approach into our method.

As mentioned earlier in chapter [REF intro], the caveats of a typical anomaly-based network intrusion detection system (NIDS), in terms of the relatively low detection and high false alarm rates, are minimised in such a manner that each of those problems is addressed individually: (1) the anomaly detection technique in our method is executed in a generous way to yield improved detection, and (2) a clustering scheme is applied to identify the cluster of false positives which will be removed to reduce overall false alarm rates. The result of such disintegration of the problems is a two-phase NIDS shown in Figure \ref{fig:overview}.

\begin{figure}[h]
\centering
  \begin{tikzpicture}
    \node(input)  [simple]                                         {Input data};
    \node(phase1) [process, below of=input,   node distance=2.8cm] {Phase 1: \\ Anomaly detection};
    \node(anomaly)[simple,  right of=phase1,  node distance=3.8cm] {Anomalies};
    \node(phase2) [process, right of=anomaly, node distance=3.8cm] {Phase 2: \\  Clustering and trimming};
    \node(output) [simple,  above of=phase2,  node distance=2.8cm] {Anomalies \\ (false positives removed)};
    
    \draw [->] (input)   to (phase1);
    \draw [->] (phase1)  to (anomaly);
    \draw [->] (anomaly) to (phase2);
    \draw [->] (phase2)  to (output);
  \end{tikzpicture}
  \caption{System overview}
  \label{fig:overview}
\end{figure}

The rest of this chapter is divided into sections with in-depth discussion... (fix this)


%----------------------------------------------------------
\section{Anomaly Detection}

Supervised and semi-supervised anomaly detection techniques are not considered in our method as they require a labelled training dataset from which a classifier or a model is constructed. Since the desired outcome of this research is a NIDS with the novelty detection capability, any labels in a dataset are treated as non-existent and are only used for system evaluation.

From the unsupervised anomaly detection domain, the local outlier factor (LOF) algorithm is chosen for its advantages over others. The main advantage is that anomalous objects are detected by a local approach rather than global. Figure \ref{fig:lof-example} illustrates this point. Suppose a dataset with two clusters, one being extremely dense and the other being sparse. Distance-based nearest neighbour approaches such as the $k$-th Nearest Neighbour ($k$NN) will fail to detect the outlier, $p$, as the distance between any point in the sparse cluster to its nearest neighbours is always greater than the distance between $p$ and its nearest neighbours. Hence the distance involving $p$ can never be significantly greater than the other distances to be detected as an outlier.

<<lof-example, echo=FALSE, fig.align='center', fig.cap="Advantages of the LOF algorithm", out.width="0.8\\linewidth">>=
set.seed(3)
a = cbind(rnorm(30, mean = 10, sd = 4), rnorm(30, mean = 10, sd = 4))
set.seed(3)
b = cbind(rnorm(100, mean = 1, sd = 0.4), rnorm(100, mean = 1, sd = 0.4))

xmin = min(c(min(a[,1]), min(b[,1])))
xmax = max(c(max(a[,1]), max(b[,1])))
ymin = min(c(min(a[,2]), min(b[,2])))
ymax = max(c(max(a[,2]), max(b[,2])))

plot(a, xlim = c(xmin, xmax), ylim = c(ymin, ymax), xlab = "", ylab = "", xaxt = "n", yaxt = "n")
points(b)
points(x = max(b[,1])+0.3, y = max(b[,2])+0.2, pch = 4)
text(x = max(b[,1])+0.3, y = max(b[,2])+0.2, labels = "p", pos = 3, family = "sans")
@

The LOF algorithm overcomes this problem by comparing local densities given by the distance between objects and their nearest neighbours. The local density, or deviation, of an object is measured with respect to its neighbours so that sparse objects, relative to their neighbours, can be identified as outliers. The result of LOF is a score for every object in a dataset to indicate whether an object is an outlier. Because the algorithm implements the local approach, the LOF score of an object can not imply the degree of outlierness of the object. The scores are dependent on the local environment, or the neighbourhood, associated with individual objects. The difficulty in interpreting the result can be avoided by assigning the Local Outlier Probability (LoOP) for each object. The probability can provide reasonable and comparative indication of an object's outlierness due to its normalised nature but this is beyond the scope of our research as simple yet effective detection is what we are after.

The rest of this section is divided into subsections that discuss other anomaly detection approaches along with their limitations and our implementation of anomaly detection that corresponds to phase 1 of the proposed scheme.

\subsection{Other approaches}
\subsubsection{Statistical model-based approach}
A classical approach using statistical models is to compute the Mahalanobis distance of all points in a dataset, given by
\[ D_M(x) = \sqrt{(x-\mu)^{T}\Sigma^{-1}(x-\mu)}, \]
where $\mu$ and $\Sigma$ denote the vector of means and covariance matrix of the points, respectively. Under the multivariate normality assumption, the Mahalanobis distance follows a $\chi^{2}_{k}$-distribution, where $k$ is the degrees of freedom associated with the dataset. Hence a point whose $D_M$ is greater than the 97.5th percentile of the corresponding $\chi^{2}_{k}$ quantile function is a statistical outlier. The main problem with this approach is that it is extremely difficult to satisfy the multivariate normality assumption for any real life data. Another problem is that the mean of a set of points is extremely sensitive to outlier itself to result in a bias. Obtaining the inverse of a covariance matrix is also problematic when the matrix is singular, which could happen when there are duplicates in the data. Other possible problem to note is that the Mahalanobis distance metric is heavily affected by the curse of dimensionality.

\subsubsection{Neighbourhood-based approach}
As mentioned earlier, the simple distance-based nearest neighbour approaches such as the $k$NN suffers from varying densities in a dataset. There is no guarantee to suggest that the dataset is always dense, especially when it contains binary features whose components are mostly zeroes. (Check with Mazier which packet information can be binary).

\subsubsection{Clustering-based approach}
Unsupervised clustering techniques such as the PCA-based method... (more reading)

The $k$-means clustering is another popular technique that is efficient with reasonable performance. However its drawbacks include (1) using variance as a measure of cluster scatter which can be sensitive to outliers, (2) requirement of the input parameter, $k$, which can affect the result and (3) most notably the convergence to local minima to produce incorrect results. Some of these limitations can be prevented by using the $X$-means clustering but the fundamental problems associated with any distance-based clustering techniques, such as the adequacy of the distance metric, are still present.


\subsection{Phase 1 algorithm}
The aim of phase 1 is to detect as much anomalies as possible. The desired detection rate of phase 1 is essentially 100\%. Hence the anomaly detection is executed generously. This leads to more false positives being included in the result, which are to be trimmed out in phase 2.

The initial step of phase 1 is to apply the LOF algorithm on the numeric portion of a test dataset. Kernel density estimates are computed on the set of resulting LOF scores so that their corresponding probability distribution can be plotted. The kernel density estimation (KDE) is a non-parametric m

The set of resulting LOF scores are then plotted to show their density distribution. Kernel density estimation (KDE) is used 


<<phase1-example>>=

@


%----------------------------------------------------------
% \section{Clustering}
% \subsection{X-Means}


%----------------------------------------------------------
% \section{Cluster Identification}






