<<set-parent, echo=FALSE, cache=FALSE>>=
knitr::set_parent(parent = "../Main/Main.Rnw")
@

\chapter{Methods}
\label{ch:methods}

% Our aim is to detect hostile connections within a network with minimal false alarm occurrence through an unsupervised anomaly detection technique. As introduced in chapter [REF intro], the domain of unsupervised anomaly detection is, especially, attractive to the field of network intrusion detection as it allows the possibility of detecting novel threats. The most crucial aspect of securing a network or a system is in the ability to prevent additional losses from the attacks of previously known characteristics. What is of an equal, or possibly greater, importance is the ability to identify previously unknown threats before any harm is inflicted. As with any forms of crime, new trends of cyber-crime are constantly emerging for a need to fortify networks and systems with the intelligence to cope with any possible novelty. This has given us a sounding reason to pursue the idea of implementing an unsupervised approach into our method. 
% 
% %Supervised and semi-supervised anomaly detection techniques are not considered in our method as they require a labelled training dataset from which a classifier or a model is constructed. Since the desired outcome of this research is a network intrusion detection system (NIDS) with the novelty detection capability, any labels in a dataset are treated as non-existent and are only used for system evaluation.
% 
% As mentioned earlier in chapter [REF intro], the caveats of a typical anomaly-based network intrusion detection system (NIDS), in terms of the relatively low detection and high false alarm rates, are minimised in such a manner that each of those problems is addressed individually: (1) the anomaly detection technique in our method is executed in a generous way to maximise detection, and (2) a clustering scheme is applied to identify the cluster of false positives which will be eliminated to reduce overall false alarm rates. The result of such disintegration of the problems is a two-phase NIDS whose schematic overview is represented in figure \ref{fig:overview}.
% 
% \begin{figure}[h]
% \centering
%   \begin{tikzpicture}
%     \node(input)  [simple]                                         {Input data};
%     \node(phase1) [process, below of=input,   node distance=2.8cm] {Phase 1: \\ Anomaly detection};
%     \node(anomaly)[simple,  right of=phase1,  node distance=3.8cm] {Anomalies};
%     \node(phase2) [process, right of=anomaly, node distance=3.8cm] {Phase 2: \\  Clustering and trimming};
%     \node(output) [simple,  above of=phase2,  node distance=2.8cm] {Anomalies \\ (false positives removed)};
%     
%     \draw [->] (input)   to (phase1);
%     \draw [->] (phase1)  to (anomaly);
%     \draw [->] (anomaly) to (phase2);
%     \draw [->] (phase2)  to (output);
%   \end{tikzpicture}
%   \caption{System overview}
%   \label{fig:overview}
% \end{figure}
% 
% The rest of this chapter is divided into sections to bring detailed discussion for each phase.
% 
% 
% %----------------------------------------------------------
\section{Phase 1: Anomaly Detection}
(commented out)
% The main purpose of phase 1 is to detect all possible anomalies associated with a dataset, which suggests that the desired detection rate is essentially 1. To achieve such a high detection rate means that the resulting false alarm rate will be correspondingly high as the two rate measures are usually positively correlated. An attempt to attain a higher detection rate often involves changing a certain threshold, on which detection criteria are based, that will in turn allow more false positives to be included in the result. As an example, consider a univariate random sample, $X$, from a normal distribution with the mean of 0 and variance of 4, whose probability distribution function is as shown in figure \ref{fig:rate-example}. Supposing the detection criterion for this particular example is such that an object, $o$, is an anomaly if $o \leq \theta_1$. Then the detection rate, as indicated by the area under the curve to the left of $\theta_1$, is $p_1$ and the false alarm rate is determined by the amount of false positives scattered in this area. To achieve a higher detection rate, the initial threshold, $\theta_1$, can be increased to $\theta_2$ which will increase the previous detection rate by $p_2$ to produce an overall detection rate of 0.95. Also the amount of false positives in the new area will be added to that of the previous area to result in a higher overall false alarm rate. Thus the result of phase 1 is expected to have a relatively high false positive rate which is to be reduced in phase 2.
% 
% <<rate-example, echo=FALSE, fig.align='center', fig.cap="Density of a random sample, $X \\sim N(\\mu = 0, \\sigma^2 = 4)$", out.width="0.6\\linewidth">>=
% set.seed(2)
% den = density(rnorm(100, sd = 2))
% plot(den, xlab = "", ylab = "", main = "", yaxt = "n", xaxs = "i", yaxs = "i", type = "n")
% 
% t0 = which.min(abs(den$x-0))
% polygon(x = c(den$x[1], den$x[1:t0], den$x[t0]), y = c(0, den$y[1:t0], 0), col = "gray75", border = "gray75")
% 
% t1 = which.min(abs(den$x-4))
% polygon(x = c(den$x[t0], den$x[t0:t1], den$x[t1]), y = c(0, den$y[t0:t1], 0), col = "gray80", border = "gray80")
% 
% lines(x = c(den$x[t0], den$x[t0]), c(0, den$y[t0]), lty = 2)
% text(0, 0.01, labels = expression(theta[1]), pos = 4, cex = 1.2)
% lines(x = c(den$x[t1], den$x[t1]), c(0., den$y[t1]), lty = 2)
% text(4, 0.01, labels = expression(theta[2]), pos = 4, cex = 1.2)
% 
% p1 = round(sfsmisc::integrate.xy(den$x[1:t0], den$y[1:t0]), digits = 2)
% p2 = round(sfsmisc::integrate.xy(den$x[t0:t1], den$y[t0:t1]), digits = 2)
% 
% text(-2, 0.04, labels = bquote(p[1] == .(p1)), cex = 1.3)
% text(2, 0.04, labels = bquote(p[2] == .(p2)), cex = 1.3)
% 
% lines(den)
% box()
% @
% 
% The initial step of phase 1 is to apply the LOF algorithm on the numeric portion of a test dataset. Kernel density estimates of the resulting LOF scores are then computed so that their corresponding probability distribution function, or the density, can be plotted. Provided that the choice of parameters, \emph{MinPts} and \emph{MaxPts} (also known as $k$), for the LOF algorithm has been appropriate for the dataset, the density curve is expected to show multiple peaks that represent normal and anomalous connections. The fundamental assumption in anomaly detection is that the majority of a dataset, to which a detection method is applied, is composed of normal instances. This means that the set of LOF scores computed for a test dataset is always expected to be dense around 1. Thus the density curve plotted using the kernel density estimation (KDE) has a global maximum at 1 with a few local maxima afterwards. An example of an expected density curve is shown in figure \ref{fig:density-example} where the global maximum around 1 corresponds to normal connections while the rest of the local maxima correspond to anomalous connections.
% 
% <<density-example, echo=FALSE, fig.align='center', fig.cap="Density of LOF scores", out.width="0.6\\linewidth">>=
% load(file = "lof_scores.RData")
% d = density(lof_30p)
% plot(d, main = "")
% @
% 
% <<threshold-example, echo=FALSE, fig.align='center', fig.cap="Density of LOF scores", out.width="0.6\\linewidth">>=
% load(file = "lof_scores.RData")
% d = density(lof_30p)
% plot(d, main = "")
% local_min = min(which(diff(sign(diff(d$y))) ==   2) + 1)
% local_max = min(which(diff(sign(diff(d$y))) ==  -2) + 1)
% abline(v = d$x[local_min], lty = 2, col = "red")
% abline(v = d$x[local_min]-diff(c(d$x[local_max], d$x[local_min]))*0.1, lty = 2, col = "blue")
% legend(x = 2.355, y = 1.6, c("our threshold", "intuitive threshold"), lty = c(2, 2), col = c("blue", "red"))
% @
% 
% What this observation allows us to determine is the detection threshold. Intuitively, a logical choice for the threshold would be the value where the first local minimum occurs as this point is where the curvature for the density of normal connections ends. However there are a couple of points to consider: (1) there is high possibility that the computed LOF scores themselves are associated with false positives and (2) the goal of phase 1 to detect all possible anomalies should be satisfied. The false positives from the LOF computation, that is the normal connections being identified as anomalous, can be removed by lowering the threshold. Also the purpose of phase 1 can be accomplished by lowering the threshold to allow more true positives, that is the actual anomalous instances, to be detected. Hence we have decided to set the threshold to be at a point 10 percent shy of the first local minimum from the global maximum. The choice of the value, 10 percent, is arbitrary and determination of an optimal amount is a possible future work to be discussed in later chapter.
% 
% Table \ref{tab:phase1} reports the result from 100 experiments each on a random sample of KDD'98 dataset. The sample size was 5,000, where 3,500 of those were connections labelled as normal and the remaining 1,500 were connections labelled as attacks. The detection rate is not exactly 1 as we had hoped but one notable observation is that lowering the threshold further, to achieve a detection rate closer to 1, results in a significant gain in false positive rate. In short, there is law of diminishing returns as the detection rate shifts towards 1.
% 
% <<phase1-table, echo=FALSE, results="asis">>=
% load(file = "phase1.RData")
% library(xtable)
% xtable(phase1$p1.table, caption = "Phase 1 result", label = "tab:phase1")
% @
% 
% The rest of this section is divided into subsections that discuss other notable anomaly detection techniques and whether they can be plausible alternatives to the LOF algorithm.
% 
% 
% \subsection{Other anomaly detection techniques}
% \subsubsection{Statistical model-based approach}
% A classical approach using statistical models is to compute the Mahalanobis distance of all points in a dataset, given by
% \[ D_M(x) = \sqrt{(x-\mu)^{T}\Sigma^{-1}(x-\mu)}, \]
% where $\mu$ and $\Sigma$ denote the vector of means and covariance matrix of the points, respectively. Under the multivariate normality assumption, the Mahalanobis distance follows a $\chi^{2}_{k}$-distribution, where $k$ is the degrees of freedom associated with the dataset. Hence a point whose $D_M$ is greater than the 97.5th percentile of the corresponding $\chi^{2}_{k}$ quantile function is a statistical outlier. The main problem with this approach is that it is extremely difficult to satisfy the multivariate normality assumption for any real life data which tend to be skewed most of the time. Another problem is that the mean of a set of points is extremely sensitive to outlier itself to result in a bias. Obtaining the inverse of a covariance matrix is also problematic when the matrix is singular, which could happen when the dataset contains duplicates. Other possible problem to note is that the Mahalanobis distance metric is heavily affected by the curse of dimensionality.
% 
% \subsubsection{Neighbourhood-based approach}
% The LOF algorithm has advantages over some of the neighbourhood-based anomaly detection techniques. The main advantage is that anomalous objects are detected by a local approach rather than global. Figure \ref{fig:lof-example} illustrates this point. Suppose a dataset with two clusters, one being extremely dense and the other being sparse. Distance-based nearest neighbour approaches such as the $k$-th Nearest Neighbour ($k$NN) will fail to detect the outlier, $p$, as the distance between any point in the sparse cluster to its nearest neighbours is always greater than the distance between $p$ and its nearest neighbours. Hence the distance involving $p$ can never be significantly greater than the other distances to be detected as an outlier. Hence the simple distance-based nearest neighbour approaches do not perform well when there are clusters of varying densities in a dataset whereas the LOF algorithm is designed for such scenario.
% 
% <<lof-example, echo=FALSE, fig.align='center', fig.cap="Advantages of the LOF algorithm", out.width="0.6\\linewidth">>=
% set.seed(3)
% a = cbind(rnorm(30, mean = 10, sd = 4), rnorm(30, mean = 10, sd = 4))
% set.seed(3)
% b = cbind(rnorm(100, mean = 1, sd = 0.4), rnorm(100, mean = 1, sd = 0.4))
% 
% xmin = min(c(min(a[,1]), min(b[,1])))
% xmax = max(c(max(a[,1]), max(b[,1])))
% ymin = min(c(min(a[,2]), min(b[,2])))
% ymax = max(c(max(a[,2]), max(b[,2])))
% 
% plot(a, xlim = c(xmin, xmax), ylim = c(ymin, ymax), xlab = "", ylab = "", xaxt = "n", yaxt = "n")
% points(b)
% points(x = max(b[,1])+0.3, y = max(b[,2])+0.2, pch = 4, cex = 1.1, col = "blue")
% text(x = max(b[,1])+0.3, y = max(b[,2])+0.2, labels = "p", pos = 3, family = "sans", cex = 1.1, col = "blue")
% @
% 
% %The LOF algorithm overcomes this problem by comparing local densities given by the distance between objects and their $k$-th nearest neighbours. The local density, or deviation, of an object is measured with respect to its neighbours so that sparse objects, relative to the densities of their neighbours, can be identified as outliers. The result of LOF is a score for every object in a dataset to indicate whether an object is an outlier. Because the algorithm implements the local approach, the LOF score of an object does not imply the degree of outlierness of the object. The scores are dependent on the local environment, or the neighbourhood, associated with individual objects. The difficulty in interpreting the result can be avoided by assigning the Local Outlier Probability (LoOP) [CITE] for each object. The probability can provide reasonable and comparative indication of an object's outlierness due to its normalised nature but this is beyond the scope of our research as pure detection is what we are after.
% 
% % \subsubsection{Clustering-based approach}
% % The $k$-means clustering is a popular clustering technique that is efficient with reasonable performance. However its drawbacks include (1) using variance as a measure of cluster scatter which can be sensitive to outliers, (2) requirement of the input parameter, $k$, which can affect the result and (3) most notably the convergence to local minima to produce incorrect results. 
% % 
% % Some of these limitations can be prevented by using the $X$-means clustering but the fundamental problems associated with any distance-based clustering techniques, such as the adequacy of the distance metric, are still present.
% % 
% % Unsupervised clustering techniques such as the PCA-based method... (more reading)


%----------------------------------------------------------
\section{Phase 2: False Alarm Reduction}

The collection of anomalous instances detected from phase 1 contains a relatively high volume of false positives. In fact, it should be noted that the major composition of this detected set is now the false positive. Our goal in phase 2 is to: (1) identify the false positive instances within the detected set and (2) remove them to reduce the overall system false alarm rate.

Identification of a false positive instance can be accomplished using two approaches. The first approach is to apply the idea of anomaly detection once again to identify which instances are dissimilar to the majority of the dataset, with expectation that possible anomalies are now normal instances. While this approach provides consistency with phase 1, and therefore over the whole system, it also introduces a couple of obstacles:

\begin{enumerate}

\item \emph{Randomness}: When a detected set, from phase 1, poses random patterns in a sense that those patterns cannot be modelled effectively, it may be extremely difficult to distinguish between possible false positives and true positives, let alone merely recognising truly false positive instances. Suppose a two-dimensional dataset such as shown in Figure \ref{fig:phase2-example1} that resembles a densely packed circle with a few obvious outliers. Imagine an anomaly detection scheme has been applied and points marked with crosses have been identified as possible anomalies under the particular detection criteria of that scheme. Then these marked points, as shown in the plot on the right, represent the detected set of this particular dataset that we may face in phase 2. It may still be arguable that the true outliers in the detected set are distinguishable as they are reasonably isolated from the false positive instances that are favourably clumped together. However the idea of anomaly detection would not be so successful in a more serious situation, as shown in Figure \ref{fig:phase2-example2}, where false positive instances are rather sparse without obvious patterns. A quick glance at Figure \ref{fig:phase2-example2} by itself tells us that there appears to be no apprant patterns but a random scatter of points that hinders effective model construction.

\item \emph{Outlierness within outliers}: one must reconsider using an additional anomaly detection scheme to a dataset that has been previously detected as possible outliers. In our case, the density-based anomaly detection algorithm, LOF, has been applied in phase 1 to provide a set of data points that are considered to be highly sparse and isolated from one another. The sparsity and the extent of isolation among these points are directly related to the fact that the points are distant from one another as well as to the rest of the data. This means that the use of distance, which happens to be what most anomaly detection methods rely on, is not so advantageous in identifying false positive instances.

\end{enumerate}

<<phase2-example1, echo=FALSE, fig.align='center', fig.cap="", out.width="0.45\\linewidth", fig.show='hold'>>=
set.seed(1)
t1 = runif(2000, 0, 2*pi)
set.seed(2)
r1 = sqrt(runif(2000)) * 0.8
a = cbind(r1*cos(t1), r1*sin(t1))
# max(sqrt(a[,1]^2 + a[,2]^2))

set.seed(2)
r2 = runif(100, 0.83, 0.85)
t2 = runif(100, 0, 2*pi)
b = cbind(r2*cos(t2), r2*sin(t2))

set.seed(2)
r3 = 1.1
t3 = runif(5, 0, 2*pi)
c = cbind(r3*cos(t3), r3*sin(t3))
d = rbind(a, b, c)
e = numeric()
for (i in 1:nrow(c)) {
    dis = apply(d[1:2100, ], 1, function(x) sqrt((x[1]-c[i,1])^2 + (x[2]-c[i,2])^2))
    ord = order(dis, decreasing = FALSE)[2:10]
    e = c(e, ord[ord > 2000])
}

plot(d, type = "n", xaxt = "n", yaxt = "n", xlab = "", ylab = "", asp = 1)
points(d[1:2000, ], col = "grey70")
points(d[c(2100:2105, e), ], col = "grey30", pch = 4)
legend("topleft", "detected instances", pch = 4, adj = c(0, 0.5), cex = 1.3, pt.cex = 1.1)

plot(d, type = "n", xaxt = "n", yaxt = "n", xlab = "", ylab = "", asp = 1)
points(d[2101:2105, ], col = "grey30", pch = 4)
points(d[e, ], col = "grey70")
legend("topleft", c("True positive", "False positive"), pch = c(4, 1), adj = c(0, 0.5), cex = 1.3, pt.cex = 1.1)
@

<<phase2-example2, echo=FALSE, fig.align='center', fig.cap="", out.width="0.45\\linewidth">>=
set.seed(1)
t1 = runif(2000, 0, 2*pi)
set.seed(2)
r1 = sqrt(runif(2000)) * 0.8
a = cbind(r1*cos(t1), r1*sin(t1))
# max(sqrt(a[,1]^2 + a[,2]^2))

set.seed(2)
r2 = runif(100, 0.83, 0.85)
t2 = runif(100, 0, 2*pi)
b = cbind(r2*cos(t2), r2*sin(t2))

set.seed(2)
r3 = 1.1
t3 = runif(5, 0, 2*pi)
c = cbind(r3*cos(t3), r3*sin(t3))
d = rbind(a, b, c)
e = numeric()
for (i in 1:nrow(c)) {
    dis = apply(d[1:2100, ], 1, function(x) sqrt((x[1]-c[i,1])^2 + (x[2]-c[i,2])^2))
    ord = order(dis, decreasing = FALSE)[2:10]
    e = c(e, ord[ord > 2000])
}

plot(d, type = "n", xaxt = "n", yaxt = "n", xlab = "", ylab = "", asp = 1)
points(d[2101:2105, ], col = "grey30", pch = 4)
points(d[e[18], 1], d[e[18], 2], pch = 4)
points(d[e[17], 1], d[e[17], 2], pch = 4)
points(d[e[11], 1], d[e[11], 2], pch = 4)
points(d[e[12], 1], d[e[12], 2], pch = 4)
points(d[e[9], 1], d[e[9], 2], pch = 4)
points(d[e[25], 1], d[e[25], 2], pch = 4)
points(d[e[31], 1], d[e[31], 2], pch = 4)
points(d[e[29], 1], d[e[29], 2], pch = 4)
@

The second approach is to apply a clustering scheme to clump similar instances into clusters. The resulting clusters can then be compared with the actual normal instances, i.e., the undetected set in phase 1, in an unsupervised fashion. Even when the situation is dire such as shown in Figure \ref{fig:phase2-exmaple2}, the data points can still be effectively clustered as shown in Figure \ref{fig:phase2-example3}. 

<<phase2-example3, echo=FALSE, fig.align='center', fig.cap="", out.width="0.45\\linewidth">>=
set.seed(1)
t1 = runif(2000, 0, 2*pi)
set.seed(2)
r1 = sqrt(runif(2000)) * 0.8
a = cbind(r1*cos(t1), r1*sin(t1))
# max(sqrt(a[,1]^2 + a[,2]^2))

set.seed(2)
r2 = runif(100, 0.83, 0.85)
t2 = runif(100, 0, 2*pi)
b = cbind(r2*cos(t2), r2*sin(t2))

set.seed(2)
r3 = 1.1
t3 = runif(5, 0, 2*pi)
c = cbind(r3*cos(t3), r3*sin(t3))
d = rbind(a, b, c)
e = numeric()
for (i in 1:nrow(c)) {
    dis = apply(d[1:2100, ], 1, function(x) sqrt((x[1]-c[i,1])^2 + (x[2]-c[i,2])^2))
    ord = order(dis, decreasing = FALSE)[2:10]
    e = c(e, ord[ord > 2000])
}

f = rbind(d[2101:2105, ], d[e[c(18,17,11,12,9,25,31,29)], ])
g = kmeans(f, 4, nstart = 20)

plot(f, type = "n", xaxt = "n", yaxt = "n", xlab = "", ylab = "", asp = 1)
points(f[g$cluster==1, ], pch = 3)
points(f[g$cluster==2, ], pch = 4)
points(f[g$cluster==3, ], pch = 0)
points(f[g$cluster==4, ], pch = 1)



@


\subsection{X-Means Clustering}







%----------------------------------------------------------
\subsection{Cluster Identification}






