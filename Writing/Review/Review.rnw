<<set-parent2, echo=FALSE, cache=FALSE>>=
knitr::set_parent(parent = "../Main/Main.Rnw")
@

% ---------------------------
% CHAPTER: Literature Review
% ---------------------------
\chapter{Literature Review}
\label{ch:review}
There have been many anomaly-based topics studied in unsupervised network intrusion detection among the academic community. One that appears to be the most extensively studied in the literature is clustering-based detection, some of which is discussed in Section \ref{review:sec1}. Every one of these approaches has either filled the gap in the knowledge stated by the corresponding authors or contributed towards improving the ideas of intrusion detection, or both. For some of them, a common theme to notice was in finding different or better ways of implementing intrusion detection in terms of achieving high accuracy. The problem arises when the majority of threats is correctly identified, as True Positives, but many of non-threats are also labelled incorrectly as threats in the process, known as False Positives. The subject of preventing the occurrence of False Positives or reducing their number in the result is rarely seen in the literature, and this is what we consider a gap in the field that deserves attention. Finally, discussion of other works that are not primarily clustering-based is presented in Section \ref{review:sec2}.


% ----------------------------
% SECTION 1: Clustering-based
% ----------------------------
\section{Clustering-based anomaly detection}
\label{review:sec1}
From a historical point of view, Portnoy, Eskin and Stolfo were the pioneers in implementing an unsupervised clustering-based approach in network intrusion detection \cite{por01}. In their work, problems associated with signature-based detection methods, in terms of difficulties in detecting novel threats and gaining access to labelled data, are addressed with an approach using a single-linkage hierarchical clustering method. Initially, input data are \emph{standardised}, that is each feature is normalised in terms of standard deviation units away from its mean. Then clusters of normal and anomalous instances are formed using training data with an overwhelmingly high frequency of normal instances ($> 98\%$), and each instance in test data is assigned to the closest cluster based on the standard Euclidean distance. The overall outcome was a method which did not require labelled data for training unlike anything at the time. But there was much room for improvement in performance and automation as suggested by the authors.

Eskin et al.\ investigated the effectiveness of three algorithms: (1) the fixed-width clustering algorithm, (2) an optimised version of the k-nearest neighbour (k-NN) algorithm and (3) the one class support vector machine (SVM) algorithm \cite{esk02} where the authors' criterion for detecting anomalies is to find points in sparse regions. A different data transformation is proposed, that is the discrete features of network connection data are fuzzy coded, to assign each instance a degree of membership in its corresponding feature between 0 and 1, while the numeric features are standardised. In addition, data for system call traces are transformed using a spectrum kernel \cite{les02} so that the algorithms can work on the sequential data. For the fixed-width clustering approach, the starting point is the centre of the first cluster and subsequent points within the width, $w$, from the centre are added to the cluster while those whose distance is further than $w$ form new centres. Then the members of small clusters are labelled as anomalies. For the K-NN approach, the aforementioned fixed-width clustering algorithm is applied initially. Then a variant of canopy clustering \cite{mcc00} is run on each cluster to remove the necessity of computing through every point for the k-NN algorithm, therefore increasing the efficiency. Lastly, the one class SVM approach involves maximally separating entire data from the origin as two classes. The three algorithms provide improved results than the previously mentioned single-linkage based algorithm. Possibility of avoiding a relatively ineffective clustering algorithm such as the single-linkage algorithm is portrayed. However increasingly high false positive rates are expected in order to achieve true positive rate above 0.9. Oldmeadow, Ravinutala and Leckie contributed to the three algorithms by proposing feature weighting and an extension of the fixed-width algorithm adaptive to non-stationary traffic data \cite{old04}.

Leung and Leckie aimed to reduce the computational complexity of the (aforementioned) existing algorithms by proposing a hybrid method that combines density- and grid-based clustering algorithms \cite{leu05}. They proposed an optimised version of the grid-based clustering algorithm, pMAFIA \cite{nag00}, by implementing the Frequency-Pattern Tree (FP-Tree) from the frequent itemset mining algorithm, FP-growth \cite{han00} and named it fpMAFIA. While the computational complexity improved, False Positive Rate was relatively higher compared to the existing methods, roughly at 30\% to achieve True Positive Rate of 90\%.

A more recent work by Casas, Mazel and Owezarski \cite{cas11, cas12, cas14} is an Unsupervised Network Intrusion Detection System (UNIDS) which involves the concept of subspace-clustering, that is the task of finding all clusters in \emph{all} subspaces of given data and a variant of Evidence Accumulation Clustering (EAC), that is the task of combining the results of multiple clusterings in order to assess within-cluster similarity more closely so that potentially better groupings can be achieved. More specifically, the property of monotonicity known as the \emph{downward-closure property}, that says the members of a cluster in a $k$-dimensional space are also part of a cluster in any $(k-1)$ projections of the space, is applied which leads to the use of multiple permutations of a pairwise clustering task using DBSCAN clustering algorithm. Then, for each data instance, distance to the largest corresponding cluster is computed and added on iteratively to accumulate how far the given instance is away from the respective cluster. This method outperforms all the approaches mentioned previously. It is of a particular interest for us to compare our results with UNIDS because both methods use DBSCAN clustering algorithm, although in different ways, and the results produced by UNIDS seem to be the best in the academic community so far.




% ------------------
% SECTION 2: Others
% ------------------
\section{Other types of anomaly detection}
\label{review:sec2}
A noteworthy contribution to the one class SVM approach discussed in the previous section is a quarter-sphere SVM approach proposed by Laskov et al.\ \cite{las04}. Instead of using the planar approach in the ordinary one class SVM, a sphere variant known as the Support Vector Data Description 
(SVDD) is discussed. The concept of SVDD is further studied and, relying on the property of intrusion data that most of the features are non-negatively defined, the quarter-sphere approach is proposed with moderate success.

Zanero and Savaresi addressed difficulties associated with the large size of network data and proposed a two-tier intrusion detection system (IDS) \cite{zan04}. The first tier is an unsupervised clustering algorithm whose purpose is to compress the size of data into more manageable structure so that a subsequent traditional anomaly detection algorithm for the second tier works at improved efficiency. This novel system shares a similarity with our approach in that both methods are executed in a two-phase, or a two-tier, manner. A critical difference between the two lies in the objective set for each phase, that is, in our case, the attempt to (1) improve the accuracy at the expense of allowing more False Positives to arise in the process and (2) eliminate the allowed False Positives along with the naturally occurred False Positives altogether from the result.

Lastly, there are a couple of notable spectral anomaly detection methods in the literature \cite{tho03,lak05}. The former used Principal Component Analysis (PCA) on time series to detect high eigenvalues as they indicate abrupt changes that translate to anomalous activities and the latter used PCA to find a subspace that explains about 85\% of the variability in given data and a clustering algorithm to classify data instances as either an anomaly or normal.

There are other contributions in the field that are not mentioned in this chapter, simply because it is impossible to mention them all. A more extensive list of anomaly detection techniques in various domains is presented in \cite{cha09} and a list of anomaly-based NIDS in deployment, both commercial and freeware, is presented in \cite{gar09}.




% Spectral-based, as Chandola et al. describes, or subspace-clustering based...


% For the PCA-based approach, we perform: (1) a \emph{Principal Component Analysis} (PCA) on a test dataset to project it into a subspace that explains a reasonable amount of variability in the dataset, (2) use a clustering technique on the projected data and (3) eliminate the members of the most sizeable cluster under an assumption that the size of such cluster corresponds to the high frequency of normal instances in the data.




% Comment below out when building the whole doc:
% \bibliographystyle{ieeetr}
% \bibliography{../Main/bibliography}

