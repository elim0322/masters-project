<<set-parent, echo=FALSE, cache=FALSE>>=
knitr::set_parent(parent = "../Main/Main.Rnw")
@
<<scores, cache=TRUE, echo=FALSE>>=
load(file = "~/../Desktop/masters-project/Data/LOF_scores/scores.RData")
@
<<result, cache=TRUE, echo=FALSE>>=
load(file = "~/../Desktop/masters-project/Data/System_results/dat1_05a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat1_10a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat1_20a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat1_30a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat1_40a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat1_50a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat2_05a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat2_10a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat2_20a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat2_30a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat2_40a.RData")
load(file = "~/../Desktop/masters-project/Data/System_results/dat2_50a.RData")
processResults = function(results, graph = FALSE) {
    d = numeric()
    f = numeric()
    for (i in 1:100) {
        dif     = diff(results$p2.proportion.within.cluster[i, 2:10])
        dif.neg = which(dif < 0)
        mn      = dif.neg[which.min(dif[dif.neg])] + 1
        # mn      = which.min(dif[dif.neg]) + 2
        d       = c(d, results$system.detection.rate[i, mn])
        f       = c(f, results$system.false.alarm.rate[i, mn])
    }
    if (graph) {
        avg.TPR = colMeans(results$system.detection.rate)
        avg.FPR = colMeans(results$system.false.alarm.rate)
        avg.pwc = colMeans(results$p2.proportion.within.cluster)
        avg.pws = colMeans(results$p2.proportion.within.sample)
        plot(x = 0:10, y = seq(0, 1, by = 0.1), type = "n")
        lines(avg.TPR, col = "blue")
        lines(avg.FPR, col = "red")
        lines(avg.pwc)
        lines(avg.pws)
    }
    return(list(TPR = mean(d), FPR = mean(f), TPR.sd = sd(d), FPR.sd = sd(f)))
}
a = function(results) {
    tpr = numeric()
    fpr = numeric()
    tpr.sd = numeric()
    fpr.sd = numeric()
    for (i in 1:length(results)) {
        tpr = c(tpr, mean(results[[i]]$P1.detection.rate))
        fpr = c(fpr, mean(results[[i]]$P1.false.alarm.rate))
        tpr.sd = c(tpr.sd, sd(results[[i]]$P1.detection.rate))
        fpr.sd = c(fpr.sd, sd(results[[i]]$P1.false.alarm.rate))
    }
    w = which.max(tpr)
    list(tpr[w], fpr[w], tpr.sd[w], fpr.sd[w])
}
b = function(results) {
    tpr = numeric()
    fpr = numeric()
    for (i in 1:length(results)) {
        tpr = c(tpr, sd(results[[i]]$P1.detection.rate))
        fpr = c(fpr, sd(results[[i]]$P1.false.alarm.rate))
    }
    w = which.max(tpr)
    list(tpr[w], fpr[w])
}
{
dat1.means.05k = a(list(system.results_dat1_05a05k, system.results_dat1_10a05k, system.results_dat1_20a05k, system.results_dat1_30a05k, system.results_dat1_40a05k, system.results_dat1_50a05k))
dat1.means.10k = a(list(system.results_dat1_05a10k, system.results_dat1_10a10k, system.results_dat1_20a10k, system.results_dat1_30a10k, system.results_dat1_40a10k, system.results_dat1_50a10k))
dat1.means.20k = a(list(system.results_dat1_05a20k, system.results_dat1_10a20k, system.results_dat1_20a20k, system.results_dat1_30a20k, system.results_dat1_40a20k, system.results_dat1_50a20k))
dat1.means.30k = a(list(system.results_dat1_05a30k, system.results_dat1_10a30k, system.results_dat1_20a30k, system.results_dat1_30a30k, system.results_dat1_40a30k, system.results_dat1_50a30k))
dat1.means.40k = a(list(system.results_dat1_05a40k, system.results_dat1_10a40k, system.results_dat1_20a40k, system.results_dat1_30a40k, system.results_dat1_40a40k, system.results_dat1_50a40k))
dat1.means.50k = a(list(system.results_dat1_05a50k, system.results_dat1_10a50k, system.results_dat1_20a50k, system.results_dat1_30a50k, system.results_dat1_40a50k, system.results_dat1_50a50k))

dat1.sd.05k = b(list(system.results_dat1_05a05k, system.results_dat1_10a05k, system.results_dat1_20a05k, system.results_dat1_30a05k, system.results_dat1_40a05k, system.results_dat1_50a05k))
dat1.sd.10k = b(list(system.results_dat1_05a10k, system.results_dat1_10a10k, system.results_dat1_20a10k, system.results_dat1_30a10k, system.results_dat1_40a10k, system.results_dat1_50a10k))
dat1.sd.20k = b(list(system.results_dat1_05a20k, system.results_dat1_10a20k, system.results_dat1_20a20k, system.results_dat1_30a20k, system.results_dat1_40a20k, system.results_dat1_50a20k))
dat1.sd.30k = b(list(system.results_dat1_05a30k, system.results_dat1_10a30k, system.results_dat1_20a30k, system.results_dat1_30a30k, system.results_dat1_40a30k, system.results_dat1_50a30k))
dat1.sd.40k = b(list(system.results_dat1_05a40k, system.results_dat1_10a40k, system.results_dat1_20a40k, system.results_dat1_30a40k, system.results_dat1_40a40k, system.results_dat1_50a40k))
dat1.sd.50k = b(list(system.results_dat1_05a50k, system.results_dat1_10a50k, system.results_dat1_20a50k, system.results_dat1_30a50k, system.results_dat1_40a50k, system.results_dat1_50a50k))


## dat2
dat2.means.05k = a(list(system.results_dat2_05a05k, system.results_dat2_10a05k, system.results_dat2_20a05k, system.results_dat2_30a05k, system.results_dat2_40a05k, system.results_dat2_50a05k))
dat2.means.10k = a(list(system.results_dat2_05a10k, system.results_dat2_10a10k, system.results_dat2_20a10k, system.results_dat2_30a10k, system.results_dat2_40a10k, system.results_dat2_50a10k))
dat2.means.20k = a(list(system.results_dat2_05a20k, system.results_dat2_10a20k, system.results_dat2_20a20k, system.results_dat2_30a20k, system.results_dat2_40a20k, system.results_dat2_50a20k))
dat2.means.30k = a(list(system.results_dat2_05a30k, system.results_dat2_10a30k, system.results_dat2_20a30k, system.results_dat2_30a30k, system.results_dat2_40a30k, system.results_dat2_50a30k))
dat2.means.40k = a(list(system.results_dat2_05a40k, system.results_dat2_10a40k, system.results_dat2_20a40k, system.results_dat2_30a40k, system.results_dat2_40a40k, system.results_dat2_50a40k))
dat2.means.50k = a(list(system.results_dat2_05a50k, system.results_dat2_10a50k, system.results_dat2_20a50k, system.results_dat2_30a50k, system.results_dat2_40a50k, system.results_dat2_50a50k))

dat2.sd.05k = b(list(system.results_dat2_05a05k, system.results_dat2_10a05k, system.results_dat2_20a05k, system.results_dat2_30a05k, system.results_dat2_40a05k, system.results_dat2_50a05k))
dat2.sd.10k = b(list(system.results_dat2_05a10k, system.results_dat2_10a10k, system.results_dat2_20a10k, system.results_dat2_30a10k, system.results_dat2_40a10k, system.results_dat2_50a10k))
dat2.sd.20k = b(list(system.results_dat2_05a20k, system.results_dat2_10a20k, system.results_dat2_20a20k, system.results_dat2_30a20k, system.results_dat2_40a20k, system.results_dat2_50a20k))
dat2.sd.30k = b(list(system.results_dat2_05a30k, system.results_dat2_10a30k, system.results_dat2_20a30k, system.results_dat2_30a30k, system.results_dat2_40a30k, system.results_dat2_50a30k))
dat2.sd.40k = b(list(system.results_dat2_05a40k, system.results_dat2_10a40k, system.results_dat2_20a40k, system.results_dat2_30a40k, system.results_dat2_40a40k, system.results_dat2_50a40k))
dat2.sd.50k = b(list(system.results_dat2_05a50k, system.results_dat2_10a50k, system.results_dat2_20a50k, system.results_dat2_30a50k, system.results_dat2_40a50k, system.results_dat2_50a50k))
}

## dat1
dat1.means.05a = a(list(system.results_dat1_05a05k, system.results_dat1_05a10k, system.results_dat1_05a20k, system.results_dat1_05a30k, system.results_dat1_05a40k, system.results_dat1_05a50k))
dat1.means.10a = a(list(system.results_dat1_10a05k, system.results_dat1_10a10k, system.results_dat1_10a20k, system.results_dat1_10a30k, system.results_dat1_10a40k, system.results_dat1_10a50k))
dat1.means.20a = a(list(system.results_dat1_20a05k, system.results_dat1_20a10k, system.results_dat1_20a20k, system.results_dat1_20a30k, system.results_dat1_20a40k, system.results_dat1_20a50k))
dat1.means.30a = a(list(system.results_dat1_30a05k, system.results_dat1_30a10k, system.results_dat1_30a20k, system.results_dat1_30a30k, system.results_dat1_30a40k, system.results_dat1_30a50k))
dat1.means.40a = a(list(system.results_dat1_40a05k, system.results_dat1_40a10k, system.results_dat1_40a20k, system.results_dat1_40a30k, system.results_dat1_40a40k, system.results_dat1_40a50k))
dat1.means.50a = a(list(system.results_dat1_50a05k, system.results_dat1_50a10k, system.results_dat1_50a20k, system.results_dat1_50a30k, system.results_dat1_50a40k, system.results_dat1_50a50k))

## dat2
dat2.means.05a = a(list(system.results_dat2_05a05k, system.results_dat2_05a10k, system.results_dat2_05a20k, system.results_dat2_05a30k, system.results_dat2_05a40k, system.results_dat2_05a50k))
dat2.means.10a = a(list(system.results_dat2_10a05k, system.results_dat2_10a10k, system.results_dat2_10a20k, system.results_dat2_10a30k, system.results_dat2_10a40k, system.results_dat2_10a50k))
dat2.means.20a = a(list(system.results_dat2_20a05k, system.results_dat2_20a10k, system.results_dat2_20a20k, system.results_dat2_20a30k, system.results_dat2_20a40k, system.results_dat2_20a50k))
dat2.means.30a = a(list(system.results_dat2_30a05k, system.results_dat2_30a10k, system.results_dat2_30a20k, system.results_dat2_30a30k, system.results_dat2_30a40k, system.results_dat2_30a50k))
dat2.means.40a = a(list(system.results_dat2_40a05k, system.results_dat2_40a10k, system.results_dat2_40a20k, system.results_dat2_40a30k, system.results_dat2_40a40k, system.results_dat2_40a50k))
dat2.means.50a = a(list(system.results_dat2_50a05k, system.results_dat2_50a10k, system.results_dat2_50a20k, system.results_dat2_50a30k, system.results_dat2_50a40k, system.results_dat2_50a50k))
@

% --------------------
% CHAPTER: Evaluation
% --------------------
\chapter{Evaluation}
\label{ch:evaluation}

% Maybe write this in discussion/conclusion?
%The dataset chosen for benchmarking our intrusion detection system is the KDD Cup 1999 data \cite{kdd99}. There are a few issues associated with the dataset such as the presence of redundant features and duplicated instances, mentioned by Tavallaee et al. \cite{tav09} who also proposed an improved version of the dataset called NSL-KDD. Despite the issues, our decision to proceed with KDD'99 is because it is a real world data captured at MIT Lincoln Labs which means it is more likely to be accurate at describing the characteristics of real world connections compared to those described by the processed data, NSL-KDD. Another possible limitation with KDD'99 dataset is its age. Because the dataset is dated, there is high possibility that the characteristics of modern attacks may be different to those described in the dataset. However it is the only real world dataset with complete attack labels to remain as the most popular choice as a benchmarking medium for any IDS evaluation. The lack of other public network data appears to be due to the highly sensitive nature associated with network information. Technical description of KDD'99 dataset is presented in Section \ref{sec:evaluation-data}.

In this chapter, we validate fulfillment of the ideas described in the previous chapter as well as evaluate the strength of our proposed NIDS in detecting intrusions in the well-known and widely used KDD Cup 1999 data \cite{kdd99}. For this, we are mainly interested in True and False Positive Rates (TPR and FPR), also known as the detection and false alarm rates respectively, of the system to assess how accurately each instance is classified as either an intrusion or a non-intrusion.

Each experiment is conducted using a random sample of 5,000 instances from the 10\% subset version of the original KDD'99 dataset and is repeated one hundred times to compensate for the rather large size of the dataset. The sample size, of 5,000 records, is chosen such that the LOF algorithm runs comfortably without reaching beyond the memory limit of the machine used for this experiment. The results from the repeated experiments are reported on their means and standard deviations. Lastly, we treat both the original training and test datasets as two separate datasets as our system operates under the unsupervised learning paradigm, that is the labels are only used for external evaluation purposes.

% These two performance measures are evaluted for other popular unsupervised NID techniques to compare our method against them.

% The dataset chosen for benchmarking our NIDS is the KDD Cup 1999 data \cite{kdd99} for its public availability with complete sets of labels. There have been a couple of datasets to replace the original KDD'99 data such as the NSL-KDD data \cite{tav09} and gureKddcup data \cite{per08}. The former dataset is essentially a carefully selected set of instances, mainly to avoid redundant features and duplicated entries, from the original KDD'99 data that does not introduce any novelty for us to be interested in. The latter dataset is generated equivalently as the original data with additional information on the payload of each connection record, that is the information of the transferred data. Because the payload data is processed as a sequence of bytes, for which sequential pattern mining is more appropriate, the gureKddcup data has not been a suitable choice for us. Other alternative datasets \cite{cre13} and \cite{shi12} were unavailable for access. All things considered, the KDD'99 data seems to remain as the most widely used dataset for the purpose of evaluating and benchmarking an IDS, especially because of the sheer number of literatures based on it. More technical description of the KDD'99 dataset is presented in Section \ref{evaluation:sec1}.

% The rest of this chapther includes the experimental setup for the evaluation in Section \ref{evaluation:sec2}, phase-specific evaluation results in Section \ref{evaluation:sec3} and system evaluation results  in Section \ref{evaluation:sec4}, along with our choice of performance measures.

%Each experiment, or run, is conducted using a random sample of 5,000 records as a testset. The ratio of intrusion and non-intrusion instances is varied in the testset to report the performance of the IDS between the differing ratios. The variation is specified for the percentage of intrusion instances in a test dataset at 10, 20, 30 and 40 percents. The sample size of 5,000 is chosen such that the LOF algorithm runs comfortably without reaching beyond the memory limit of the machine used for this experiment.

%The parameter $k$ for the LOF algorithm is varied to assess whether the value of $k$ has an impact on the performance of LOF.

% -------------------------------
% SECTION 1: Validation of ideas
% -------------------------------
\section{Validation of ideas}
\label{evaluation:sec1}
The two-phase structure of our proposed NIDS, each with its unique objective, means that each phase must be evaluated to show the assigned objective is successfully accomplished. As presented in the previous chapter, the first phase is intended for maximum classification of all possible intrusions at the cost of higher false positive rate. Therefore Phase 1 is evaluated with its: (1) TPR to test our hypothesis that it is capable of achieving the said maximal classification and (2) FPR to confirm our belief that a higher number of false positives, than under ordinary circumstances, is resulted as expected. For Phase 2, our primary interest, as well as its purpose, is in the reduction of overall false positive rate thus percentage reductions in the FPR are investigated.
% Lastly, our claim that the system of our proposal is truly comparable to DBSCAN clustering algorithm is tested with their respective TPR and FPR across the parameter, $\epsilon$ (Eps). The other parameter of DBSCAN, minPts, is fixed at 6 as we have experimentally determined that the result is not critically affected by varying the parameter.

\subsubsection{Phase 1}
Appendix A presents numerical and graphical results from experiments where intrusion frequency in a test set and $k$ are varied as proportions of the sample size. What we have inferred from the results is that $k$ is the most appropriate when set to be greater or equal to the actual number of intrusion instances in a test set. If $k$ is set too low, the range of resulting LOF scores increases along with noise to introduce difficulties in inferring thresholds. As $k$ is increased towards the number of intrusion instances, the resulting LOF scores tend to stabilise in a range within more manageable boundaries. For the purpose of this section, we assume that $k$ is chosen appropriately for each intrusion frequency, that is we report on the best scenario of $k$ from here on.

Figure \ref{fig:lofResults} reveals that the classification process of Phase 1 is reasonably accurate for the \emph{training} dataset as shown by the very high TPRs but not as accurate for the \emph{test} dataset. Upon manual examination, we have found that, for the \emph{test} dataset, many intrusion instances share similar local densities as their non-intrusion counterparts leading to failure in finding adequate thresholds. When the kernel density esimates  were plotted, a number of intrusion instances had unusually low LOF scores than the rest to be positioned within the supposed region of non-intrusion instances, and vice versa. In other words, the LOF algorithm was not able to differentiate between the non-intrusion and the intrusion instances because their respective local densities are analogous. On the other hand, the detection is quite precise as the lengths of the error bars are extremely small.

We conclude that our classification method implemented in Phase 1 performs well as long as the proportion of intrusion is not too large and the local densities are reasonably different between intrusion and non-intrusion instances. For the cases where local densities are similar, an anomaly detection algorithm that is not density-based could be a more viable alternative.

<<lofResults, dependson="result", echo=FALSE, out.width="0.49\\linewidth", fig.show='hold', fig.align='center', fig.cap="TPR and FPR across intrusion frequencies from the training and test sets">>=
x = 1:6
y1 = c(dat1.means.05a[[1]], dat1.means.10a[[1]], dat1.means.20a[[1]], dat1.means.30a[[1]], dat1.means.40a[[1]], dat1.means.50a[[1]])
y2 = c(dat1.means.05a[[2]], dat1.means.10a[[2]], dat1.means.20a[[2]], dat1.means.30a[[2]], dat1.means.40a[[2]], dat1.means.50a[[2]])
y1.sd = c(dat1.means.05a[[3]], dat1.means.10a[[3]], dat1.means.20a[[3]], dat1.means.30a[[3]], dat1.means.40a[[3]], dat1.means.50a[[3]])
y2.sd = c(dat1.means.05a[[4]], dat1.means.10a[[4]], dat1.means.20a[[4]], dat1.means.30a[[4]], dat1.means.40a[[4]], dat1.means.50a[[4]])

# par(xpd = T, mar = par()$mar+c(0,0,0,3))
plot(y1, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Rates", lwd = 1.3, type="b", main=expression(italic("Training dataset")))
lines(y2, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6, 0.6, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), cex = 1.2, xjust=1)
arrows(x, y1-y1.sd, x, y1+y1.sd, length=0.05, angle=90, code=3)
arrows(x, y2-y2.sd, x, y2+y2.sd, length=0.05, angle=90, code=3)

y3 = c(dat2.means.05a[[1]], dat2.means.10a[[1]], dat2.means.20a[[1]], dat2.means.30a[[1]], dat2.means.40a[[1]], dat2.means.50a[[1]])
y4 = c(dat2.means.05a[[2]], dat2.means.10a[[2]], dat2.means.20a[[2]], dat2.means.30a[[2]], dat2.means.40a[[2]], dat2.means.50a[[2]])
y3.sd = c(dat2.means.05a[[3]], dat2.means.10a[[3]], dat2.means.20a[[3]], dat2.means.30a[[3]], dat2.means.40a[[3]], dat2.means.50a[[3]])
y4.sd = c(dat2.means.05a[[4]], dat2.means.10a[[4]], dat2.means.20a[[4]], dat2.means.30a[[4]], dat2.means.40a[[4]], dat2.means.50a[[4]])

plot(y3, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Rates", lwd = 1.3, type="b", main=expression(italic("Test dataset")))
lines(y4, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6, 0.6, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), cex = 1.2, xjust=1)
arrows(x, y3-y3.sd, x, y3+y3.sd, length=0.05, angle=90, code=3)
arrows(x, y4-y4.sd, x, y4+y4.sd, length=0.05, angle=90, code=3)
@

\subsubsection{Phase 2}
Figure \ref{fig:dbscanResults} presents percentage reduction in FPRs from Phase 1 to Phase 2. It seems that the FP elimination step of Phase 2 works really well, especially for the \emph{test} dataset. The unusual decline in the percentage at the intrusion frequency of 0.2 for the \emph{training} dataset is not something to worry about as the initial FPR, from Phase 1, has been quite low to begin with.

Some of the FPs remain included in the result, limiting further percentage reduction in FPR, because they are incorrectly classified, or clustered, by the DBSCAN algorithm. These FPs are positioned in relatively dense regions nearby some of the intrusion instances to make them indistinguishable from one another based on density. Therefore the DBSCAN algorithm fails to cluster them with non-intrusion instances by definition.

<<dbscanResults, dependson="result", echo=FALSE, out.width="0.49\\linewidth", fig.show='hold', fig.align='center', fig.cap="Percentage reduction in FPR across intrusion frequencies from the training and test sets">>=
dat1.FPR = c(dat1.means.05a[[2]], dat1.means.10a[[2]], dat1.means.20a[[2]], dat1.means.30a[[2]], dat1.means.40a[[2]], dat1.means.50a[[2]])
dat2.FPR = c(dat2.means.05a[[2]], dat2.means.10a[[2]], dat2.means.20a[[2]], dat2.means.30a[[2]], dat2.means.40a[[2]], dat2.means.50a[[2]])
y1 = 100 * (dat1.FPR - c(processResults(system.results_dat1_05a10k)$FPR,processResults(system.results_dat1_10a30k)$FPR,processResults(system.results_dat1_20a20k)$FPR,processResults(system.results_dat1_30a30k)$FPR,processResults(system.results_dat1_40a20k)$FPR,processResults(system.results_dat1_50a05k)$FPR)) / dat1.FPR
y2 = 100 * (dat2.FPR - c(processResults(system.results_dat2_05a10k)$FPR,processResults(system.results_dat2_10a20k)$FPR,processResults(system.results_dat2_20a20k)$FPR,processResults(system.results_dat2_30a30k)$FPR,processResults(system.results_dat2_40a30k)$FPR,processResults(system.results_dat2_50a10k)$FPR)) / dat2.FPR
plot(y1, ylim=c(0,100), xlab = "Intrusion Freq.", xaxt="n", ylab="Percentage Reduction in FPR", lwd = 1.3, type="b", main=expression(italic("Training dataset")))
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
plot(y2, ylim=c(0,100), xlab = "Intrusion Freq.", xaxt="n", ylab="Percentage Reduction in FPR", lwd = 1.3, type="b", main=expression(italic("Training dataset")))
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
@



% -----------------------------
% SECTION 2: System evaluation
% -----------------------------
\section{System evaluation}
\label{evaluation:sec2}

Figure \ref{fig:sysResults} presents the system evaluation results for the \emph{training} and \emph{test} datasets. For the \emph{training} dataset between the intrusion frequencies of 0.05 and 0.4, we can infer graphically that the system is accurate, as the TPRs are very close to 1, and precise, as the lengths of the error bars are short. We suspect that the decrease in performance from the intrusion frequency of 0.5 is caused by the balanced nature of the classes in the random samples. As the data become more balanced, there is higher chance for DBSCAN to form balanced clusters. This in turn increases the likelihood that the largest clusters, which we remove, actually consist of intrusion instances to result in a decrease in overall TPR. Because our method assumes of higher frequencies of non-intrusion instances in test data, a mendatory assumption for any anomaly-based intrusion detection, and removes the largest resulting clusters under that assumption, it cannot be functional if that assumption is violated. In other words, perfectly balanced data cannot be classified correctly in an unsupervised learning manner unless some external information is available. The unusually high variation in the TPR at this particular intrusion frequency means that the system was highly indecisive because there were essentially no classification criteria. In fact about half of the hundred runs produced better results than the other half to further support our reasoning.

For the \emph{test} dataset, both the TPRs and FPRs are generally lower compared to those from the \emph{training} dataset. We suspect the primary cause of this to be the similarities in local densities introducing challenges against effective classification. Also the performance starts to decrease at the intrusion frequency of 0.4, that is lower compared to the \emph{training} set. This can also be explained by the similarities between the local densities that scale down the previously mentioned threshold at which the classification criteria cease to exist. The problem of indistinguishable local densities is difficult to address because each dataset is associated with a unique degree of similarity within its respective local densities. For data that suffer from extremely similar densities, the best solution may be to avoid using anything distance-based, because densities are essentially computed using distances, and rather attempt to incorporate some external information capable of describing different types of instances. Another possible cause may be associated with our random sampling method. We attempted to cover as much portion of the data as possible by running repeated experiments as well as by sampling without replacement. However running repeated experiments with small samples, which may not be fitting representatives of the entire dataset, can skew the results in certain directions. In addition, samples, randomly selected without replacement, are no longer independent of one another, which may have an effect on the overall results. We could not increase the sample size further due to the expensive nature of the LOF algorithm.

% One relevant metric, as a measure of dispersion, to evaluate the standard deviations is the \emph{coefficient of variation}, $c_v$, given by
% \[ c_v = \frac{\sigma}{\mu}, \]
% all of which are well under 1 to support the preciseness of our results for those intrusion frequencies.

<<sysResults, dependson="result", cache=TRUE, echo=FALSE, out.width="0.49\\linewidth", fig.show='hold', fig.align='center', fig.cap="System TPR and FPR across intrusion frequencies from the training and test sets">>=
# res05a = processResults(system.results_dat1_10a30k)
res05a = processResults(system.results_dat1_05a05k)
res10a = processResults(system.results_dat1_10a30k)
res20a = processResults(system.results_dat1_20a20k)
res30a = processResults(system.results_dat1_30a40k)
res40a = processResults(system.results_dat1_40a20k)
res50a = processResults(system.results_dat1_50a05k)
x = 1:6
y1 = c(res05a$TPR, res10a$TPR, res20a$TPR, res30a$TPR, res40a$TPR, res50a$TPR)
y2 = c(res05a$FPR, res10a$FPR, res20a$FPR, res30a$FPR, res40a$FPR, res50a$FPR)
y1.sd = c(res05a$TPR.sd, res10a$TPR.sd, res20a$TPR.sd, res30a$TPR.sd, res40a$TPR.sd, res50a$TPR.sd)
y2.sd = c(res05a$FPR.sd, res10a$FPR.sd, res20a$FPR.sd, res30a$FPR.sd, res40a$FPR.sd, res50a$FPR.sd)

plot(y1, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Rates", lwd = 1.3, type="b", main=expression(italic("Training dataset")))
lines(y2, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6*1.034, 1.04, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), xjust=1, yjust=0, xpd=TRUE)
arrows(x, y1-y1.sd, x, y1+y1.sd, length=0.05, angle=90, code=3)
arrows(x, y2-y2.sd, x, y2+y2.sd, length=0.05, angle=90, code=3)


dat2.res05a = processResults(system.results_dat2_05a20k)
dat2.res10a = processResults(system.results_dat2_10a30k)
dat2.res20a = processResults(system.results_dat2_20a20k)
dat2.res30a = processResults(system.results_dat2_30a30k)
dat2.res40a = processResults(system.results_dat2_40a20k)
dat2.res50a = processResults(system.results_dat2_50a05k)
y3 = c(dat2.res05a$TPR, dat2.res10a$TPR, dat2.res20a$TPR, dat2.res30a$TPR, dat2.res40a$TPR, dat2.res50a$TPR)
y4 = c(dat2.res05a$FPR, dat2.res10a$FPR, dat2.res20a$FPR, dat2.res30a$FPR, dat2.res40a$FPR, dat2.res50a$FPR)
y3.sd = c(dat2.res05a$TPR.sd, dat2.res10a$TPR.sd, dat2.res20a$TPR.sd, dat2.res30a$TPR.sd, dat2.res40a$TPR.sd, dat2.res50a$TPR.sd)
y4.sd = c(dat2.res05a$FPR.sd, dat2.res10a$FPR.sd, dat2.res20a$FPR.sd, dat2.res30a$FPR.sd, dat2.res40a$FPR.sd, dat2.res50a$FPR.sd)
plot(y3, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Percentage Reduction in FPR", lwd = 1.3, type="b", main=expression(italic("Training dataset")))
lines(y4, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6*1.034, 1.04, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), xjust=1, yjust=0, xpd = TRUE)
arrows(x, y3-y3.sd, x, y3+y3.sd, length=0.05, angle=90, code=3)
arrows(x, y4-y4.sd, x, y4+y4.sd, length=0.05, angle=90, code=3)
@




% ----------------------
% SECTION 3: Comparison
% ----------------------
\section{Our method vs. others}
We compare our method with other possible approaches based on some of the proposed schemes in the literature, that is PCA-based and clustering-based approaches. For the PCA-based approach, we implement a scheme similar to \cite{lak05}, in which the authors perform a \emph{Principal Component Analysis} (PCA) on test data to project it into the principal subspace and the $k$-means algorithm on the subspace to compare resulting clusters with the trained clusters of known anomalies, essentially in a semi-supervised manner. One difference in our approach is that we select the number of Principal Components based on the Zoski and Jurs' $b$ coefficient \cite{zos93, zos96}, which is a robust non-graphical alternative to Cattell's scree plots discussed in \cite{rai13}, to avoid inefficiency in manual examination through hundreds of scree plots. Another difference is that we replace $k$-means with $X$-means \cite{pel00} as to avoid, again, manually having to determine optimal number of clusters, $k$. Lastly the largest cluster is removed under an assumption that the size of such cluster should correspond to the high frequency of normal instances, therefore mostly contain non-intrusion instances. For the clustering-based approach, we use simply use the $X$-means algorithm and remove the largest cluster based on the same previously mentioned assumption. As a third comparison, we hoped to reproduce a subspace clustering approach proposed in \cite{cas12, cas14} because it is the most recent work with the most promising results and involves DBSCAN. However we failed to reproduce their work and decided to leave it out of the comparison. Random samples of the same seeds as our previous experiments are used to provide fair comparision between the methods. All the methods operate under the unsupervised learning paradigm to further assure comparability. The results are presented in Figure \ref{fig:compare}.

<<compare, dependson="sysResults", echo=FALSE, out.width="0.7\\linewidth", fig.show='hold', fig.align='center', fig.cap="Comparison with other methods">>=
load("~/../Desktop/masters-project/Data/System_results/pca.results.RData")
load("~/../Desktop/masters-project/Data/System_results/xmeans.results.RData")

## x-axis
w = 0.1  # width
x1.l = seq(1, 30, by = 5) + w
x1.r = seq(2, 29, by = 5) - w
x2.l = seq(2, 30, by = 5) + w
x2.r = seq(3, 31, by = 5) - w
x3.l = seq(3, 30, by = 5) + w
x3.r = seq(4, 31, by = 5) - w
## y-axis
## xmeans results
y5 = c(mean(xmeans.results.dat1.05a$TPR),mean(xmeans.results.dat1.10a$TPR),mean(xmeans.results.dat1.20a$TPR),mean(xmeans.results.dat1.30a$TPR),mean(xmeans.results.dat1.40a$TPR),mean(xmeans.results.dat1.50a$TPR))
y6 = c(mean(xmeans.results.dat1.05a$FPR),mean(xmeans.results.dat1.10a$FPR),mean(xmeans.results.dat1.20a$FPR),mean(xmeans.results.dat1.30a$FPR),mean(xmeans.results.dat1.40a$FPR),mean(xmeans.results.dat1.50a$FPR))
y5.sd = c(sd(xmeans.results.dat1.05a$TPR),sd(xmeans.results.dat1.10a$TPR),sd(xmeans.results.dat1.20a$TPR),sd(xmeans.results.dat1.30a$TPR),sd(xmeans.results.dat1.40a$TPR),sd(xmeans.results.dat1.50a$TPR))
y6.sd = c(sd(xmeans.results.dat1.05a$FPR),sd(xmeans.results.dat1.10a$FPR),sd(xmeans.results.dat1.20a$FPR),sd(xmeans.results.dat1.30a$FPR),sd(xmeans.results.dat1.40a$FPR),sd(xmeans.results.dat1.50a$FPR))
## pca results
y7 = c(mean(pca.results.dat1.05a$TPR),mean(pca.results.dat1.10a$TPR),mean(pca.results.dat1.20a$TPR),mean(pca.results.dat1.30a$TPR),mean(pca.results.dat1.40a$TPR),mean(pca.results.dat1.50a$TPR))
y8 = c(mean(pca.results.dat1.05a$FPR),mean(pca.results.dat1.10a$FPR),mean(pca.results.dat1.20a$FPR),mean(pca.results.dat1.30a$FPR),mean(pca.results.dat1.40a$FPR),mean(pca.results.dat1.50a$FPR))
y7.sd = c(sd(pca.results.dat1.05a$TPR),sd(pca.results.dat1.10a$TPR),sd(pca.results.dat1.20a$TPR),sd(pca.results.dat1.30a$TPR),sd(pca.results.dat1.40a$TPR),sd(pca.results.dat1.50a$TPR))
y8.sd = c(sd(pca.results.dat1.05a$FPR),sd(pca.results.dat1.10a$FPR),sd(pca.results.dat1.20a$FPR),sd(pca.results.dat1.30a$FPR),sd(pca.results.dat1.40a$FPR),sd(pca.results.dat1.50a$FPR))

## plot setup
x = 0:28
arw = 0.5 * (x1.r - x1.l)
arw.length = 0.03
arw.lwd = 0.8
sat = 0.3
TPR.col = grey.colors(1, 11.5/12)
FPR.col = grey.colors(1, 10/12)

## plot
par(mar = par()$mar+c(0,0,1,0))
plot(x, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Rates", type="n", main=expression(italic("Training dataset")))
legend(30.12, 1.04, xjust=1, yjust=0, xpd=TRUE, c("Our Method","X-Means-based","PCA-based","Respective FPR"), density=c(NA,NA,20,NA), angle=c(NA,NA,45,NA), fill=c(TPR.col,NA,"black",FPR.col))
axis(1, at = seq(2.5, 30, by=5), labels = c(0.05,0.1,0.2,0.3,0.4,0.5), tick = FALSE)
axis(1, at = seq(0, 30, by=5), labels = NA)

rect(x1.l, 0, x1.r, y1, col = TPR.col)
rect(x1.l, 0, x1.r, y2, col = FPR.col)
arrows(x1.l+arw, y1-y1.sd, x1.l+arw, y1+y1.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows(x1.l+arw, y2-y2.sd, x1.l+arw, y2+y2.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)

rect(x2.l, 0, x2.r, y5)
rect(x2.l, 0, x2.r, y6, col = FPR.col)
arrows(x2.l+arw, y5-y5.sd, x2.l+arw, y5+y5.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows(x2.l+arw, y6-y6.sd, x2.l+arw, y6+y6.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)

rect(x3.l, 0, x3.r, y7, col = "black", density=10, angle=45)
rect(x3.l, 0, x3.r, y8, col = FPR.col)
# arrows(x3.l+arw, y7-y7.sd, x3.l+arw, y7+y7.sd, length=arw.length, angle=90, code=3)
arrows(x3.l+arw, y7-y7.sd, x3.l+arw, 1, length=arw.length, angle=90, code=1)  ## goes over 1
# arrows(x3.l+arw, y8-y8.sd, x3.l+arw, y8+y8.sd, length=arw.length, angle=90, code=3)
arrows((x3.l+arw)[-c(5,6)], (y8-y8.sd)[-c(5,6)], (x3.l+arw)[-c(5,6)], (y8+y8.sd)[-c(5,6)], length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows((x3.l+arw)[c(5,6)], 0, (x3.l+arw)[c(5,6)], (y8+y8.sd)[c(5,6)], length=arw.length, angle=90, code=2, lwd=arw.lwd)  ## goes below 0


## dat2
y9 = c(mean(xmeans.results.dat2.05a$TPR),mean(xmeans.results.dat2.10a$TPR),mean(xmeans.results.dat2.20a$TPR),mean(xmeans.results.dat2.30a$TPR),mean(xmeans.results.dat2.40a$TPR),mean(xmeans.results.dat2.50a$TPR))
y10 = c(mean(xmeans.results.dat2.05a$FPR),mean(xmeans.results.dat2.10a$FPR),mean(xmeans.results.dat2.20a$FPR),mean(xmeans.results.dat2.30a$FPR),mean(xmeans.results.dat2.40a$FPR),mean(xmeans.results.dat2.50a$FPR))
y9.sd = c(sd(xmeans.results.dat2.05a$TPR),sd(xmeans.results.dat2.10a$TPR),sd(xmeans.results.dat2.20a$TPR),sd(xmeans.results.dat2.30a$TPR),sd(xmeans.results.dat2.40a$TPR),sd(xmeans.results.dat2.50a$TPR))
y10.sd = c(sd(xmeans.results.dat2.05a$FPR),sd(xmeans.results.dat2.10a$FPR),sd(xmeans.results.dat2.20a$FPR),sd(xmeans.results.dat2.30a$FPR),sd(xmeans.results.dat2.40a$FPR),sd(xmeans.results.dat2.50a$FPR))
## pca results
y11 = c(mean(pca.results.dat2.05a$TPR),mean(pca.results.dat2.10a$TPR),mean(pca.results.dat2.20a$TPR),mean(pca.results.dat2.30a$TPR),mean(pca.results.dat2.40a$TPR),mean(pca.results.dat2.50a$TPR))
y12 = c(mean(pca.results.dat2.05a$FPR),mean(pca.results.dat2.10a$FPR),mean(pca.results.dat2.20a$FPR),mean(pca.results.dat2.30a$FPR),mean(pca.results.dat2.40a$FPR),mean(pca.results.dat2.50a$FPR))
y11.sd = c(sd(pca.results.dat2.05a$TPR),sd(pca.results.dat2.10a$TPR),sd(pca.results.dat2.20a$TPR),sd(pca.results.dat2.30a$TPR),sd(pca.results.dat2.40a$TPR),sd(pca.results.dat2.50a$TPR))
y12.sd = c(sd(pca.results.dat2.05a$FPR),sd(pca.results.dat2.10a$FPR),sd(pca.results.dat2.20a$FPR),sd(pca.results.dat2.30a$FPR),sd(pca.results.dat2.40a$FPR),sd(pca.results.dat2.50a$FPR))

plot(x, ylim=c(0,1), xlab = "Intrusion Freq.", ylab="Rates", type="n", main=expression(italic("Test dataset")))
legend(30.12, 1.04, xjust=1, yjust=0, xpd=TRUE, c("Our Method","X-Means-based","PCA-based","Respective FPR"), density=c(NA,NA,20,NA), angle=c(NA,NA,45,NA), fill=c(TPR.col,NA,"black",FPR.col))
axis(1, at = seq(2.5, 30, by=5), labels = c(0.05,0.1,0.2,0.3,0.4,0.5), tick = FALSE)
axis(1, at = seq(0, 30, by=5), labels = NA)

rect(x1.l, 0, x1.r, y3, col = TPR.col)
rect(x1.l, 0, x1.r, y4, col = FPR.col)
arrows(x1.l+arw, y3-y3.sd, x1.l+arw, y3+y3.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows(x1.l+arw, y4-y4.sd, x1.l+arw, y4+y4.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)

rect(x2.l, 0, x2.r, y9)
rect(x2.l, 0, x2.r, y10, col = FPR.col)
arrows(x2.l+arw, y9-y9.sd, x2.l+arw, y9+y9.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows((x2.l+arw)[-c(3:5)], (y10-y10.sd)[-c(3:5)], (x2.l+arw)[-c(3:5)], (y10+y10.sd)[-c(3:5)], length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows((x2.l+arw)[3:5], 0, (x2.l+arw)[3:5], (y10+y10.sd)[3:5], length=arw.length, angle=90, code=2, lwd=arw.lwd)

rect(x3.l, 0, x3.r, y11, col = "black", density=10, angle=45)
rect(x3.l, 0, x3.r, y12, col = FPR.col)
# arrows(x3.l+arw, y11-y11.sd, x3.l+arw, y11+y11.sd, length=arw.length, angle=90, code=3)
arrows((x3.l+arw)[-c(5:6)], (y11-y11.sd)[-c(5:6)], (x3.l+arw)[-c(5:6)], (y11+y11.sd)[-c(5:6)], length=arw.length, angle=90, code=3, lwd=arw.lwd)  ## goes over 1
arrows((x3.l+arw)[c(5:6)], (y11-y11.sd)[c(5:6)], (x3.l+arw)[c(5:6)], 1, length=arw.length, angle=90, code=1, lwd=arw.lwd)
# arrows(x3.l+arw, y12-y12.sd, x3.l+arw, y12+y12.sd, length=arw.length, angle=90, code=3)
arrows(x3.l+arw, 0, x3.l+arw, y12+y12.sd, length=arw.length, angle=90, code=2, lwd=arw.lwd)  ## goes below 0
@

For the \emph{training} dataset, our method appears to be more precise than the other methods as indicated by the small variation. This is a promising piece of information to claim that our method functions exactly as designed and is very robust as the performance is reliable and independent of any discrepancies between the random samples.  The accuracy of our method is also reasonably comparable to the others, especially as it manages to retain the FPRs at a relatively low level. One exception is when the intrusion frequency reaches at 0.5, and we have already established that such frequency of intrusion instances may be too high for any anomaly detection schemes. The simple $X$-means-based method produced unexpectedly high TPRs but their respective FPRs are generally too high to be reliable. A steady decrease in the FPRs towards a higher intrusion frequency can be observed to suggest that the $X$-means-based method may be a more reliable alternative for balanced data. However one shortcoming of the $X$-means algorithm that it favours balanced data must be carefully noted here. In any case, our method outperforms the $X$-means-based method at the lower intrusion frequencies. The PCA-based approach is too imprecise to be a reliable method, even though \emph{most} of the runs produced reasonably high TPRs, on average, to bring up the overall detection rate. Because the PCA-based approach is just an extended version of the $X$-means approach that implements PCA, we can deduce that the PCA part solely explains the poor performance. The ineffectiveness of PCA means that the information contained in the data cannot be effectively summarised in lower dimensions and the dimensionality reduction using linear transformation is inefficient for the data. Our conclusion from this finding is that the shape of the data cloud is too spherical to find a suitable subspace that explains a sufficient amount of the information contained in the data as an acceptable alternative representation in lower dimensions. The \emph{test} dataset presents similar patterns in accordance with the previously examined findings.

To conclude, our method is the most reliable out of all three approaches as long as intrusion frequencies are not overly high. The expected outcome of our method can be dependent on the type of data, in terms of densities, but the good side is that resulting FPs will be kept at a moderately low level. The promise of low false alarm rate is valuable as users will be further assured that the detected intrusion instances are that much more likely to be what they wish to catch.



%' A range of different values for the input parameter of the LOF algorithm, $k$, is tested. Assuming there is not enough domain knowledge and the most appropriate value for $k$ is not known, we have specified the range of $k$ to be tested as percentage, relative to the size of a test dataset, at 10, 20, 30 and 40 percents of the test dataset size. We refer to the percentages of $k$ as $\%k$ (percentage $k$).
%' 
%' The rest of this section is divided into four, each of which presents the density plots of LOF scores at different $\%k$ and the means and standard deviations of detection and false positive rates from one hundred repeated experiments for fixed percentage of intrusions.
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 10\% intrusions}
%' The test dataset used in this section is composed of 10\% intrusion and 90\% non-intrusion instances. Figure \ref{fig:attack10} represents the density plots at the four different percentages of $k$ at fixed 10\% intrusions. The means of thresholds, detection rates and false positive rates are tabulated, along with thier standard deviations in brackets, in Table \ref{evaluation:tab1}.
%' 
%' <<attack10, cache=TRUE, echo = FALSE, fig.align='center', fig.cap="Estimated density plots of LOF scores using KDE at different $\\%k$", out.width="0.45\\linewidth", fig.show='hold', fig.pos="h">>=
%' require(scales)
%' ## 10% attack, 10 %k
%' scores1 = as.matrix(read.csv("phase1_scores-10a-10k.csv", header = FALSE))
%' plot(density(scores1[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 10 %k")
%' invisible(lapply(2:nrow(scores1), function(i) lines(density(scores1[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 10% attack, 20 %k
%' scores2 = as.matrix(read.csv("phase1_scores-10a-20k.csv", header = FALSE))
%' plot(density(scores2[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 20 %k")
%' invisible(lapply(2:nrow(scores2), function(i) lines(density(scores2[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 10% attack, 30 %k
%' scores3 = as.matrix(read.csv("phase1_scores-10a-30k.csv", header = FALSE))
%' plot(density(scores3[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 30 %k")
%' invisible(lapply(2:nrow(scores3), function(i) lines(density(scores3[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 10% attack, 430 %k
%' scores4 = as.matrix(read.csv("phase1_scores-10a-40k.csv", header = FALSE))
%' plot(density(scores4[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 40 %k")
%' invisible(lapply(2:nrow(scores4), function(i) lines(density(scores4[i, ]), col = alpha("black", 0.1))))
%' @
%' 
%' <<phase1-data, cache=TRUE, echo = FALSE, results='asis'>>=
%' load(file = "phase1_evaluation.RData")
%' @
%' 
%' <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' options(digits = 3)
%' library(xtable)
%' tab = data.frame(rbind(c(paste0(round(a10_10k$threshold[1], 3), " (", round(a10_10k$threshold[2], 2), ")"),
%'                          paste0(round(a10_20k$threshold[1], 3), " (", round(a10_20k$threshold[2], 2), ")"),
%'                          paste0(round(a10_30k$threshold[1], 3), " (", round(a10_30k$threshold[2], 2), ")"),
%'                          paste0(round(a10_40k$threshold[1], 3), " (", round(a10_40k$threshold[2], 2), ")")),
%'                        c(paste0(round(a10_10k$detection.rate[1], 3), " (", round(a10_10k$detection.rate[2], 2), ")"),
%'                          paste0(round(a10_20k$detection.rate[1], 3), " (", round(a10_20k$detection.rate[2], 2), ")"),
%'                          paste0(round(a10_30k$detection.rate[1], 3), " (", round(a10_30k$detection.rate[2], 2), ")"),
%'                          paste0(round(a10_40k$detection.rate[1], 3), " (", round(a10_40k$detection.rate[2], 2), ")")),
%'                        c(paste0(round(a10_10k$false.alarm[1], 3), " (", round(a10_10k$false.alarm[2], 2), ")"),
%'                          paste0(round(a10_20k$false.alarm[1], 3), " (", round(a10_20k$false.alarm[2], 2), ")"),
%'                          paste0(round(a10_30k$false.alarm[1], 3), " (", round(a10_30k$false.alarm[2], 2), ")"),
%'                          paste0(round(a10_40k$false.alarm[1], 3), " (", round(a10_40k$false.alarm[2], 2), ")"))))
%' 
%' colnames(tab) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' rownames(tab) = c("threshold", "detection rate", "false alarm rate")
%' xtab        = xtable(tab, caption = "Threshold, detection rate and false alarm rate at 10\\% intrusions", label = "evaluation:tab1", digits = 3)
%' align(xtab) = "r|llll"
%' print(xtab, booktabs = TRUE, caption.placement = "top")
%' @
%' 
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 20\% intrusions}
%' The test dataset used in this section is composed of 20\% intrusion and 80\% non-intrusion instances. Figure \ref{fig:attack20} represents the density plots at the different percentages of $k$  at fixed 20\% intrusions. The means of thresholds, detection rates and false positive rates are tabulated, along with thier standard deviations in brackets, in Table \ref{evaluation:tab2}.
%' 
%' <<attack20, cache=TRUE, echo = FALSE, fig.align='center', fig.cap="Estimated density plots of LOF scores using KDE at different $\\%k$", out.width="0.45\\linewidth", fig.show='hold', fig.pos="h">>=
%' require(scales)
%' ## 20% attack, 10 %k
%' scores1 = as.matrix(read.csv("phase1_scores-20a-10k.csv", header = FALSE))
%' plot(density(scores1[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 10 %k")
%' invisible(lapply(2:nrow(scores1), function(i) lines(density(scores1[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 20% attack, 20 %k
%' scores2 = as.matrix(read.csv("phase1_scores-20a-20k.csv", header = FALSE))
%' plot(density(scores2[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 20 %k")
%' invisible(lapply(2:nrow(scores2), function(i) lines(density(scores2[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 20% attack, 30 %k
%' scores3 = as.matrix(read.csv("phase1_scores-20a-30k.csv", header = FALSE))
%' plot(density(scores3[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 30 %k")
%' invisible(lapply(2:nrow(scores3), function(i) lines(density(scores3[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 20% attack, 430 %k
%' scores4 = as.matrix(read.csv("phase1_scores-20a-40k.csv", header = FALSE))
%' plot(density(scores4[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 40 %k")
%' invisible(lapply(2:nrow(scores4), function(i) lines(density(scores4[i, ]), col = alpha("black", 0.1))))
%' @
%' 
%' <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' options(digits = 3)
%' library(xtable)
%' tab = data.frame(rbind(c(paste0(round(a20_10k$threshold[1], 3), " (", round(a20_10k$threshold[2], 2), ")"),
%'                          paste0(round(a20_20k$threshold[1], 3), " (", round(a20_20k$threshold[2], 2), ")"),
%'                          paste0(round(a20_30k$threshold[1], 3), " (", round(a20_30k$threshold[2], 2), ")"),
%'                          paste0(round(a20_40k$threshold[1], 3), " (", round(a20_40k$threshold[2], 2), ")")),
%'                        c(paste0(round(a20_10k$detection.rate[1], 3), " (", round(a20_10k$detection.rate[2], 2), ")"),
%'                          paste0(round(a20_20k$detection.rate[1], 3), " (", round(a20_20k$detection.rate[2], 2), ")"),
%'                          paste0(round(a20_30k$detection.rate[1], 3), " (", round(a20_30k$detection.rate[2], 2), ")"),
%'                          paste0(round(a20_40k$detection.rate[1], 3), " (", round(a20_40k$detection.rate[2], 2), ")")),
%'                        c(paste0(round(a20_10k$false.alarm[1], 3), " (", round(a20_10k$false.alarm[2], 2), ")"),
%'                          paste0(round(a20_20k$false.alarm[1], 3), " (", round(a20_20k$false.alarm[2], 2), ")"),
%'                          paste0(round(a20_30k$false.alarm[1], 3), " (", round(a20_30k$false.alarm[2], 2), ")"),
%'                          paste0(round(a20_40k$false.alarm[1], 3), " (", round(a20_40k$false.alarm[2], 2), ")"))))
%' 
%' colnames(tab) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' rownames(tab) = c("threshold", "detection rate", "false alarm rate")
%' xtab        = xtable(tab, caption = "Threshold, detection rate and false alarm rate at 20\\% intrusions", label = "evaluation:tab2", digits = 3)
%' align(xtab) = "r|llll"
%' print(xtab, booktabs = TRUE, caption.placement = "top")
%' @
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 30\% intrusions}
%' The test dataset used in this section is composed of 30\% intrusion and 70\% non-intrusion instances. Figure \ref{fig:attack30} represents the density plots at the different percentages of $k$  at fixed 30\% intrusions. The means of thresholds, detection rates and false positive rates are tabulated, along with thier standard deviations in brackets, in Table \ref{evaluation:tab3}.
%' 
%' <<attack30, cache=TRUE, echo = FALSE, fig.align='center', fig.cap="Estimated density plots of LOF scores using KDE at different $\\%k$", out.width="0.45\\linewidth", fig.show='hold', fig.pos="h">>=
%' require(scales)
%' ## 30% attack, 10 %k
%' scores1 = as.matrix(read.csv("phase1_scores-30a-10k.csv", header = FALSE))
%' plot(density(scores1[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 10 %k")
%' invisible(lapply(2:nrow(scores1), function(i) lines(density(scores1[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 30% attack, 20 %k
%' scores2 = as.matrix(read.csv("phase1_scores-30a-20k.csv", header = FALSE))
%' plot(density(scores2[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 20 %k")
%' invisible(lapply(2:nrow(scores2), function(i) lines(density(scores2[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 30% attack, 30 %k
%' scores3 = as.matrix(read.csv("phase1_scores-30a-30k.csv", header = FALSE))
%' plot(density(scores3[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 30 %k")
%' invisible(lapply(2:nrow(scores3), function(i) lines(density(scores3[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 30% attack, 430 %k
%' scores4 = as.matrix(read.csv("phase1_scores-30a-40k.csv", header = FALSE))
%' plot(density(scores4[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 40 %k")
%' invisible(lapply(2:nrow(scores4), function(i) lines(density(scores4[i, ]), col = alpha("black", 0.1))))
%' @
%' 
%' <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' options(digits = 3)
%' library(xtable)
%' tab = data.frame(rbind(c(paste0(round(a30_10k$threshold[1], 3), " (", round(a30_10k$threshold[2], 2), ")"),
%'                          paste0(round(a30_20k$threshold[1], 3), " (", round(a30_20k$threshold[2], 2), ")"),
%'                          paste0(round(a30_30k$threshold[1], 3), " (", round(a30_30k$threshold[2], 2), ")"),
%'                          paste0(round(a30_40k$threshold[1], 3), " (", round(a30_40k$threshold[2], 2), ")")),
%'                        c(paste0(round(a30_10k$detection.rate[1], 3), " (", round(a30_10k$detection.rate[2], 2), ")"),
%'                          paste0(round(a30_20k$detection.rate[1], 3), " (", round(a30_20k$detection.rate[2], 2), ")"),
%'                          paste0(round(a30_30k$detection.rate[1], 3), " (", round(a30_30k$detection.rate[2], 2), ")"),
%'                          paste0(round(a30_40k$detection.rate[1], 3), " (", round(a30_40k$detection.rate[2], 2), ")")),
%'                        c(paste0(round(a30_10k$false.alarm[1], 3), " (", round(a30_10k$false.alarm[2], 2), ")"),
%'                          paste0(round(a30_20k$false.alarm[1], 3), " (", round(a30_20k$false.alarm[2], 2), ")"),
%'                          paste0(round(a30_30k$false.alarm[1], 3), " (", round(a30_30k$false.alarm[2], 2), ")"),
%'                          paste0(round(a30_40k$false.alarm[1], 3), " (", round(a30_40k$false.alarm[2], 2), ")"))))
%' 
%' colnames(tab) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' rownames(tab) = c("threshold", "detection rate", "false alarm rate")
%' xtab        = xtable(tab, caption = "Threshold, detection rate and false alarm rate at 30\\% intrusions", label = "evaluation:tab3", digits = 3)
%' align(xtab) = "r|llll"
%' print(xtab, booktabs = TRUE, caption.placement = "top")
%' @
%' 
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 40\% intrusions}
%' The test dataset used in this section is composed of 40\% intrusion and 60\% non-intrusion instances. Figure \ref{fig:attack40} represents the density plots at the different percentages of $k$ at fixed 40\% intrusions. The means of thresholds, detection rates and false positive rates are tabulated, along with thier standard deviations in brackets, in Table \ref{evaluation:tab4}.
%' 
%' <<attack40, cache=TRUE, echo = FALSE, fig.align='center', fig.cap="Estimated density plots of LOF scores using KDE at different $\\%k$", out.width="0.45\\linewidth", fig.show='hold', fig.pos="h">>=
%' require(scales)
%' ## 40% attack, 10 %k
%' scores1 = as.matrix(read.csv("phase1_scores-40a-10k.csv", header = FALSE))
%' plot(density(scores1[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 10 %k")
%' invisible(lapply(2:nrow(scores1), function(i) lines(density(scores1[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 40% attack, 20 %k
%' scores2 = as.matrix(read.csv("phase1_scores-40a-20k.csv", header = FALSE))
%' plot(density(scores2[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 20 %k")
%' invisible(lapply(2:nrow(scores2), function(i) lines(density(scores2[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 40% attack, 30 %k
%' scores3 = as.matrix(read.csv("phase1_scores-40a-30k.csv", header = FALSE))
%' plot(density(scores3[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 30 %k")
%' invisible(lapply(2:nrow(scores3), function(i) lines(density(scores3[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 40% attack, 430 %k
%' scores4 = as.matrix(read.csv("phase1_scores-40a-40k.csv", header = FALSE))
%' plot(density(scores4[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 40 %k")
%' invisible(lapply(2:nrow(scores4), function(i) lines(density(scores4[i, ]), col = alpha("black", 0.1))))
%' @
%' 
%' <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' options(digits = 3)
%' library(xtable)
%' tab = data.frame(rbind(c(paste0(round(a40_10k$threshold[1], 3), " (", round(a40_10k$threshold[2], 2), ")"),
%'                          paste0(round(a40_20k$threshold[1], 3), " (", round(a40_20k$threshold[2], 2), ")"),
%'                          paste0(round(a40_30k$threshold[1], 3), " (", round(a40_30k$threshold[2], 2), ")"),
%'                          paste0(round(a40_40k$threshold[1], 3), " (", round(a40_40k$threshold[2], 2), ")")),
%'                        c(paste0(round(a40_10k$detection.rate[1], 3), " (", round(a40_10k$detection.rate[2], 2), ")"),
%'                          paste0(round(a40_20k$detection.rate[1], 3), " (", round(a40_20k$detection.rate[2], 2), ")"),
%'                          paste0(round(a40_30k$detection.rate[1], 3), " (", round(a40_30k$detection.rate[2], 2), ")"),
%'                          paste0(round(a40_40k$detection.rate[1], 3), " (", round(a40_40k$detection.rate[2], 2), ")")),
%'                        c(paste0(round(a40_10k$false.alarm[1], 3), " (", round(a40_10k$false.alarm[2], 2), ")"),
%'                          paste0(round(a40_20k$false.alarm[1], 3), " (", round(a40_20k$false.alarm[2], 2), ")"),
%'                          paste0(round(a40_30k$false.alarm[1], 3), " (", round(a40_30k$false.alarm[2], 2), ")"),
%'                          paste0(round(a40_40k$false.alarm[1], 3), " (", round(a40_40k$false.alarm[2], 2), ")"))))
%' 
%' colnames(tab) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' rownames(tab) = c("threshold", "detection rate", "false alarm rate")
%' xtab        = xtable(tab, caption = "Threshold, detection rate and false alarm rate at 40\\% intrusions", label = "evaluation:tab4", digits = 3)
%' align(xtab) = "r|llll"
%' print(xtab, booktabs = TRUE, caption.placement = "top")
%' @
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' % %The choice of the input parameter, $k$, heavily depends on the application domain and prior knowledge about data. Without knowing the actual sizes, or the local densities, of possible clusters in the data, $k$ can only be set arbitrarily. According to the authors of the LOF algorithm, $k$ must be large enough to avoid any statistical fluctuations in the computed scores. Also $k$ must be less than the size of the entire dataset to avoid including all the instances as the neighbourhood. What can be deducted from the examples given in the literature is that the choice of $k$ is related to the size of data that the algorithm is applied to. Therefore we have decided to set $k$ to be proportional to the size of a testset such that $k$ is appropriately high for a large dataset and vice versa.
%' % 
%' % We have tested the performance of LOF on four different values of $k$, namely 500, 1,000, 1,500 and 2,000, that is 10\%, 20\%, 30\% and 40\% of the testset size of 5,000. The amount of intrusion instances, as denoted by \emph{p.attack}, in the testset is also varied to assess whether the adequacy of $k$ depends on the number of intrusion instances in the testset. Tables \ref{tab:phase1-detection}, \ref{tab:phase1-false} and \ref{tab:phase1-threshold} present the means of the detection and false alarm rates and thresholds from the 100 repeated experiments. The corresponding standard deviations are represented in brackets.
%' % 
%' % <<phase1-data, cache=TRUE, echo = FALSE, results='asis'>>=
%' % load(file = "phase1_evaluation.RData")
%' % @
%' % 
%' % <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' % library(xtable)
%' % detection.rate = data.frame(rbind(
%' %     c(paste0(round(a10_10k$detection.rate[1], 3), " (", round(a10_10k$detection.rate[2], 2), ")"),
%' %       paste0(round(a10_20k$detection.rate[1], 3), " (", round(a10_20k$detection.rate[2], 2), ")"),
%' %       paste0(round(a10_30k$detection.rate[1], 3), " (", round(a10_30k$detection.rate[2], 2), ")"),
%' %       paste0(round(a10_40k$detection.rate[1], 3), " (", round(a10_40k$detection.rate[2], 2), ")")),
%' %     
%' %     c(paste0(round(a20_10k$detection.rate[1], 3), " (", round(a20_10k$detection.rate[2], 2), ")"),
%' %       paste0(round(a20_20k$detection.rate[1], 3), " (", round(a20_20k$detection.rate[2], 2), ")"),
%' %       paste0(round(a20_30k$detection.rate[1], 3), " (", round(a20_30k$detection.rate[2], 2), ")"),
%' %       paste0(round(a20_40k$detection.rate[1], 3), " (", round(a20_40k$detection.rate[2], 2), ")")),
%' %     
%' %     c(paste0(round(a30_10k$detection.rate[1], 3), " (", round(a30_10k$detection.rate[2], 2), ")"),
%' %       paste0(round(a30_20k$detection.rate[1], 3), " (", round(a30_20k$detection.rate[2], 2), ")"),
%' %       paste0(round(a30_30k$detection.rate[1], 3), " (", round(a30_30k$detection.rate[2], 2), ")"),
%' %       paste0(round(a30_40k$detection.rate[1], 3), " (", round(a30_40k$detection.rate[2], 2), ")")),
%' %     
%' %     c(paste0(round(a40_10k$detection.rate[1], 3), " (", round(a40_10k$detection.rate[2], 2), ")"),
%' %       paste0(round(a40_20k$detection.rate[1], 3), " (", round(a40_20k$detection.rate[2], 2), ")"),
%' %       paste0(round(a40_30k$detection.rate[1], 3), " (", round(a40_30k$detection.rate[2], 2), ")"),
%' %       paste0(round(a40_40k$detection.rate[1], 3), " (", round(a40_40k$detection.rate[2], 2), ")"))
%' % ))
%' % 
%' % colnames(detection.rate) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' % rownames(detection.rate) = c("p.attack = 10%", "p.attack = 20%", "p.attack = 30%", "p.attack = 40%")
%' % 
%' % xtab        = xtable(detection.rate, caption = "Detection rate", label = "tab:phase1-detection")
%' % align(xtab) = "r|llll"
%' % print(xtab, booktabs = TRUE)
%' % @
%' % 
%' % <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' % library(xtable)
%' % false.alarm = data.frame(rbind(
%' %     c(paste0(round(a10_10k$false.alarm[1], 3), " (", round(a10_10k$false.alarm[2], 2), ")"),
%' %       paste0(round(a10_20k$false.alarm[1], 3), " (", round(a10_20k$false.alarm[2], 2), ")"),
%' %       paste0(round(a10_30k$false.alarm[1], 3), " (", round(a10_30k$false.alarm[2], 2), ")"),
%' %       paste0(round(a10_40k$false.alarm[1], 3), " (", round(a10_40k$false.alarm[2], 2), ")")),
%' %     
%' %     c(paste0(round(a20_10k$false.alarm[1], 3), " (", round(a20_10k$false.alarm[2], 2), ")"),
%' %       paste0(round(a20_20k$false.alarm[1], 3), " (", round(a20_20k$false.alarm[2], 2), ")"),
%' %       paste0(round(a20_30k$false.alarm[1], 3), " (", round(a20_30k$false.alarm[2], 2), ")"),
%' %       paste0(round(a20_40k$false.alarm[1], 3), " (", round(a20_40k$false.alarm[2], 2), ")")),
%' %     
%' %     c(paste0(round(a30_10k$false.alarm[1], 3), " (", round(a30_10k$false.alarm[2], 2), ")"),
%' %       paste0(round(a30_20k$false.alarm[1], 3), " (", round(a30_20k$false.alarm[2], 2), ")"),
%' %       paste0(round(a30_30k$false.alarm[1], 3), " (", round(a30_30k$false.alarm[2], 2), ")"),
%' %       paste0(round(a30_40k$false.alarm[1], 3), " (", round(a30_40k$false.alarm[2], 2), ")")),
%' %     
%' %     c(paste0(round(a40_10k$false.alarm[1], 3), " (", round(a40_10k$false.alarm[2], 2), ")"),
%' %       paste0(round(a40_20k$false.alarm[1], 3), " (", round(a40_20k$false.alarm[2], 2), ")"),
%' %       paste0(round(a40_30k$false.alarm[1], 3), " (", round(a40_30k$false.alarm[2], 2), ")"),
%' %       paste0(round(a40_40k$false.alarm[1], 3), " (", round(a40_40k$false.alarm[2], 2), ")"))
%' % ))
%' % 
%' % colnames(false.alarm) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' % rownames(false.alarm) = c("p.attack = 10%", "p.attack = 20%", "p.attack = 30%", "p.attack = 40%")
%' % 
%' % xtab        = xtable(false.alarm, caption = "False alarm rate", label = "tab:phase1-false")
%' % align(xtab) = "r|llll"
%' % print(xtab, booktabs = TRUE)
%' % @
%' % 
%' % <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' % library(xtable)
%' % threshold = data.frame(rbind(
%' %     c(paste0(round(a10_10k$threshold[1], 3), " (", round(a10_10k$threshold[2], 2), ")"),
%' %       paste0(round(a10_20k$threshold[1], 3), " (", round(a10_20k$threshold[2], 2), ")"),
%' %       paste0(round(a10_30k$threshold[1], 3), " (", round(a10_30k$threshold[2], 2), ")"),
%' %       paste0(round(a10_40k$threshold[1], 3), " (", round(a10_40k$threshold[2], 2), ")")),
%' %     
%' %     c(paste0(round(a20_10k$threshold[1], 3), " (", round(a20_10k$threshold[2], 2), ")"),
%' %       paste0(round(a20_20k$threshold[1], 3), " (", round(a20_20k$threshold[2], 2), ")"),
%' %       paste0(round(a20_30k$threshold[1], 3), " (", round(a20_30k$threshold[2], 2), ")"),
%' %       paste0(round(a20_40k$threshold[1], 3), " (", round(a20_40k$threshold[2], 2), ")")),
%' %     
%' %     c(paste0(round(a30_10k$threshold[1], 3), " (", round(a30_10k$threshold[2], 2), ")"),
%' %       paste0(round(a30_20k$threshold[1], 3), " (", round(a30_20k$threshold[2], 2), ")"),
%' %       paste0(round(a30_30k$threshold[1], 3), " (", round(a30_30k$threshold[2], 2), ")"),
%' %       paste0(round(a30_40k$threshold[1], 3), " (", round(a30_40k$threshold[2], 2), ")")),
%' %     
%' %     c(paste0(round(a40_10k$threshold[1], 3), " (", round(a40_10k$threshold[2], 2), ")"),
%' %       paste0(round(a40_20k$threshold[1], 3), " (", round(a40_20k$threshold[2], 2), ")"),
%' %       paste0(round(a40_30k$threshold[1], 3), " (", round(a40_30k$threshold[2], 2), ")"),
%' %       paste0(round(a40_40k$threshold[1], 3), " (", round(a40_40k$threshold[2], 2), ")"))
%' % ))
%' % 
%' % colnames(threshold) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' % rownames(threshold) = c("p.attack = 10%", "p.attack = 20%", "p.attack = 30%", "p.attack = 40%")
%' % 
%' % xtab        = xtable(threshold, caption = "Threshold", label = "tab:phase1-threshold")
%' % align(xtab) = "r|llll"
%' % print(xtab, booktabs = TRUE)
%' % @
%' % 
%' % From the tables, a few observations can be made:
%' % 
%' % \begin{itemize}
%' % 
%' % \item As $k$ increases, the detection and false positive rates as well as the thresholds are more consistent, as shown by the decreases in their respective standard deviations. This suggests that $k$ should be adequately large to produce more stable results.
%' % 
%' % \item As the number of intrusion instances, denoted by \emph{p.attack}, increases in the testset, the false alarm rates at $k = 10\%$ seems to increase. This mildly suggests that the LOF algorithm for this particular instance has performed well when the ratio between the numbers of intrusion and non-intrusion instances are further apart. However, the other rates vary widely between the different values of p.attack to support this claim.
%' % 
%' % \item The thresholds fluctuate widely for the different testsets. This is expected as our threshold is set to be adaptive to the density curve of the computed scores.
%' % 
%' % \item The performance seems to be at best for the case where p.attack = 10\% and $k = 40\%$, as indicated by the highest detection rate and the lowest false alarm rate. The density curves of the 100 sets of LOF scores can be seen in Figure \ref{fig:phase1-eval-density}.
%' % 
%' % \end{itemize}
%' % 
%' % 
%' % <<phase1-eval-density, echo=FALSE, fig.align='center', fig.cap='The density curves of the 100 sets of the LOF scores computed for the testset of 4,500 non-intrusion and 500 intrusion instances at k = 2,000.', out.width="0.6\\linewidth">>=
%' % a10_40k_scores = read.csv("phase1_scores-10a-40k.csv")
%' % a10_40k_scores = as.matrix(a10_40k_scores)
%' % 
%' % library(scales)
%' % plot(density(a10_40k_scores[1, ]), ylim = c(0, 4), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density curves for 100 sets of LOF scores")
%' % for (i in 2:99) {
%' %     lines(density(a10_40k_scores[i, ]), col = alpha("black", 0.1))
%' % }
%' % @
%' % 
%' % %The input parameter, $k$, is set to be proportional to the size of the testset. We have tested the performance of LOF using $k$ values at 10\%, 20\%, 30\% and 40\% of the testset size, that is 500, 1,000, 1,500 and 2,000 respectively. The initial $k$ value of 500 may seem rather large but our intention is to avoid any statistical inconsistency by setting it too low, as mentioned by the authors of the LOF algorithm. Four different types of testsets are used, where the number of intrusion instances is varied between 2,000, 1,500, 1,000 and 500.
%' % 
%' % % As discussed by the authors of the LOF algorithm, a set of LOF scores can fluctuate non-monotonically depending on the choice of the input parameter, $k$. What they have found was that the variation, measured by the standard deviation, in the scores computed for a random uniform sample stabilises for $k$ greater than 10. From this, they suggested a heuristic that $k$ should be greater than 10 to avoid any statistical inconsistency in the computed LOF scores. On the other hand, if $k$ is set to be too large, non-outliers can end up being detected as outliers because the size of the neighbourhood increases to potentially include non-outliers from other clusters as $k$ increases. The authors' suggestion is to compute LOF scores over a range of $k$ and take the maximum of the scores to report on the most outlying scores.
%' % % 
%' % % The difficulty to proceed with their heuristic is the computation time and memory resource consumption, both of which depend on the size of $k$ range. If $k$ is set to be over a wide range, the LOF algorithm is going to occur that much more compared to over a narrow range.
%' % % 
%' % % Hence, the size 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' % uncomment the subsection below:
%' % -----------------
%' % SUB-SECTION: 3.2
%' % -----------------
%' \subsection{Phase 2}
%' \label{evaluation:sec3.2}
%' 
%' Since the aim of the DBSCAN clustering technique used in Phase 2 is to categorise instances belonging to the same attack type into each cluster, it is of the most interest to evaluate whether the members of each cluster belong to one particular type of attacks. Hence the externel criterion of clustering quality, \emph{purity}, is measured, that is given by
%' \[ purity = \frac{1}{N} \sum_{i=1}^{k} max_j |c_i \cap t_j |, \]
%' where $N$ is the number of objects, $k$ is the number of clusters, $c_i$ is a cluster in $C$ and $t_j$ is the class $j$. Because purity is an average measure aggregated over all clusters, the presence of relatively large clusters can result in an increased purity. To avoid the bias, clustering purity is computed at an individual cluster level for Phase 2 by
%' \[ purity_i = \frac{1}{n_i} max_j |c_i \cap t_j |, \]
%' for the $i$th cluster.
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 10\% intrusions}
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 20\% intrusions}
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 30\% intrusions}
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 40\% intrusions}

% As an example, the most frequent class, its count, the size and the purity computed for the first iteration of the repeated experiment is presented in Table \ref{tab:phase2-first1}. The classical purity measure is $\frac{1}{1719}(349+957+261+28) = 0.9279$, which is slightly greater than our approach of $\frac{1}{4}(0.751+0.994+1+0.933) = 0.9195$. The standard deviation of our purity measure is about 0.1163, which gives us a measure of consistency between the individual purity.
% 
% <<phase2-data, echo=FALSE, cache=TRUE>>=
% load(file = "phase2_evaluation.RData")
% @
% 
% <<cache=TRUE, dependson="phase2-data", echo=FALSE, results='asis'>>=
% library(xtable)
% tab = rbind(phase2_results$max_class[1, ], c(349,957,261,28), phase2_results$size[1, ], round(phase2_results$purity[1, ], 3))
% rownames(tab) = c("$class", "$count", "$size", "$purity")
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first1")
% align(xtab) = "r|llll"
% print(xtab, booktabs = TRUE)
% @
% 
% Table \ref{tab:phase2-first2} presents the Euclidean distance to the centroid of the non-intrusion instances from the centroid of each cluster for the same iteration. Since the second cluster is the closest to the centroid of the non-intrusion instances, it is removed and the remaining three clusters will form the set of detected intrusions. The third cluster, which is purely of the non-intrusion instances, is counterintuitively distant from the non-intrusion centroid. It is more distant than the first cluster of the Smurf attacks and less distant than the fourth cluster of the Neptune attacks. It could be possible that the non-intrusion instances in this cluster are rather behaving as if they are anomalies. A simple example would be that a user who has forgotten his or her password could show similar behaviour as a dictionary attack by multiple attempts at logging into a system.
% 
% <<cache=TRUE, dependson="phase2-data", echo=FALSE, results='asis'>>=
% library(xtable)
% tab = matrix(round(phase2_results$distances[1, ], 3), nrow = 1)
% rownames(tab) = "$distance"
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first2")
% align(xtab) = "r|llll"
% print(xtab, booktabs = TRUE)
% @
% 
% Finally, the mean and the standard deviation of 100 sets of purity measures are presented in Table \ref{tab:phase2-first3}.
% 
% <<cache=TRUE, dependson="phase2-data", echo=FALSE, results='asis'>>=
% library(xtable)
% tab = matrix(c(round(mean(phase2_results$purity), 4), round(sd(phase2_results$purity), 4)), nrow = 1)
% rownames(tab) = "$overall.purity"
% colnames(tab) = c("mean", "sd")
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first3", digits = 4)
% align(xtab) = "r|ll"
% print(xtab, booktabs = TRUE)
% @

% 
% 
% %----------------------------------------------------------
% \section{System Evaluation}
% \label{sec:evaluation-system}
% 
% Table \ref{tab:phase2-first3} is the result of the overall system evaluation. There are 61 occasions where the clusters of non-intrusion instances are correctly identified by being the closest to the non-intrusion centroid to result in the detection rates that are all above 96\%. For the remaining 39 occasions, the clusters are incorrectly identified to result in the detection rates below 81\%. For these 39 cases, the clusters of intrusions are identified as being the closest to the non-intrusion centroid while the actual clusters of non-intrusion instances that we want to identify are further away. The reason is because there are multiple clusters of non-intrusion instances present in the dataset. These non-intrusion instances behave abnormally relative to the majority of the other non-intrusion instances to the extent that they are detected as possible anomalies.
% 
% <<cache=TRUE, dependson="phase2-data", echo=FALSE, results='asis'>>=
% library(xtable)
% tab = matrix(c(round(mean(phase2_results$detection.rate), 4), round(sd(phase2_results$detection.rate), 4),
%                round(mean(phase2_results$false.alarm.rate), 4), round(sd(phase2_results$false.alarm.rate), 4)), nrow = 2, byrow = TRUE)
% rownames(tab) = c("detection rate", "false alarm rate")
% colnames(tab) = c("mean", "sd")
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first3", digits = 4)
% align(xtab) = "r|ll"
% print(xtab, booktabs = TRUE)
% @
% 
% The same experiment is conducted on the test dataset of KDD'99 where there are 14 unique attack types. The result is presented in Table \ref{tab:phase2-first4}. The similar situation mentioned previously is also present for the results using the test dataset.
% 
% <<echo=FALSE, results='asis'>>=
% library(xtable)
% load(file = "system_testset.RData")
% tab = t(test_testdata$table)
% tab[1, 1] = as.numeric(tab[1, 1])
% tab[2, 1] = as.numeric(tab[2, 1])
% tab[1, 2] = as.numeric(gsub("[(]|[)]", "", tab[1, 2]))
% tab[2, 2] = as.numeric(gsub("[(]|[)]", "", tab[2, 2]))
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first3", digits = 4)
% align(xtab) = "r|ll"
% print(xtab, booktabs = TRUE)
% @
% 
% % \subsection{Experimental Results}
% % 
% % <<load_results, echo=FALSE>>=
% % load(file = "evaluation.results.RData")
% % @
% % 
% % 
% % \subsubsection{Phase 1}
% % 
% % \begin{itemize}
% % \item Detection rate from LOF
% % \item False alarm rate from LOF
% % \item Computation time
% % \end{itemize}
% % 
% % \subsubsection{Phase 2}
% % 
% % \begin{itemize}
% % \item Average purity of clusters and s.d.
% % \item Rate of correctly identifying "normal" clusters
% % \item Six different distance metrics
% % \item Computation time
% % \end{itemize}
% % 
% % \subsubsection{Overall}
% % 
% % <<echo=FALSE, results=tex>>=
% % library(xtable)
% % xtable(k30.euc$table,  caption = "Euclidean")
% % xtable(k30.weuc$table, caption = "Weighted Euclidean")
% % xtable(k30.man$table,  caption = "Manhattan")
% % xtable(k30.che$table,  caption = "Chebyshev")
% % xtable(k30.min$table,  caption = "Minkoski")
% % xtable(k30.mah$table,  caption = "Mahalanobis")
% % @





% Comment below out when building the whole doc:
\bibliographystyle{plain}
\bibliography{../Main/bibliography}
