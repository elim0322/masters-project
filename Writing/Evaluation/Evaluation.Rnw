<<set-parent5, echo=FALSE, cache=FALSE>>=
knitr::set_parent(parent = "../Main/Main.Rnw")
@
<<scores, cache=TRUE, echo=FALSE>>=
# load(file = "~/../Desktop/masters-project/Data/LOF_scores/scores.RData")
load(file = "//Users/eric/masters-project/Data/LOF_scores/scores.RData")
@
<<result, cache=TRUE, echo=FALSE>>=
# load(file = "~/../Desktop/masters-project/Data/System_results/dat1_05a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat1_10a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat1_20a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat1_30a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat1_40a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat1_50a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat2_05a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat2_10a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat2_20a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat2_30a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat2_40a.RData")
# load(file = "~/../Desktop/masters-project/Data/System_results/dat2_50a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat1_05a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat1_10a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat1_20a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat1_30a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat1_40a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat1_50a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat2_05a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat2_10a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat2_20a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat2_30a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat2_40a.RData")
load(file = "//Users/eric/masters-project/Data/System_results/dat2_50a.RData")

processResults = function(results, graph = FALSE) {
    d = numeric()
    f = numeric()
    e = vector()
    for (i in 1:100) {
        dif     = diff(results$p2.proportion.within.cluster[i, 2:10])
        dif.neg = which(dif < 0)
        mn      = dif.neg[which.min(dif[dif.neg])] + 1
        # mn      = which.min(dif[dif.neg]) + 2
        d       = c(d, results$system.detection.rate[i, mn])
        f       = c(f, results$system.false.alarm.rate[i, mn])
        if (length(mn)==0) {
            e = c(e, NULL)
        } else {
            e = c(e, mn)
        }
    }
    if (graph) {
        avg.TPR = colMeans(results$system.detection.rate)
        avg.FPR = colMeans(results$system.false.alarm.rate)
        avg.pwc = colMeans(results$p2.proportion.within.cluster)
        avg.pws = colMeans(results$p2.proportion.within.sample)
        plot(x = 0:10, y = seq(0, 1, by = 0.1), type = "n")
        lines(avg.TPR, col = "blue")
        lines(avg.FPR, col = "red")
        lines(avg.pwc)
        lines(avg.pws)
    }
    return(list(TPR = mean(d), FPR = mean(f), TPR.sd = sd(d), FPR.sd = sd(f), FPRs))
}
a = function(results) {
    tpr = numeric()
    fpr = numeric()
    tpr.sd = numeric()
    fpr.sd = numeric()
    for (i in 1:length(results)) {
        tpr = c(tpr, mean(results[[i]]$P1.detection.rate))
        fpr = c(fpr, mean(results[[i]]$P1.false.alarm.rate))
        tpr.sd = c(tpr.sd, sd(results[[i]]$P1.detection.rate))
        fpr.sd = c(fpr.sd, sd(results[[i]]$P1.false.alarm.rate))
    }
    w = which.max(tpr)
    list(tpr[w], fpr[w], tpr.sd[w], fpr.sd[w])
}
b = function(results) {
    tpr = numeric()
    fpr = numeric()
    for (i in 1:length(results)) {
        tpr = c(tpr, sd(results[[i]]$P1.detection.rate))
        fpr = c(fpr, sd(results[[i]]$P1.false.alarm.rate))
    }
    w = which.max(tpr)
    list(tpr[w], fpr[w])
}
{
dat1.means.05k = a(list(system.results_dat1_05a05k, system.results_dat1_10a05k, system.results_dat1_20a05k, system.results_dat1_30a05k, system.results_dat1_40a05k, system.results_dat1_50a05k))
dat1.means.10k = a(list(system.results_dat1_05a10k, system.results_dat1_10a10k, system.results_dat1_20a10k, system.results_dat1_30a10k, system.results_dat1_40a10k, system.results_dat1_50a10k))
dat1.means.20k = a(list(system.results_dat1_05a20k, system.results_dat1_10a20k, system.results_dat1_20a20k, system.results_dat1_30a20k, system.results_dat1_40a20k, system.results_dat1_50a20k))
dat1.means.30k = a(list(system.results_dat1_05a30k, system.results_dat1_10a30k, system.results_dat1_20a30k, system.results_dat1_30a30k, system.results_dat1_40a30k, system.results_dat1_50a30k))
dat1.means.40k = a(list(system.results_dat1_05a40k, system.results_dat1_10a40k, system.results_dat1_20a40k, system.results_dat1_30a40k, system.results_dat1_40a40k, system.results_dat1_50a40k))
dat1.means.50k = a(list(system.results_dat1_05a50k, system.results_dat1_10a50k, system.results_dat1_20a50k, system.results_dat1_30a50k, system.results_dat1_40a50k, system.results_dat1_50a50k))

dat1.sd.05k = b(list(system.results_dat1_05a05k, system.results_dat1_10a05k, system.results_dat1_20a05k, system.results_dat1_30a05k, system.results_dat1_40a05k, system.results_dat1_50a05k))
dat1.sd.10k = b(list(system.results_dat1_05a10k, system.results_dat1_10a10k, system.results_dat1_20a10k, system.results_dat1_30a10k, system.results_dat1_40a10k, system.results_dat1_50a10k))
dat1.sd.20k = b(list(system.results_dat1_05a20k, system.results_dat1_10a20k, system.results_dat1_20a20k, system.results_dat1_30a20k, system.results_dat1_40a20k, system.results_dat1_50a20k))
dat1.sd.30k = b(list(system.results_dat1_05a30k, system.results_dat1_10a30k, system.results_dat1_20a30k, system.results_dat1_30a30k, system.results_dat1_40a30k, system.results_dat1_50a30k))
dat1.sd.40k = b(list(system.results_dat1_05a40k, system.results_dat1_10a40k, system.results_dat1_20a40k, system.results_dat1_30a40k, system.results_dat1_40a40k, system.results_dat1_50a40k))
dat1.sd.50k = b(list(system.results_dat1_05a50k, system.results_dat1_10a50k, system.results_dat1_20a50k, system.results_dat1_30a50k, system.results_dat1_40a50k, system.results_dat1_50a50k))


## dat2
dat2.means.05k = a(list(system.results_dat2_05a05k, system.results_dat2_10a05k, system.results_dat2_20a05k, system.results_dat2_30a05k, system.results_dat2_40a05k, system.results_dat2_50a05k))
dat2.means.10k = a(list(system.results_dat2_05a10k, system.results_dat2_10a10k, system.results_dat2_20a10k, system.results_dat2_30a10k, system.results_dat2_40a10k, system.results_dat2_50a10k))
dat2.means.20k = a(list(system.results_dat2_05a20k, system.results_dat2_10a20k, system.results_dat2_20a20k, system.results_dat2_30a20k, system.results_dat2_40a20k, system.results_dat2_50a20k))
dat2.means.30k = a(list(system.results_dat2_05a30k, system.results_dat2_10a30k, system.results_dat2_20a30k, system.results_dat2_30a30k, system.results_dat2_40a30k, system.results_dat2_50a30k))
dat2.means.40k = a(list(system.results_dat2_05a40k, system.results_dat2_10a40k, system.results_dat2_20a40k, system.results_dat2_30a40k, system.results_dat2_40a40k, system.results_dat2_50a40k))
dat2.means.50k = a(list(system.results_dat2_05a50k, system.results_dat2_10a50k, system.results_dat2_20a50k, system.results_dat2_30a50k, system.results_dat2_40a50k, system.results_dat2_50a50k))

dat2.sd.05k = b(list(system.results_dat2_05a05k, system.results_dat2_10a05k, system.results_dat2_20a05k, system.results_dat2_30a05k, system.results_dat2_40a05k, system.results_dat2_50a05k))
dat2.sd.10k = b(list(system.results_dat2_05a10k, system.results_dat2_10a10k, system.results_dat2_20a10k, system.results_dat2_30a10k, system.results_dat2_40a10k, system.results_dat2_50a10k))
dat2.sd.20k = b(list(system.results_dat2_05a20k, system.results_dat2_10a20k, system.results_dat2_20a20k, system.results_dat2_30a20k, system.results_dat2_40a20k, system.results_dat2_50a20k))
dat2.sd.30k = b(list(system.results_dat2_05a30k, system.results_dat2_10a30k, system.results_dat2_20a30k, system.results_dat2_30a30k, system.results_dat2_40a30k, system.results_dat2_50a30k))
dat2.sd.40k = b(list(system.results_dat2_05a40k, system.results_dat2_10a40k, system.results_dat2_20a40k, system.results_dat2_30a40k, system.results_dat2_40a40k, system.results_dat2_50a40k))
dat2.sd.50k = b(list(system.results_dat2_05a50k, system.results_dat2_10a50k, system.results_dat2_20a50k, system.results_dat2_30a50k, system.results_dat2_40a50k, system.results_dat2_50a50k))
}

dat1.res05a = processResults(system.results_dat1_05a05k)
dat1.res10a = processResults(system.results_dat1_10a30k)
dat1.res20a = processResults(system.results_dat1_20a20k)
dat1.res30a = processResults(system.results_dat1_30a40k)
dat1.res40a = processResults(system.results_dat1_40a20k)
dat1.res50a = processResults(system.results_dat1_50a05k)
dat2.res05a = processResults(system.results_dat2_05a20k)
dat2.res10a = processResults(system.results_dat2_10a30k)
dat2.res20a = processResults(system.results_dat2_20a20k)
dat2.res30a = processResults(system.results_dat2_30a30k)
dat2.res40a = processResults(system.results_dat2_40a20k)
dat2.res50a = processResults(system.results_dat2_50a05k)

## dat1
dat1.means.05a = a(list(system.results_dat1_05a05k, system.results_dat1_05a10k, system.results_dat1_05a20k, system.results_dat1_05a30k, system.results_dat1_05a40k, system.results_dat1_05a50k))
dat1.means.10a = a(list(system.results_dat1_10a05k, system.results_dat1_10a10k, system.results_dat1_10a20k, system.results_dat1_10a30k, system.results_dat1_10a40k, system.results_dat1_10a50k))
dat1.means.20a = a(list(system.results_dat1_20a05k, system.results_dat1_20a10k, system.results_dat1_20a20k, system.results_dat1_20a30k, system.results_dat1_20a40k, system.results_dat1_20a50k))
dat1.means.30a = a(list(system.results_dat1_30a05k, system.results_dat1_30a10k, system.results_dat1_30a20k, system.results_dat1_30a30k, system.results_dat1_30a40k, system.results_dat1_30a50k))
dat1.means.40a = a(list(system.results_dat1_40a05k, system.results_dat1_40a10k, system.results_dat1_40a20k, system.results_dat1_40a30k, system.results_dat1_40a40k, system.results_dat1_40a50k))
dat1.means.50a = a(list(system.results_dat1_50a05k, system.results_dat1_50a10k, system.results_dat1_50a20k, system.results_dat1_50a30k, system.results_dat1_50a40k, system.results_dat1_50a50k))

## dat2
dat2.means.05a = a(list(system.results_dat2_05a05k, system.results_dat2_05a10k, system.results_dat2_05a20k, system.results_dat2_05a30k, system.results_dat2_05a40k, system.results_dat2_05a50k))
dat2.means.10a = a(list(system.results_dat2_10a05k, system.results_dat2_10a10k, system.results_dat2_10a20k, system.results_dat2_10a30k, system.results_dat2_10a40k, system.results_dat2_10a50k))
dat2.means.20a = a(list(system.results_dat2_20a05k, system.results_dat2_20a10k, system.results_dat2_20a20k, system.results_dat2_20a30k, system.results_dat2_20a40k, system.results_dat2_20a50k))
dat2.means.30a = a(list(system.results_dat2_30a05k, system.results_dat2_30a10k, system.results_dat2_30a20k, system.results_dat2_30a30k, system.results_dat2_30a40k, system.results_dat2_30a50k))
dat2.means.40a = a(list(system.results_dat2_40a05k, system.results_dat2_40a10k, system.results_dat2_40a20k, system.results_dat2_40a30k, system.results_dat2_40a40k, system.results_dat2_40a50k))
dat2.means.50a = a(list(system.results_dat2_50a05k, system.results_dat2_50a10k, system.results_dat2_50a20k, system.results_dat2_50a30k, system.results_dat2_50a40k, system.results_dat2_50a50k))

########################################################################################
d = function(result,a,res) {
    ind = which(!is.na(res$e))
    p2.fpr = numeric()
    for (i in 1:length(ind)) {
        p2.fpr = c(p2.fpr, result$system.false.alarm[ind[i], res$e[ind[i]]])
    }
    fp = ( (5000 * (1-a*0.01) * result$P1.false.alarm.rate[ind]) - (5000 * (1-a*0.01) * p2.fpr) ) / (5000 * (1-a*0.01) * result$P1.false.alarm.rate[ind])
    list(mean(fp), sd(fp))
}

p2_dat1_05a = d(system.results_dat1_05a05k, 5, res05a)
p2_dat1_10a = d(system.results_dat1_10a30k, 23, res10a)
p2_dat1_20a = d(system.results_dat1_20a20k, 20, res20a)
p2_dat1_30a = d(system.results_dat1_30a40k, 40, res30a)
p2_dat1_40a = d(system.results_dat1_40a20k, 20, res40a)
p2_dat1_50a = d(system.results_dat1_50a05k, 5, res50a)

p2_dat2_05a = d(system.results_dat2_05a20k, 20, res05a)
p2_dat2_10a = d(system.results_dat2_10a30k, 30, res10a)
p2_dat2_20a = d(system.results_dat2_20a20k, 20, res20a)
p2_dat2_30a = d(system.results_dat2_30a30k, 30, res30a)
p2_dat2_40a = d(system.results_dat2_40a20k, 20, res40a)
p2_dat2_50a = d(system.results_dat2_50a05k, 5, res50a)
@

% --------------------
% CHAPTER: Evaluation
% --------------------
\chapter{Evaluation}
\label{ch:evaluation}

% Maybe write this in discussion/conclusion?
%The dataset chosen for benchmarking our intrusion detection system is the KDD Cup 1999 data \cite{kdd99}. There are a few issues associated with the dataset such as the presence of redundant features and duplicated instances, mentioned by Tavallaee et al. \cite{tav09} who also proposed an improved version of the dataset called NSL-KDD. Despite the issues, our decision to proceed with KDD'99 is because it is a real world data captured at MIT Lincoln Labs which means it is more likely to be accurate at describing the characteristics of real world connections compared to those described by the processed data, NSL-KDD. Another possible limitation with KDD'99 dataset is its age. Because the dataset is dated, there is high possibility that the characteristics of modern attacks may be different to those described in the dataset. However it is the only real world dataset with complete attack labels to remain as the most popular choice as a benchmarking medium for any IDS evaluation. The lack of other public network data appears to be due to the highly sensitive nature associated with network information. Technical description of KDD'99 dataset is presented in Section \ref{sec:evaluation-data}.

In this chapter, we validate fulfillment of the ideas described in the previous chapter as well as evaluate the strength of our proposed NIDS in detecting intrusions in the well-known and widely used KDD Cup 1999 data \cite{kdd99}. For this, we are mainly interested in True and False Positive Rates (TPR and FPR), also known as the detection and false alarm rates respectively, of the system to assess how accurately each instance is classified as either an intrusion or a non-intrusion.

Each experiment is conducted using a random sample of 5,000 instances from the 10\% subset version of the original KDD'99 dataset and is repeated one hundred times to compensate for the rather large size of the dataset. The sample size, of 5,000 records, is chosen such that the LOF algorithm runs comfortably without reaching beyond the memory limit of the machine used for this experiment. The results from the repeated experiments are reported on their means and standard deviations. Lastly, we treat both the original training and test datasets as two separate datasets as our system operates under the unsupervised learning paradigm, that is the labels are only used for external evaluation purposes.

% These two performance measures are evaluted for other popular unsupervised NID techniques to compare our method against them.

% The dataset chosen for benchmarking our NIDS is the KDD Cup 1999 data \cite{kdd99} for its public availability with complete sets of labels. There have been a couple of datasets to replace the original KDD'99 data such as the NSL-KDD data \cite{tav09} and gureKddcup data \cite{per08}. The former dataset is essentially a carefully selected set of instances, mainly to avoid redundant features and duplicated entries, from the original KDD'99 data that does not introduce any novelty for us to be interested in. The latter dataset is generated equivalently as the original data with additional information on the payload of each connection record, that is the information of the transferred data. Because the payload data is processed as a sequence of bytes, for which sequential pattern mining is more appropriate, the gureKddcup data has not been a suitable choice for us. Other alternative datasets \cite{cre13} and \cite{shi12} were unavailable for access. All things considered, the KDD'99 data seems to remain as the most widely used dataset for the purpose of evaluating and benchmarking an IDS, especially because of the sheer number of literatures based on it. More technical description of the KDD'99 dataset is presented in Section \ref{evaluation:sec1}.

% The rest of this chapther includes the experimental setup for the evaluation in Section \ref{evaluation:sec2}, phase-specific evaluation results in Section \ref{evaluation:sec3} and system evaluation results  in Section \ref{evaluation:sec4}, along with our choice of performance measures.

%Each experiment, or run, is conducted using a random sample of 5,000 records as a testset. The ratio of intrusion and non-intrusion instances is varied in the testset to report the performance of the IDS between the differing ratios. The variation is specified for the percentage of intrusion instances in a test dataset at 10, 20, 30 and 40 percents. The sample size of 5,000 is chosen such that the LOF algorithm runs comfortably without reaching beyond the memory limit of the machine used for this experiment.

%The parameter $k$ for the LOF algorithm is varied to assess whether the value of $k$ has an impact on the performance of LOF.



\newpage
% ----------------------------
% SECTION 1: data description
% ----------------------------
\section{Data description}
\label{evaluation:sec1}

The KDD Cup 1999 dataset is prepared by Stolfo et al. \cite{sto00} based on a much larger dataset called DARPA'98. The original DARPA'98 data was captured at MIT Lincoln Laboratory over a period of 9 weeks as a part of the 1998 DARPA Intrusion Evaluation Program \cite{lip00}.

From DARPA'98, the 7 weeks of network traffic, that correspond to about 4 gigabytes of compressed raw binary TCP dump data were processed into 4,898,431 individual connection records to make up the training set of the KDD'99 data. And the 2 weeks of network traffic were processed into 2,984,154 individual connection records to make up the test data for KDD'99. A connection is a sequence of TCP packets that flow from a source IP address to a destination IP address in some well defined time period. Hence there is no information on the IP addresses in the datasets.

The KDD'99 data is fully labelled with either normal or an attack. Each attack belongs to a category of four types: Denial of Service (DoS), Remote to Local (R2L), User to Root (U2R) and probing attacks. The DoS attacks are associated with overloading network and machine resources to make them preoccupied such that the services become unavailable to legitimate users. The R2L attacks occur when an attacker sends packets to a target machine in order to gain access as a local user of the machine. The U2R attacks involve gaining unauthorised access to root previlages on a system, followed by preobtained access as a normal user. Lastly, the probing attacks are usually precursory attacks to scan networks and machines to gain information on any exploitable vulnerabilities. The training data for KDD'99 contain 22 different attacks, which are summarised in Table~\ref{eval:attack1}, where the type, name and description of each attack is presented in each column. The test data of KDD'99 contains 14 novel attacks that are not present in the training set. The list of these novel attacks is presented in Table~\ref{eval:attack2} in a similar manner to the previous table. For more extensive information on the attacks and their attack signatures, Lincoln Laboratory's Intrusion Detection Attacks Database \cite{darpa} should be referred to.


%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\caption{List of training attack types in KDD'99 data, retrieved and modified from \cite{darpa}.}
\label{eval:attack1}
\small
\begin{tabular}{ |p{1.6cm}||p{3.2cm}|p{10cm}| }
\hline
Type & Name & Summary \\
\hline
\hline
\textbf{DoS}
& \texttt{back} & URLs containing many front slashes are requested on the target server. \\
& \texttt{land} & Local Area Network Denial; a spoofed SYN packet is sent to the victim. \\
& \texttt{neptune} & SYN flood; floods of TCP/SYN packets are sent using the neptune exploit code. \\
& \texttt{pod} & Ping of Death; malformed ping packets are sent to cause a system crash. \\
& \texttt{smurf} & Large numbers of IP packets are sent to all hosts in a network through the broadcast address of the network. \\
& \texttt{teardrop} & Mingled, or invalid, IP packets are sent to the target machine to crash the operating system. \\
\cline{1-3}

\textbf{R2L}
& \texttt{ftp\_write} & A common anonymous FTP misconfiguration is exploited. \\
& \texttt{guess\_passwd} & The password of target machine is guessed via dictionary attacks. \\
& \texttt{imap} & A buffer overflow is exploited in the IMAP server. \\
& \texttt{multihop} & No information given. \\
& \texttt{phf} & Poorly written CGI scripts are exploited to execute commands with the privilege level of the http server. \\
& \texttt{spy} & No information given. \\
& \texttt{warezclient} & Followed by a warezmaster attack (see below) where the victim downloads the uploaded warez via the FTP. \\
& \texttt{warezmaster} & System bugs associated with FTP servers are exploited to create copies of illegal software, \emph{warez} \\
\cline{1-3}

\textbf{U2R}
& \texttt{buffer\_overflow} & Abuse of a buffer overflow in which codes in the extra data enable unauthorised root access. \\
& \texttt{loadmodule} & A program that loads modules into a kernel to gain root access on the local machine. \\
& \texttt{perl} & An exploit of a bug in some Perl implementations. \\
& \texttt{rootkit} & Softwares that enable unauthorised root access. \\
\cline{1-3}

\textbf{Probing}
& \texttt{ipsweep} & A scan to determine which hosts are listening on a network. \\
& \texttt{nmap} & Network Mapper; a general-purpose scanning tool that provides different types of network scans. \\
& \texttt{portsweep} & Multiple hosts are scanned for a specific listening port. \\
& \texttt{satan} & A network vulnerability scanner and a predecessor of the SAINT scanning program. \\
\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\caption{List of test attack types in KDD'99 data, retrieved and modified from \cite{darpa}.}
\label{eval:attack2}
\small
\begin{tabular}{ |p{1.6cm}||p{2.8cm}|p{11cm}| }
\hline
Type & Name & Summary \\
\hline
\hline
\textbf{DoS}
& \texttt{apache2} & Requests with many http headers are sent to target an apache server. \\
& \texttt{mailbomb} & Many messages are sent to a server to overflow the server's mail queue. \\
& \texttt{processtable} & The process table of a target machine is flooded with multiple instantiations of network servers using an exploit of root previlages \\
& \texttt{snmpgetattack} & No information. \\
& \texttt{snmpguess} & No information. \\
& \texttt{udpstorm} & Two UDP connections that produce output are established to produce a high number of packets \\
& \texttt{worm} & No information. \\
\cline{1-3}

\textbf{R2L}
& \texttt{httptunnel} & An HTTP client is set up and configured in the target machine to send information to the attacker's web server. \\
& \texttt{named} & An exploit of a buffer overflow, where a maliciously formatted inverse query on a TCP stream is sent to a name server. \\
& \texttt{sendmail} & An exploit of a buffer overflow, malicious email messages are sent to a target system running a vulnerable version of sendmail. \\
& \texttt{xlock} & An attacker displays remotely a modified version of the xlock program in the hope that the legitimate user types the password \\
& \texttt{xsnoop} & An attacker watches keystrokes processed by an unprotected server to gain access to the victim system. \\
\cline{1-3}

\textbf{U2R}
& \texttt{ps} & Exploit of race condition in the version of 'ps' distributed with Solaris 2.5 \\
& \texttt{xterm} & An exploit of a butter overflow in the Xaw library, especially in the xterm programs that use the library. \\
\cline{1-3}

\textbf{Probing}
& \texttt{mscan} & A probing tool to use DNS zone and scan IP addresses. \\
& \texttt{saint} & Security Administrator's Integrated Network Tool; an attacker can use the tool to gain security information, despite its original intention. \\

\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%



For both the training and test datasets of KDD'99, each connection record consists of about 100 bytes and 42 features where the last feature contains the attack labels. Out of the 42 features, 9 of them represent basic features of individual TCP connections. The complete list of the basic features is presented in Table~\ref{eval:tab1}, where each column represents the feature names, descriptions and data types, respectively. The binary and nominal features are categorical features that contain two and three or more unique values, respectively, and the continuous features are numeric ones defined in their own unique interval. The basic features can all be extracted from a TCP connection using a standard packet sniffer.
%Something worth mentioning is that there are 70 levels for \texttt{service}, each representing different types of service, in both the training and test datasets but the 10\% subset versions of the datasets, which we use for our experiments, only contain 65 and 66 of them.

%%%%%%%%%%%%%%%%%%%%
\begin{table}
\caption{List of basic features in KDD'99 data, retrieved and modified from \cite{kdd99}.}
\label{eval:tab1}
\small
 \begin{tabular}{ |p{2.9cm}||p{10cm}|p{1.9cm}| }
 \hline
 Feature & Description & Type \\
 \hline
 \hline
 \texttt{duration} & length of the connection in seconds & continuous \\
 \texttt{protocol\_type} & type of the protocol & nominal \\
 \texttt{service} & network service on the destination & nominal \\
 \texttt{flag} & normal or error status of the connection & nominal \\
 \texttt{src\_bytes} & amount of data in bytes from source to destination & continuous \\
 \texttt{dst\_bytes} & amount of data in bytes from destination to source & continuous \\
 \texttt{land} & 1 if connection is from the same host or port; 0 otherwise & binary \\
 \texttt{wrong\_fragment} & number of ``wrong'' fragments & continuous \\
 \texttt{urgent} & number of urgent packets & continuous \\
 \hline
 \end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%

The remaining 32 features (excluding the last one for the labels) are derived features that fall into two categories: (1) content and (2) traffic features. Firstly, the content features (13 of the 32 features) are created from the domain knowledge of Stolfo et al. and are listed in Table~\ref{eval:tab2} with the same structure as the previous table. The purpose of these features is to provide information that can be used to detect the U2R and R2L attacks which lack intrusion frequent sequential patterns. Unlike the DoS and probing attacks, which happen in bursts in a very short period of time, the U2R and R2L attacks are usually embedded in the data portion of the packets and involve only a single connection, which make them difficult to be detected based only on the basic features. For example, \texttt{num\_failed\_logins} can provide useful information to help capture threats such as dictionary attacks. A couple of points to be made about the content features are: (1) there are three unique values (0, 1 and 2) in \texttt{su\_attempted}, despite its description being a binary variable, and (2) \texttt{num\_outbound\_cmds} is redundant as it contains only one unique value of 0. No information on the discrepancy in \texttt{su\_attempted} was available for clarification and more domain knowledge was needed to determine whether it was intended or a data entry error. In addition, the numbers for the values of 0 are exceedingly dominant compared to those for the values of 1 and 2, e.g., the odds are $40/4898431$ for 1's and $70/4898431$ for 2's for the training set and even lower odds for the test set. Thus the results will not be affected much.

Secondly, the traffic features (19 of the 32 features) are time-based, created using a two-second time window, and are further divided into two types: (1) same-host and (2) same-service features. The same-host features examine connections in the past 2 seconds that have the same destination as the current connection. Similarly, the same-service features examine the connections that have the same feature as the current connection in the past 2 seconds. The complete list of the same-host and the same-service features is presented in Table~\ref{eval:tab3}, where each column represents the sub-type of the traffic features, feature names and their descriptions, respectively. The data types are not included in the table as these features are all continuous.

There is a potential problem with the traffic features, especially the two-second time window, where some probing attacks are deployed gradually in duration that spans more than 2 seconds. These slow attacks are difficult to detect using the aforementioned traffic features, which led to 10 additional traffic features to be created. They are derived from the same-host and same-service features based on the connection window of 100 connections rather than the two-second time window. These are called the host-based traffic features and their complete list is presented in Table~\ref{eval:tab4}. In the table, all the feature names have a prefix of ``\texttt{dst\_host\_}" and their descriptions are synonymous with the matching features from Table~\ref{eval:tab3}. For example, \texttt{dst\_host\_diff\_srv\_rate} represents the percentage of connections to different services within the connection window of 100 connections.


%%%%%%%%%%%%%%%%%%%%
\begin{table}
\caption{List of content features within a connection suggested by domain knowledge in KDD'99 data, retrieved and modified from \cite{kdd99}.}
\label{eval:tab2}
\small
 \begin{tabular}{ |p{3.5cm}||p{9.3cm}|p{1.8cm}| }
 \hline
 Feature & Description & Type \\
 \hline
 \hline
 \texttt{hot} & number of ``hot'' indicators & continuous \\
 \texttt{num\_failed\_logins} & number of failed login attempts & continuous \\
 \texttt{logged\_in} & 1 if successfully logged in; 0 otherwise & binary \\
 \texttt{num\_compromised} & number of ``compromised'' conditions & continuous \\
 \texttt{root\_shell} & 1 if root shell is obtained; 0 otherwise & binary \\
 \texttt{su\_attempted} & 1 if ``su root'' command attempted; 0 otherwise & binary \\
 \texttt{num\_root} & number of root accesses & continuous \\
 \texttt{num\_file\_creations} & number of file creation operations & continuous \\
 \texttt{num\_shells} & number of shell prompts & continuous \\
 \texttt{num\_access\_files} & number of operations on access control files & continuous \\
 \texttt{num\_outbound\_cmds} & number of outbound commands in an ftp session & continuous \\
 \texttt{is\_hot\_login} & 1 if the login belongs to the ``hot'' list; 0 otherwise & binary \\
 \texttt{is\_guest\_login} & 1 if the login is a ``guest''login; 0 otherwise & binary \\
 \hline
 \end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
\begin{table}
\caption{List of traffic features in KDD'99 data, retrieved and modified from \cite{kdd99}.}
\label{eval:tab3}
\small
 \begin{tabular}{ |p{2.32cm}|p{3.62cm}||p{8.7cm}| }
 \hline
 Traffic Type & Feature & Description\\
 \hline
 \hline
 & \texttt{count} & number of connections to the same host as the current connection in the past two seconds \\
 \hline
 Same-host & \texttt{serror\_rate} & \% of connections that have ``SYN'' errors \\
 & \texttt{rerror\_rate} & \% of connections that have ``REJ'' errors \\
 & \texttt{same\_srv\_rate} & \% of connections to the same service \\
 & \texttt{diff\_srv\_rate} & \% of connections to different services \\
 & \texttt{srv\_count} & number of connections to the same service as the current connection in the past two seconds \\
 \hline
 Same-service & \texttt{srv\_serror\_rate} & \% of connections that have ``SYN'' errors \\
 & \texttt{srv\_rerror\_rate} & \% of connections that have ``REJ'' errors \\
 & \texttt{srv\_diff\_host\_rate} & \% of connections to different hosts \\
 \hline
 \end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
\begin{table}
\caption{List of host-based traffic features in KDD'99 data}
\label{eval:tab4}
\small
 \begin{tabular}{ |p{15.56cm}| }
 \hline
 Feature \\
 \hline
 \hline
 \texttt{dst\_host\_count, dst\_host\_srv\_count, dst\_host\_same\_srv\_rate, dst\_host\_diff\_srv\_rate, dst\_host\_same\_src\_port\_rate, dst\_host\_srv\_diff\_host\_rate, dst\_host\_serror\_rate, dst\_host\_srv\_serror\_rate, dst\_host\_rerror\_rate, dst\_host\_srv\_rerror\_rate} \\
 \hline
 \end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%

Finally, it must be emphasised that the training and test data are not from the same probability distribution, i.e., the two datasets are statistically different.

All the features with their types and how they were created have been described. The following section describes how we set up our experiments to test our ideas.


% ----------------------------------
% SECTION 2: Experimental framework
% ----------------------------------
\section{Experimental framework}
\label{evaluation:sec2}

All our experiments are performed using random samples from the 10\% subset versions of the training and test KDD'99 data. We used 5,000 connection records, sampled without replacement, for each run of our experiment, and repeated a hundred times using different samples each time. We summarised the results of the one hundred repeated runs using the mean and the standard deviation for each experiment. The mean was used because it provided the best summary statistics of the results by providing the inference on their central locations. The standard deviation was necessary to evaluate the stability and statistical dispersion among the results. The mean and standard deviation measures, coupled together, described our results from a few hundred runs well.

We considered variations in the frequencies of intrusion instances in data an important problem because unsupervised anomaly-based intrusion detection relies on the assumption that the frequencies of intrusions are much less than those of normal in the given data. To evaluate the effect of the variations in the intrusion frequencies, we varied the value with 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5 in our experiments. For example, a dataset for the intrusion frequency of 0.1 consists of $0.1 \times 5000 = 500$ intrusions and $(1-0.1) \times 5000 = 4500$ normal instances. For this particular set, a random sample of 500 intrusions would be taken from the group of intrusions and 4,500 normal instances from that of normal instances, separately, to make up a total of 5,000 instances.

As mentioned in the previous section, the training and test datasets are statistically different, which means they can be treated as two distinct and independent sets of data. Furthermore, we can ignore the attack labels during the experiments as our method runs under a fully unsupervised learnning manner. The attack labels are only used at the \emph{end} of the experiments to carry out external evaluation on the performance. Therefore, each of our experiments on the training and test data should be regarded as two independent experiments performed on two different datasets. The meaning of \emph{training} and \emph{test} data are hence conveniently ignored from here on.

The first hypotheses we test are whether each phase is achieving its objective, as discussed in the previous chapter. Then we evaluate the overll system performance and compare it with other existing methods in the following sections.



% -------------------------------
% SECTION 3: Phase evaluation
% -------------------------------
\section{Phase evaluation}
\label{evaluation:sec3}

Our proposed intrusion detection framework was built upon a two-phase structure, as previously discussed, where each phase was designed and tasked with a unique objective. As the first step of evaluating our method, the two phases must be tested to show the objectives are met.

As presented in the previous chapter, the first phase was intended to detect as many anomalies as possible, i.e., achieve maximal TPR, at the cost of allowing a few False Positives to also arise. Thus, Phase~1 is evaluated with its: (1) TPR to test our hypothesis that it is capable of achieving the said maximal detection rate and (2) FPR to assess the amount of False Positives in the results. Additionally, two parameters are varied: the input parameter, $k$, for LOF and the intrusion frequency to evaluate their effect on the performance of LOF. For Phase~2, our primary interest, as well as its purpose, is in the reduction of overall FPR thus percentage reductions in the FPR are investigated.

Sections \ref{evaluation:sec3.1} and \ref{evaluation:sec3.2} that follow describe the results 


\subsection{Phase 1}
\label{evaluation:sec3.1}
As mentioned previously, the intrusion frequencies are varied between 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The parameter, $k$, is also varied as a proportion of the given data, which we denoted as $k_p$, using the same values of 0.05, 0.1, 0.2,$\dotsc$, 0.5 such that $k_p=0.05$ is equivalent to $k=0.05 \times 5000 = 250$. The reason for such variation method is that small values of $k$, e.g., anything below 250, produced extremely noisy output. As such the resulting TPR was low while the FPR was high. Also, the LOF algorithm considers $k$ points as a cluster, which means if the value of $k$ is set too low, fewer points are needed for a cluster and more points will be excluded from the cluster. This will cause more points to be recognised as anomalies. Hence, $k$ is varied as a proportion of the given data so that it is effectively adjusted for the size of the data.

Firstly, we evaluate the effect of varying $k$ on the performance of LOF. For this, we fixed the intrusion frequency and varied $k_p$, and a hundred runs of LOF, using different samples each time, were performed. Then we summarised the computed LOF scores by using KDE to produce graphical output. Each of this experiment is repeated for different values of the intrusion frequencies. We kept the random seeds for the one hundred random samples and used them for different experiments so that the results of each experiment is fully comparable. Appendices~\ref{appendixa} and \ref{appendixb} present the results, where each \emph{figure} represents the plots for the corresponding intrusion frequency and each \emph{plot} corresponds to each of the different values of $k_p$, as indicated by the plot title. The appendices are divided such that they present the results for the training and test data, respectively. There are one hundred density curves on each plot, as previously described, and the transparency of $\alpha=0.2$ is used for all curves such that the curves of similar patterns are more visually apparent. Examining between the plots and comparing them for each figure, we can observe that as we increase the value of $k_p$, the computed LOF scores become less noisy and the differences in the densities, i.e., different peaks, are more clearly visible. If $k$ is set too low, the range of the resulting LOF scores is too great with noise, meaning an inflated score is given to each connection record. This leads to too many points being considered anomalies.. From this, we conclude that the greater values of $k$ are preferred but the question remains as to how much $k$ is appropriate. The optimal value for $k$ should depend on data but the scores appear to be the most stable when $k_p=0.5$. Also, $k_p$ must not be less than the actual intrusion frequency. For example, in Figure~\ref{dat1-a3}, we observe that the scores are more stable for $k_p=0.3$, $k_p=0.4$ and $k_p=0.5$.

Secondly, we evaluate the performance of LOF assuming that an appropriate value for $k$ is chosen for each experiment. We present the resulting TPR and FPR in Figure~\ref{fig:lofResults}, where the TPR and FPR are represented by the solid and dashed lines, respectively, with the horizontal error bars. It must be noted that the TPR and FPR for each value of the intrusion frequencies are the means of one hundred TPR and FPR, respectively. Similarly, the standard deviations are also averaged and represented by the error bars.

Figure \ref{fig:lofResults} reveals that the anomaly detection of LOF in Phase~1 is very accurate for the training data as shown by the very high TPR. The TPR for the test data are slightly lower than that for the training data. Upon manual examination, we have found that, for the test data, many intrusion instances share similar local densities as their non-intrusion counterparts leading to failure in finding adequate thresholds. When the kernel density estimates  were plotted, a number of intrusion instances had unusually low LOF scores than the rest, which made them difficult for detection. The LOF algorithm was not able to differentiate between the non-intrusion and the intrusion instances because their respective local densities were similar. On the other hand, Phase~1 anomaly detection is extremely precise for both the training and test data as the lengths of the error bars are nearly invisible. Tables~\ref{tab:app-tab1} and \ref{tab:app-tab2}, in the appendices, present the numerical results used to plot the figure, where the values for the small error bars can be examined.

When the intrusion frequency is 0.5, we observe a steep drop in the TPR. The reason may be that, at that intrusion frequency, the corresponding data become perfectly balanced with the equal numbers of the attack and normal instances. In this case, it becomes difficult to tell which cluster is the correct representative of the normal, or the attack, pattern. For example, if the given data are distributed such that the intrusion instances are positioned in closer proximity to one another than their normal counterparts, the results from an anomaly detection task may unintentionally label the normal as anomalies. In any case, the intrusion frequency of 0.5 is simply too high for any network unless nearly half of its users suddenly become mischievous.

Lastly, the FPR appear to be high as expected. If we adjusted the threshold, $t$, to lower the FPR, we would not have achieved the extremely high TPR. Not much comment can be made on the FPR other than to visualise them along with the TPR.

We conclude that the anomaly detection using LOF in Phase~1 performs well. As we hypothesised in the previous chapter, using the optimistic threshold, $t$, can achieve maximal detection at the cost of higher FPR. In addition, it is strongly likely that the results will be lower than expected when (1) the given data are perfectly balanced (i.e., when the intrusion frequency is 0.5) and (2) the local densities of normal and attacks are similar. For the cases where the local densities are similar, an anomaly detection algorithm that is not density-based could be a more viable alternative.

<<lofResults, dependson="result", echo=FALSE, out.width="0.7\\linewidth", fig.show='hold', fig.align='center', fig.cap="TPR and FPR across intrusion frequencies from the training and test sets">>=
x = 1:6
y1 = c(dat1.means.05a[[1]], dat1.means.10a[[1]], dat1.means.20a[[1]], dat1.means.30a[[1]], dat1.means.40a[[1]], dat1.means.50a[[1]])
y2 = c(dat1.means.05a[[2]], dat1.means.10a[[2]], dat1.means.20a[[2]], dat1.means.30a[[2]], dat1.means.40a[[2]], dat1.means.50a[[2]])
y1.sd = c(dat1.means.05a[[3]], dat1.means.10a[[3]], dat1.means.20a[[3]], dat1.means.30a[[3]], dat1.means.40a[[3]], dat1.means.50a[[3]])
y2.sd = c(dat1.means.05a[[4]], dat1.means.10a[[4]], dat1.means.20a[[4]], dat1.means.30a[[4]], dat1.means.40a[[4]], dat1.means.50a[[4]])

# par(xpd = T, mar = par()$mar+c(0,0,0,3))
plot(y1, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Rates", lwd = 1.3, type="b", main="Training data")
lines(y2, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6, 0.6, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), cex = 1.2, xjust=1)
arrows(x, y1-y1.sd, x, y1+y1.sd, length=0.05, angle=90, code=3)
arrows(x, y2-y2.sd, x, y2+y2.sd, length=0.05, angle=90, code=3)

y3 = c(dat2.means.05a[[1]], dat2.means.10a[[1]], dat2.means.20a[[1]], dat2.means.30a[[1]], dat2.means.40a[[1]], dat2.means.50a[[1]])
y4 = c(dat2.means.05a[[2]], dat2.means.10a[[2]], dat2.means.20a[[2]], dat2.means.30a[[2]], dat2.means.40a[[2]], dat2.means.50a[[2]])
y3.sd = c(dat2.means.05a[[3]], dat2.means.10a[[3]], dat2.means.20a[[3]], dat2.means.30a[[3]], dat2.means.40a[[3]], dat2.means.50a[[3]])
y4.sd = c(dat2.means.05a[[4]], dat2.means.10a[[4]], dat2.means.20a[[4]], dat2.means.30a[[4]], dat2.means.40a[[4]], dat2.means.50a[[4]])

plot(y3, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Rates", lwd = 1.3, type="b", main="Test data")
lines(y4, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6, 0.6, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), cex = 1.2, xjust=1)
arrows(x, y3-y3.sd, x, y3+y3.sd, length=0.05, angle=90, code=3)
arrows(x, y4-y4.sd, x, y4+y4.sd, length=0.05, angle=90, code=3)
@




\subsection{Phase 2}
\label{evaluation:sec3.2}
Figure \ref{fig:dbscanResults} presents percentage reduction in FPRs from Phase~1 to Phase~2. It seems that the FP elimination step of Phase~2 works really well, especially for the \emph{test} dataset. The unusual decline in the percentage at the intrusion frequency of 0.2 for the \emph{training} dataset is not something to worry about as the initial FPR, from Phase~1, has been quite low to begin with.

Some of the FPs remain included in the result, limiting further percentage reduction in FPR, because they are incorrectly classified, or clustered, by the DBSCAN algorithm. These FPs are positioned in relatively dense regions nearby some of the intrusion instances to make them indistinguishable from one another based on density. Therefore the DBSCAN algorithm fails to cluster them with non-intrusion instances by definition.

<<dbscanResults, dependson="result", echo=FALSE, out.width="0.7\\linewidth", fig.show='hold', fig.align='center', fig.cap="Percentage reduction in FPR across intrusion frequencies from the training and test sets">>=
# dat1.FPR = c(dat1.means.05a[[2]], dat1.means.10a[[2]], dat1.means.20a[[2]], dat1.means.30a[[2]], dat1.means.40a[[2]], dat1.means.50a[[2]])
# dat2.FPR = c(dat2.means.05a[[2]], dat2.means.10a[[2]], dat2.means.20a[[2]], dat2.means.30a[[2]], dat2.means.40a[[2]], dat2.means.50a[[2]])
# y1 = 100 * (dat1.FPR - c(processResults(system.results_dat1_05a10k)$FPR,processResults(system.results_dat1_10a30k)$FPR,processResults(system.results_dat1_20a20k)$FPR,processResults(system.results_dat1_30a30k)$FPR,processResults(system.results_dat1_40a20k)$FPR,processResults(system.results_dat1_50a05k)$FPR)) / dat1.FPR
# y2 = 100 * (dat2.FPR - c(processResults(system.results_dat2_05a10k)$FPR,processResults(system.results_dat2_10a20k)$FPR,processResults(system.results_dat2_20a20k)$FPR,processResults(system.results_dat2_30a30k)$FPR,processResults(system.results_dat2_40a30k)$FPR,processResults(system.results_dat2_50a10k)$FPR)) / dat2.FPR
x = 1:6
y1.mean = 100*c(p2_dat1_05a[[1]], p2_dat1_10a[[1]], p2_dat1_20a[[1]], p2_dat1_30a[[1]], p2_dat1_40a[[1]], p2_dat1_50a[[1]])
y2.mean = 100*c(p2_dat2_05a[[1]], p2_dat2_10a[[1]], p2_dat2_20a[[1]], p2_dat2_30a[[1]], p2_dat2_40a[[1]], p2_dat2_50a[[1]])
y1.sd = 100*c(p2_dat1_05a[[2]], p2_dat1_10a[[2]], p2_dat1_20a[[2]], p2_dat1_30a[[2]], p2_dat1_40a[[2]], p2_dat1_50a[[2]])
y2.sd = 100*c(p2_dat2_05a[[2]], p2_dat2_10a[[2]], p2_dat2_20a[[2]], p2_dat2_30a[[2]], p2_dat2_40a[[2]], p2_dat2_50a[[2]])


plot(y1.mean, ylim=c(0,100), xlab = "Intrusion Freq.", xaxt="n", ylab="Rates", lwd = 1.3, type="b", main="Training data")
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6, 0.6, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), cex = 1.2, xjust=1)
arrows(x, y1.mean-y1.sd, x, y1.mean+y1.sd, length=0.05, angle=90, code=3)


lines(y2, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
arrows(x, y2-y2.sd, x, y2+y2.sd, length=0.05, angle=90, code=3)


plot(y1, ylim=c(0,100), xlab = "Intrusion Freq.", xaxt="n", ylab="Percentage Reduction in FPR", lwd = 1.3, type="b", main="Training data")
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
plot(y2, ylim=c(0,100), xlab = "Intrusion Freq.", xaxt="n", ylab="Percentage Reduction in FPR", lwd = 1.3, type="b", main="Test data")
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))



@



% -----------------------------
% SECTION 2: System evaluation
% -----------------------------
\section{System evaluation}
\label{evaluation:sec3}

Figure \ref{fig:sysResults} presents the system evaluation results for the \emph{training} and \emph{test} datasets. For the \emph{training} dataset between the intrusion frequencies of 0.05 and 0.4, we can infer graphically that the system is accurate, as the TPRs are very close to 1, and precise, as the lengths of the error bars are short. We suspect that the decrease in performance from the intrusion frequency of 0.5 is caused by the balanced nature of the classes in the random samples. As the data become more balanced, there is higher chance for DBSCAN to form balanced clusters. This in turn increases the likelihood that the largest clusters, which we remove, actually consist of intrusion instances to result in a decrease in overall TPR. Because our method assumes of higher frequencies of non-intrusion instances in test data, a mendatory assumption for any anomaly-based intrusion detection, and removes the largest resulting clusters under that assumption, it cannot be functional if that assumption is violated. In other words, perfectly balanced data cannot be classified correctly in an unsupervised learning manner unless some external information is available. The unusually high variation in the TPR at this particular intrusion frequency means that the system was highly indecisive because there were essentially no classification criteria. In fact about half of the hundred runs produced better results than the other half to further support our reasoning.

For the \emph{test} dataset, both the TPRs and FPRs are generally lower compared to those from the \emph{training} dataset. We suspect the primary cause of this to be the similarities in local densities introducing challenges against effective classification. Also the performance starts to decrease at the intrusion frequency of 0.4, that is lower compared to the \emph{training} set. This can also be explained by the similarities between the local densities that scale down the previously mentioned threshold at which the classification criteria cease to exist. The problem of indistinguishable local densities is difficult to address because each dataset is associated with a unique degree of similarity within its respective local densities. For data that suffer from extremely similar densities, the best solution may be to avoid using anything distance-based, because densities are essentially computed using distances, and rather attempt to incorporate some external information capable of describing different types of instances. Another possible cause may be associated with our random sampling method. We attempted to cover as much portion of the data as possible by running repeated experiments as well as by sampling without replacement. However running repeated experiments with small samples, which may not be fitting representatives of the entire dataset, can skew the results in certain directions. In addition, samples, randomly selected without replacement, are no longer independent of one another, which may have an effect on the overall results. We could not increase the sample size further due to the expensive nature of the LOF algorithm.

% One relevant metric, as a measure of dispersion, to evaluate the standard deviations is the \emph{coefficient of variation}, $c_v$, given by
% \[ c_v = \frac{\sigma}{\mu}, \]
% all of which are well under 1 to support the preciseness of our results for those intrusion frequencies.

<<sysResults, dependson="result", cache=TRUE, echo=FALSE, out.width="0.49\\linewidth", fig.show='hold', fig.align='center', fig.cap="System TPR and FPR across intrusion frequencies from the training and test sets">>=
# res05a = processResults(system.results_dat1_10a30k)
# dat1.res05a = processResults(system.results_dat1_05a05k)
# dat1.res10a = processResults(system.results_dat1_10a30k)
# dat1.res20a = processResults(system.results_dat1_20a20k)
# dat1.res30a = processResults(system.results_dat1_30a40k)
# dat1.res40a = processResults(system.results_dat1_40a20k)
# dat1.res50a = processResults(system.results_dat1_50a05k)
x = 1:6
y1 = c(dat1.res05a$TPR, dat1.res10a$TPR, dat1.res20a$TPR, dat1.res30a$TPR, dat1.res40a$TPR, dat1.res50a$TPR)
y2 = c(dat1.res05a$FPR, dat1.res10a$FPR, dat1.res20a$FPR, dat1.res30a$FPR, dat1.res40a$FPR, dat1.res50a$FPR)
y1.sd = c(dat1.res05a$TPR.sd, dat1.res10a$TPR.sd, dat1.res20a$TPR.sd, dat1.res30a$TPR.sd, dat1.res40a$TPR.sd, dat1.res50a$TPR.sd)
y2.sd = c(dat1.res05a$FPR.sd, dat1.res10a$FPR.sd, dat1.res20a$FPR.sd, dat1.res30a$FPR.sd, dat1.res40a$FPR.sd, dat1.res50a$FPR.sd)

plot(y1, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Rates", lwd = 1.3, type="b", main="Training data")
lines(y2, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6*1.034, 1.04, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), xjust=1, yjust=0, xpd=TRUE)
arrows(x, y1-y1.sd, x, y1+y1.sd, length=0.05, angle=90, code=3)
arrows(x, y2-y2.sd, x, y2+y2.sd, length=0.05, angle=90, code=3)

# dat2.res05a = processResults(system.results_dat2_05a20k)
# dat2.res10a = processResults(system.results_dat2_10a30k)
# dat2.res20a = processResults(system.results_dat2_20a20k)
# dat2.res30a = processResults(system.results_dat2_30a30k)
# dat2.res40a = processResults(system.results_dat2_40a20k)
# dat2.res50a = processResults(system.results_dat2_50a05k)
y3 = c(dat2.res05a$TPR, dat2.res10a$TPR, dat2.res20a$TPR, dat2.res30a$TPR, dat2.res40a$TPR, dat2.res50a$TPR)
y4 = c(dat2.res05a$FPR, dat2.res10a$FPR, dat2.res20a$FPR, dat2.res30a$FPR, dat2.res40a$FPR, dat2.res50a$FPR)
y3.sd = c(dat2.res05a$TPR.sd, dat2.res10a$TPR.sd, dat2.res20a$TPR.sd, dat2.res30a$TPR.sd, dat2.res40a$TPR.sd, dat2.res50a$TPR.sd)
y4.sd = c(dat2.res05a$FPR.sd, dat2.res10a$FPR.sd, dat2.res20a$FPR.sd, dat2.res30a$FPR.sd, dat2.res40a$FPR.sd, dat2.res50a$FPR.sd)
plot(y3, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="Percentage Reduction in FPR", lwd = 1.3, type="b", main="Training data")
lines(y4, ylim=c(0,1), xaxt="n", lwd = 1.3, type="b", lty=2)
axis(1, at=1:6, labels=c(0.05,0.1,0.2,0.3,0.4,0.5))
legend(6*1.034, 1.04, c("TPR", "FPR"), lty=c(1,2), lwd=rep(1.3,2), xjust=1, yjust=0, xpd = TRUE)
arrows(x, y3-y3.sd, x, y3+y3.sd, length=0.05, angle=90, code=3)
arrows(x, y4-y4.sd, x, y4+y4.sd, length=0.05, angle=90, code=3)
@




% ----------------------
% SECTION 3: Comparison
% ----------------------
\section{Our method vs. others}
We compare our method with other possible approaches based on some of the proposed schemes in the literature, that is PCA-based and clustering-based approaches. For the PCA-based approach, we implement a scheme similar to \cite{lak05}, in which the authors perform a \emph{Principal Component Analysis} (PCA) on test data to project it into the principal subspace and the $k$-means algorithm on the subspace to compare resulting clusters with the trained clusters of known anomalies, essentially in a semi-supervised manner. One difference in our approach is that we select the number of Principal Components based on the Zoski and Jurs' $b$ coefficient \cite{zos93, zos96}, which is a robust non-graphical alternative to Cattell's scree plots discussed in \cite{rai13}, to avoid inefficiency in manual examination through hundreds of scree plots. Another difference is that we replace $k$-means with $X$-means \cite{pel00} as to avoid, again, manually having to determine optimal number of clusters, $k$. Lastly the largest cluster is removed under an assumption that the size of such cluster should correspond to the high frequency of normal instances, therefore mostly contain non-intrusion instances. For the clustering-based approach, we use simply use the $X$-means algorithm and remove the largest cluster based on the same previously mentioned assumption. As a third comparison, we hoped to reproduce a subspace clustering approach proposed in \cite{cas12, cas14} because it is the most recent work with the most promising results and involves DBSCAN. However we failed to reproduce their work and decided to leave it out of the comparison. Random samples of the same seeds as our previous experiments are used to provide fair comparision between the methods. All the methods operate under the unsupervised learning paradigm to further assure comparability. The results are presented in Figure \ref{fig:compareTPR}.

<<compareTPR, dependson="sysResults", cache=TRUE, echo=FALSE, out.width="0.65\\linewidth", fig.show='hold', fig.align='center', fig.cap="Comparison of TPRs across different methods">>=
# load("~/../Desktop/masters-project/Data/System_results/pca.results.RData")
# load("~/../Desktop/masters-project/Data/System_results/xmeans.results.RData")
load("//Users/eric/masters-project/Data/System_results/pca.results.RData")
load("//Users/eric/masters-project/Data/System_results/xmeans.results.RData")
## x-axis
# w = 0.1  # width
w = 0.09
x1.l = seq(1, 30, by = 5) + w
x1.r = seq(2, 29, by = 5) - w
x2.l = seq(2, 30, by = 5) + w
x2.r = seq(3, 31, by = 5) - w
x3.l = seq(3, 30, by = 5) + w
x3.r = seq(4, 31, by = 5) - w
## y-axis
## xmeans results
y5 = c(mean(xmeans.results.dat1.05a$TPR),mean(xmeans.results.dat1.10a$TPR),mean(xmeans.results.dat1.20a$TPR),mean(xmeans.results.dat1.30a$TPR),mean(xmeans.results.dat1.40a$TPR),mean(xmeans.results.dat1.50a$TPR))
y6 = c(mean(xmeans.results.dat1.05a$FPR),mean(xmeans.results.dat1.10a$FPR),mean(xmeans.results.dat1.20a$FPR),mean(xmeans.results.dat1.30a$FPR),mean(xmeans.results.dat1.40a$FPR),mean(xmeans.results.dat1.50a$FPR))
y5.sd = c(sd(xmeans.results.dat1.05a$TPR),sd(xmeans.results.dat1.10a$TPR),sd(xmeans.results.dat1.20a$TPR),sd(xmeans.results.dat1.30a$TPR),sd(xmeans.results.dat1.40a$TPR),sd(xmeans.results.dat1.50a$TPR))
y6.sd = c(sd(xmeans.results.dat1.05a$FPR),sd(xmeans.results.dat1.10a$FPR),sd(xmeans.results.dat1.20a$FPR),sd(xmeans.results.dat1.30a$FPR),sd(xmeans.results.dat1.40a$FPR),sd(xmeans.results.dat1.50a$FPR))
## pca results
y7 = c(mean(pca.results.dat1.05a$TPR),mean(pca.results.dat1.10a$TPR),mean(pca.results.dat1.20a$TPR),mean(pca.results.dat1.30a$TPR),mean(pca.results.dat1.40a$TPR),mean(pca.results.dat1.50a$TPR))
y8 = c(mean(pca.results.dat1.05a$FPR),mean(pca.results.dat1.10a$FPR),mean(pca.results.dat1.20a$FPR),mean(pca.results.dat1.30a$FPR),mean(pca.results.dat1.40a$FPR),mean(pca.results.dat1.50a$FPR))
y7.sd = c(sd(pca.results.dat1.05a$TPR),sd(pca.results.dat1.10a$TPR),sd(pca.results.dat1.20a$TPR),sd(pca.results.dat1.30a$TPR),sd(pca.results.dat1.40a$TPR),sd(pca.results.dat1.50a$TPR))
y8.sd = c(sd(pca.results.dat1.05a$FPR),sd(pca.results.dat1.10a$FPR),sd(pca.results.dat1.20a$FPR),sd(pca.results.dat1.30a$FPR),sd(pca.results.dat1.40a$FPR),sd(pca.results.dat1.50a$FPR))

## plot setup
x = 0:28
arw = 0.5 * (x1.r - x1.l)
arw.length = 0.03
arw.lwd = 0.8
sat = 0.3
TPR.col = grey.colors(1, 11.5/12)
FPR.col = grey.colors(1, 10/12)

## plot
par(mar = par()$mar+c(0,0,1,0))
plot(x, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="TPR", type="n", main="Training data")
legend(30.12, 1.04, xjust=1, yjust=0, xpd=TRUE, c("Our Method","X-Means-based","PCA-based"), density=c(NA,NA,20), angle=c(NA,NA,45), fill=c(TPR.col,NA,"black"))
axis(1, at = seq(2.5, 30, by=5), labels = c(0.05,0.1,0.2,0.3,0.4,0.5), tick = FALSE)
axis(1, at = seq(0, 30, by=5), labels = NA)
## rect
rect(x1.l, 0, x1.r, y1, col = TPR.col)
rect(x2.l, 0, x2.r, y5)
rect(x3.l, 0, x3.r, y7, col = "black", density=10, angle=45)
## arrows
arrows(x1.l+arw, y1-y1.sd, x1.l+arw, y1+y1.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows(x2.l+arw, y5-y5.sd, x2.l+arw, y5+y5.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows(x3.l+arw, y7-y7.sd, x3.l+arw, 1, length=arw.length, angle=90, code=1)  ## goes over 1


## dat2
y9 = c(mean(xmeans.results.dat2.05a$TPR),mean(xmeans.results.dat2.10a$TPR),mean(xmeans.results.dat2.20a$TPR),mean(xmeans.results.dat2.30a$TPR),mean(xmeans.results.dat2.40a$TPR),mean(xmeans.results.dat2.50a$TPR))
y10 = c(mean(xmeans.results.dat2.05a$FPR),mean(xmeans.results.dat2.10a$FPR),mean(xmeans.results.dat2.20a$FPR),mean(xmeans.results.dat2.30a$FPR),mean(xmeans.results.dat2.40a$FPR),mean(xmeans.results.dat2.50a$FPR))
y9.sd = c(sd(xmeans.results.dat2.05a$TPR),sd(xmeans.results.dat2.10a$TPR),sd(xmeans.results.dat2.20a$TPR),sd(xmeans.results.dat2.30a$TPR),sd(xmeans.results.dat2.40a$TPR),sd(xmeans.results.dat2.50a$TPR))
y10.sd = c(sd(xmeans.results.dat2.05a$FPR),sd(xmeans.results.dat2.10a$FPR),sd(xmeans.results.dat2.20a$FPR),sd(xmeans.results.dat2.30a$FPR),sd(xmeans.results.dat2.40a$FPR),sd(xmeans.results.dat2.50a$FPR))
## pca results
y11 = c(mean(pca.results.dat2.05a$TPR),mean(pca.results.dat2.10a$TPR),mean(pca.results.dat2.20a$TPR),mean(pca.results.dat2.30a$TPR),mean(pca.results.dat2.40a$TPR),mean(pca.results.dat2.50a$TPR))
y12 = c(mean(pca.results.dat2.05a$FPR),mean(pca.results.dat2.10a$FPR),mean(pca.results.dat2.20a$FPR),mean(pca.results.dat2.30a$FPR),mean(pca.results.dat2.40a$FPR),mean(pca.results.dat2.50a$FPR))
y11.sd = c(sd(pca.results.dat2.05a$TPR),sd(pca.results.dat2.10a$TPR),sd(pca.results.dat2.20a$TPR),sd(pca.results.dat2.30a$TPR),sd(pca.results.dat2.40a$TPR),sd(pca.results.dat2.50a$TPR))
y12.sd = c(sd(pca.results.dat2.05a$FPR),sd(pca.results.dat2.10a$FPR),sd(pca.results.dat2.20a$FPR),sd(pca.results.dat2.30a$FPR),sd(pca.results.dat2.40a$FPR),sd(pca.results.dat2.50a$FPR))

## plot
plot(x, ylim=c(0,1), xlab = "Intrusion Freq.", xaxt="n", ylab="TPR", type="n", main="Test data")
legend(30.12, 1.04, xjust=1, yjust=0, xpd=TRUE, c("Our Method","X-Means-based","PCA-based"), density=c(NA,NA,20), angle=c(NA,NA,45), fill=c(TPR.col,NA,"black"))
axis(1, at = seq(2.5, 30, by=5), labels = c(0.05,0.1,0.2,0.3,0.4,0.5), tick = FALSE)
axis(1, at = seq(0, 30, by=5), labels = NA)
## rect
rect(x1.l, 0, x1.r, y3, col = TPR.col)
rect(x2.l, 0, x2.r, y9)
rect(x3.l, 0, x3.r, y11, col = "black", density=10, angle=45)
## arrows
arrows(x1.l+arw, y3-y3.sd, x1.l+arw, y3+y3.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows(x2.l+arw, y9-y9.sd, x2.l+arw, y9+y9.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows((x3.l+arw)[-c(5:6)], (y11-y11.sd)[-c(5:6)], (x3.l+arw)[-c(5:6)], (y11+y11.sd)[-c(5:6)], length=arw.length, angle=90, code=3, lwd=arw.lwd)  ## goes over 1
arrows((x3.l+arw)[c(5:6)], (y11-y11.sd)[c(5:6)], (x3.l+arw)[c(5:6)], 1, length=arw.length, angle=90, code=1, lwd=arw.lwd)
@

<<compareFPR, dependson="compareTPR", echo=FALSE, out.width="0.65\\linewidth", fig.show='hold', fig.align='center', fig.cap="Comparison of FPRs across different methods">>=
par(mar = par()$mar+c(0,0,1,0))
plot(x, ylim=c(0,0.5), xlab = "Intrusion Freq.", xaxt="n", ylab="FPR", type="n", main="Training data")
# legend(30.12, 0.52, xjust=1, yjust=0, xpd=TRUE, c("Our Method","X-Means-based","PCA-based"), density=c(NA,NA,20), angle=c(NA,NA,45), fill=c(TPR.col,NA,"black"))
legend("topright", xjust=1, yjust=0, xpd=TRUE, c("Our Method","X-Means-based","PCA-based"), density=c(NA,NA,20), angle=c(NA,NA,45), fill=c(TPR.col,NA,"black"))
axis(1, at = seq(2.5, 30, by=5), labels = c(0.05,0.1,0.2,0.3,0.4,0.5), tick = FALSE)
axis(1, at = seq(0, 30, by=5), labels = NA)
## rect
rect(x1.l, 0, x1.r, y2, col = TPR.col)
rect(x2.l, 0, x2.r, y6)
rect(x3.l, 0, x3.r, y8, col = "black", density=10, angle=45)
## arrows
arrows(x1.l+arw, y2-y2.sd, x1.l+arw, y2+y2.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows(x2.l+arw, y6-y6.sd, x2.l+arw, y6+y6.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows((x3.l+arw)[-c(5,6)], (y8-y8.sd)[-c(5,6)], (x3.l+arw)[-c(5,6)], (y8+y8.sd)[-c(5,6)], length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows((x3.l+arw)[c(5,6)], 0, (x3.l+arw)[c(5,6)], (y8+y8.sd)[c(5,6)], length=arw.length, angle=90, code=2, lwd=arw.lwd)  ## goes below 0

plot(x, ylim=c(0,0.5), xlab = "Intrusion Freq.", xaxt="n", ylab="FPR", type="n", main="Test data")
legend("topright", xjust=1, yjust=0, xpd=TRUE, c("Our Method","X-Means-based","PCA-based"), density=c(NA,NA,20), angle=c(NA,NA,45), fill=c(TPR.col,NA,"black"))
axis(1, at = seq(2.5, 30, by=5), labels = c(0.05,0.1,0.2,0.3,0.4,0.5), tick = FALSE)
axis(1, at = seq(0, 30, by=5), labels = NA)
## rect
rect(x1.l, 0, x1.r, y4, col = TPR.col)
rect(x2.l, 0, x2.r, y10)
rect(x3.l, 0, x3.r, y12, col = "black", density=10, angle=45)
## arrows
arrows(x1.l+arw, y4-y4.sd, x1.l+arw, y4+y4.sd, length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows((x2.l+arw)[-c(2:5)], (y10-y10.sd)[-c(2:5)], (x2.l+arw)[-c(2:5)], (y10+y10.sd)[-c(2:5)], length=arw.length, angle=90, code=3, lwd=arw.lwd)
arrows((x2.l+arw)[2], (y10-y10.sd)[2], (x2.l+arw)[2], 0.5, length=arw.length, angle=90, code=1, lwd=arw.lwd)
arrows((x2.l+arw)[3:5], 0, (x2.l+arw)[3:5], (y10+y10.sd)[3:5], length=arw.length, angle=90, code=2, lwd=arw.lwd)
arrows(x3.l+arw, 0, x3.l+arw, y12+y12.sd, length=arw.length, angle=90, code=2, lwd=arw.lwd)  ## goes below 0
@



For the \emph{training} dataset, our method appears to be more precise than the other methods as indicated by the small variation. This is a promising piece of information to claim that our method functions exactly as designed and is very robust as the performance is reliable and independent of any discrepancies between the random samples.  The accuracy of our method is also reasonably comparable to the others, especially as it manages to retain the FPRs at a relatively low level. One exception is when the intrusion frequency reaches at 0.5, and we have already established that such frequency of intrusion instances may be too high for any anomaly detection schemes. The simple $X$-means-based method produced unexpectedly high TPRs but their respective FPRs are generally too high to be reliable. A steady decrease in the FPRs towards a higher intrusion frequency can be observed to suggest that the $X$-means-based method may be a more reliable alternative for balanced data. However one shortcoming of the $X$-means algorithm that it favours balanced data must be carefully noted here. In any case, our method outperforms the $X$-means-based method at the lower intrusion frequencies. The PCA-based approach is too imprecise to be a reliable method, even though \emph{most} of the runs produced reasonably high TPRs, on average, to bring up the overall detection rate. Because the PCA-based approach is just an extended version of the $X$-means approach that implements PCA, we can deduce that the PCA part solely explains the poor performance. The ineffectiveness of PCA means that the information contained in the data cannot be effectively summarised in lower dimensions and the dimensionality reduction using linear transformation is inefficient for the data. Our conclusion from this finding is that the shape of the data cloud is too spherical to find a suitable subspace that explains a sufficient amount of the information contained in the data as an acceptable alternative representation in lower dimensions. The \emph{test} dataset presents similar patterns in accordance with the previously examined findings.

To conclude, our method is the most reliable out of all three approaches as long as intrusion frequencies are not overly high, that is, most definitely, not as high as 0.5 but somewhere less than 0.4. The expected outcome of our method can be dependent on the type of data, in terms of densities, but the good side is that resulting FPs will be kept at a moderately low level. The promise of low false alarm rate is valuable as users will be further assured that the detected intrusion instances are that much more likely to be what they wish to catch.



%' A range of different values for the input parameter of the LOF algorithm, $k$, is tested. Assuming there is not enough domain knowledge and the most appropriate value for $k$ is not known, we have specified the range of $k$ to be tested as percentage, relative to the size of a test dataset, at 10, 20, 30 and 40 percents of the test dataset size. We refer to the percentages of $k$ as $\%k$ (percentage $k$).
%' 
%' The rest of this section is divided into four, each of which presents the density plots of LOF scores at different $\%k$ and the means and standard deviations of detection and false positive rates from one hundred repeated experiments for fixed percentage of intrusions.
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 10\% intrusions}
%' The test dataset used in this section is composed of 10\% intrusion and 90\% non-intrusion instances. Figure \ref{fig:attack10} represents the density plots at the four different percentages of $k$ at fixed 10\% intrusions. The means of thresholds, detection rates and false positive rates are tabulated, along with thier standard deviations in brackets, in Table \ref{evaluation:tab1}.
%' 
%' <<attack10, cache=TRUE, echo = FALSE, fig.align='center', fig.cap="Estimated density plots of LOF scores using KDE at different $\\%k$", out.width="0.45\\linewidth", fig.show='hold', fig.pos="h">>=
%' require(scales)
%' ## 10% attack, 10 %k
%' scores1 = as.matrix(read.csv("phase1_scores-10a-10k.csv", header = FALSE))
%' plot(density(scores1[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 10 %k")
%' invisible(lapply(2:nrow(scores1), function(i) lines(density(scores1[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 10% attack, 20 %k
%' scores2 = as.matrix(read.csv("phase1_scores-10a-20k.csv", header = FALSE))
%' plot(density(scores2[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 20 %k")
%' invisible(lapply(2:nrow(scores2), function(i) lines(density(scores2[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 10% attack, 30 %k
%' scores3 = as.matrix(read.csv("phase1_scores-10a-30k.csv", header = FALSE))
%' plot(density(scores3[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 30 %k")
%' invisible(lapply(2:nrow(scores3), function(i) lines(density(scores3[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 10% attack, 430 %k
%' scores4 = as.matrix(read.csv("phase1_scores-10a-40k.csv", header = FALSE))
%' plot(density(scores4[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 40 %k")
%' invisible(lapply(2:nrow(scores4), function(i) lines(density(scores4[i, ]), col = alpha("black", 0.1))))
%' @
%' 
%' <<phase1-data, cache=TRUE, echo = FALSE, results='asis'>>=
%' load(file = "phase1_evaluation.RData")
%' @
%' 
%' <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' options(digits = 3)
%' library(xtable)
%' tab = data.frame(rbind(c(paste0(round(a10_10k$threshold[1], 3), " (", round(a10_10k$threshold[2], 2), ")"),
%'                          paste0(round(a10_20k$threshold[1], 3), " (", round(a10_20k$threshold[2], 2), ")"),
%'                          paste0(round(a10_30k$threshold[1], 3), " (", round(a10_30k$threshold[2], 2), ")"),
%'                          paste0(round(a10_40k$threshold[1], 3), " (", round(a10_40k$threshold[2], 2), ")")),
%'                        c(paste0(round(a10_10k$detection.rate[1], 3), " (", round(a10_10k$detection.rate[2], 2), ")"),
%'                          paste0(round(a10_20k$detection.rate[1], 3), " (", round(a10_20k$detection.rate[2], 2), ")"),
%'                          paste0(round(a10_30k$detection.rate[1], 3), " (", round(a10_30k$detection.rate[2], 2), ")"),
%'                          paste0(round(a10_40k$detection.rate[1], 3), " (", round(a10_40k$detection.rate[2], 2), ")")),
%'                        c(paste0(round(a10_10k$false.alarm[1], 3), " (", round(a10_10k$false.alarm[2], 2), ")"),
%'                          paste0(round(a10_20k$false.alarm[1], 3), " (", round(a10_20k$false.alarm[2], 2), ")"),
%'                          paste0(round(a10_30k$false.alarm[1], 3), " (", round(a10_30k$false.alarm[2], 2), ")"),
%'                          paste0(round(a10_40k$false.alarm[1], 3), " (", round(a10_40k$false.alarm[2], 2), ")"))))
%' 
%' colnames(tab) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' rownames(tab) = c("threshold", "detection rate", "false alarm rate")
%' xtab        = xtable(tab, caption = "Threshold, detection rate and false alarm rate at 10\\% intrusions", label = "evaluation:tab1", digits = 3)
%' align(xtab) = "r|llll"
%' print(xtab, booktabs = TRUE, caption.placement = "top")
%' @
%' 
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 20\% intrusions}
%' The test dataset used in this section is composed of 20\% intrusion and 80\% non-intrusion instances. Figure \ref{fig:attack20} represents the density plots at the different percentages of $k$  at fixed 20\% intrusions. The means of thresholds, detection rates and false positive rates are tabulated, along with thier standard deviations in brackets, in Table \ref{evaluation:tab2}.
%' 
%' <<attack20, cache=TRUE, echo = FALSE, fig.align='center', fig.cap="Estimated density plots of LOF scores using KDE at different $\\%k$", out.width="0.45\\linewidth", fig.show='hold', fig.pos="h">>=
%' require(scales)
%' ## 20% attack, 10 %k
%' scores1 = as.matrix(read.csv("phase1_scores-20a-10k.csv", header = FALSE))
%' plot(density(scores1[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 10 %k")
%' invisible(lapply(2:nrow(scores1), function(i) lines(density(scores1[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 20% attack, 20 %k
%' scores2 = as.matrix(read.csv("phase1_scores-20a-20k.csv", header = FALSE))
%' plot(density(scores2[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 20 %k")
%' invisible(lapply(2:nrow(scores2), function(i) lines(density(scores2[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 20% attack, 30 %k
%' scores3 = as.matrix(read.csv("phase1_scores-20a-30k.csv", header = FALSE))
%' plot(density(scores3[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 30 %k")
%' invisible(lapply(2:nrow(scores3), function(i) lines(density(scores3[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 20% attack, 430 %k
%' scores4 = as.matrix(read.csv("phase1_scores-20a-40k.csv", header = FALSE))
%' plot(density(scores4[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 40 %k")
%' invisible(lapply(2:nrow(scores4), function(i) lines(density(scores4[i, ]), col = alpha("black", 0.1))))
%' @
%' 
%' <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' options(digits = 3)
%' library(xtable)
%' tab = data.frame(rbind(c(paste0(round(a20_10k$threshold[1], 3), " (", round(a20_10k$threshold[2], 2), ")"),
%'                          paste0(round(a20_20k$threshold[1], 3), " (", round(a20_20k$threshold[2], 2), ")"),
%'                          paste0(round(a20_30k$threshold[1], 3), " (", round(a20_30k$threshold[2], 2), ")"),
%'                          paste0(round(a20_40k$threshold[1], 3), " (", round(a20_40k$threshold[2], 2), ")")),
%'                        c(paste0(round(a20_10k$detection.rate[1], 3), " (", round(a20_10k$detection.rate[2], 2), ")"),
%'                          paste0(round(a20_20k$detection.rate[1], 3), " (", round(a20_20k$detection.rate[2], 2), ")"),
%'                          paste0(round(a20_30k$detection.rate[1], 3), " (", round(a20_30k$detection.rate[2], 2), ")"),
%'                          paste0(round(a20_40k$detection.rate[1], 3), " (", round(a20_40k$detection.rate[2], 2), ")")),
%'                        c(paste0(round(a20_10k$false.alarm[1], 3), " (", round(a20_10k$false.alarm[2], 2), ")"),
%'                          paste0(round(a20_20k$false.alarm[1], 3), " (", round(a20_20k$false.alarm[2], 2), ")"),
%'                          paste0(round(a20_30k$false.alarm[1], 3), " (", round(a20_30k$false.alarm[2], 2), ")"),
%'                          paste0(round(a20_40k$false.alarm[1], 3), " (", round(a20_40k$false.alarm[2], 2), ")"))))
%' 
%' colnames(tab) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' rownames(tab) = c("threshold", "detection rate", "false alarm rate")
%' xtab        = xtable(tab, caption = "Threshold, detection rate and false alarm rate at 20\\% intrusions", label = "evaluation:tab2", digits = 3)
%' align(xtab) = "r|llll"
%' print(xtab, booktabs = TRUE, caption.placement = "top")
%' @
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 30\% intrusions}
%' The test dataset used in this section is composed of 30\% intrusion and 70\% non-intrusion instances. Figure \ref{fig:attack30} represents the density plots at the different percentages of $k$  at fixed 30\% intrusions. The means of thresholds, detection rates and false positive rates are tabulated, along with thier standard deviations in brackets, in Table \ref{evaluation:tab3}.
%' 
%' <<attack30, cache=TRUE, echo = FALSE, fig.align='center', fig.cap="Estimated density plots of LOF scores using KDE at different $\\%k$", out.width="0.45\\linewidth", fig.show='hold', fig.pos="h">>=
%' require(scales)
%' ## 30% attack, 10 %k
%' scores1 = as.matrix(read.csv("phase1_scores-30a-10k.csv", header = FALSE))
%' plot(density(scores1[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 10 %k")
%' invisible(lapply(2:nrow(scores1), function(i) lines(density(scores1[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 30% attack, 20 %k
%' scores2 = as.matrix(read.csv("phase1_scores-30a-20k.csv", header = FALSE))
%' plot(density(scores2[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 20 %k")
%' invisible(lapply(2:nrow(scores2), function(i) lines(density(scores2[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 30% attack, 30 %k
%' scores3 = as.matrix(read.csv("phase1_scores-30a-30k.csv", header = FALSE))
%' plot(density(scores3[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 30 %k")
%' invisible(lapply(2:nrow(scores3), function(i) lines(density(scores3[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 30% attack, 430 %k
%' scores4 = as.matrix(read.csv("phase1_scores-30a-40k.csv", header = FALSE))
%' plot(density(scores4[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 40 %k")
%' invisible(lapply(2:nrow(scores4), function(i) lines(density(scores4[i, ]), col = alpha("black", 0.1))))
%' @
%' 
%' <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' options(digits = 3)
%' library(xtable)
%' tab = data.frame(rbind(c(paste0(round(a30_10k$threshold[1], 3), " (", round(a30_10k$threshold[2], 2), ")"),
%'                          paste0(round(a30_20k$threshold[1], 3), " (", round(a30_20k$threshold[2], 2), ")"),
%'                          paste0(round(a30_30k$threshold[1], 3), " (", round(a30_30k$threshold[2], 2), ")"),
%'                          paste0(round(a30_40k$threshold[1], 3), " (", round(a30_40k$threshold[2], 2), ")")),
%'                        c(paste0(round(a30_10k$detection.rate[1], 3), " (", round(a30_10k$detection.rate[2], 2), ")"),
%'                          paste0(round(a30_20k$detection.rate[1], 3), " (", round(a30_20k$detection.rate[2], 2), ")"),
%'                          paste0(round(a30_30k$detection.rate[1], 3), " (", round(a30_30k$detection.rate[2], 2), ")"),
%'                          paste0(round(a30_40k$detection.rate[1], 3), " (", round(a30_40k$detection.rate[2], 2), ")")),
%'                        c(paste0(round(a30_10k$false.alarm[1], 3), " (", round(a30_10k$false.alarm[2], 2), ")"),
%'                          paste0(round(a30_20k$false.alarm[1], 3), " (", round(a30_20k$false.alarm[2], 2), ")"),
%'                          paste0(round(a30_30k$false.alarm[1], 3), " (", round(a30_30k$false.alarm[2], 2), ")"),
%'                          paste0(round(a30_40k$false.alarm[1], 3), " (", round(a30_40k$false.alarm[2], 2), ")"))))
%' 
%' colnames(tab) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' rownames(tab) = c("threshold", "detection rate", "false alarm rate")
%' xtab        = xtable(tab, caption = "Threshold, detection rate and false alarm rate at 30\\% intrusions", label = "evaluation:tab3", digits = 3)
%' align(xtab) = "r|llll"
%' print(xtab, booktabs = TRUE, caption.placement = "top")
%' @
%' 
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 40\% intrusions}
%' The test dataset used in this section is composed of 40\% intrusion and 60\% non-intrusion instances. Figure \ref{fig:attack40} represents the density plots at the different percentages of $k$ at fixed 40\% intrusions. The means of thresholds, detection rates and false positive rates are tabulated, along with thier standard deviations in brackets, in Table \ref{evaluation:tab4}.
%' 
%' <<attack40, cache=TRUE, echo = FALSE, fig.align='center', fig.cap="Estimated density plots of LOF scores using KDE at different $\\%k$", out.width="0.45\\linewidth", fig.show='hold', fig.pos="h">>=
%' require(scales)
%' ## 40% attack, 10 %k
%' scores1 = as.matrix(read.csv("phase1_scores-40a-10k.csv", header = FALSE))
%' plot(density(scores1[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 10 %k")
%' invisible(lapply(2:nrow(scores1), function(i) lines(density(scores1[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 40% attack, 20 %k
%' scores2 = as.matrix(read.csv("phase1_scores-40a-20k.csv", header = FALSE))
%' plot(density(scores2[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 20 %k")
%' invisible(lapply(2:nrow(scores2), function(i) lines(density(scores2[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 40% attack, 30 %k
%' scores3 = as.matrix(read.csv("phase1_scores-40a-30k.csv", header = FALSE))
%' plot(density(scores3[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 30 %k")
%' invisible(lapply(2:nrow(scores3), function(i) lines(density(scores3[i, ]), col = alpha("black", 0.1))))
%' 
%' ## 40% attack, 430 %k
%' scores4 = as.matrix(read.csv("phase1_scores-40a-40k.csv", header = FALSE))
%' plot(density(scores4[1, ]), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density of LOF scores at 10% intrusion and 40 %k")
%' invisible(lapply(2:nrow(scores4), function(i) lines(density(scores4[i, ]), col = alpha("black", 0.1))))
%' @
%' 
%' <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' options(digits = 3)
%' library(xtable)
%' tab = data.frame(rbind(c(paste0(round(a40_10k$threshold[1], 3), " (", round(a40_10k$threshold[2], 2), ")"),
%'                          paste0(round(a40_20k$threshold[1], 3), " (", round(a40_20k$threshold[2], 2), ")"),
%'                          paste0(round(a40_30k$threshold[1], 3), " (", round(a40_30k$threshold[2], 2), ")"),
%'                          paste0(round(a40_40k$threshold[1], 3), " (", round(a40_40k$threshold[2], 2), ")")),
%'                        c(paste0(round(a40_10k$detection.rate[1], 3), " (", round(a40_10k$detection.rate[2], 2), ")"),
%'                          paste0(round(a40_20k$detection.rate[1], 3), " (", round(a40_20k$detection.rate[2], 2), ")"),
%'                          paste0(round(a40_30k$detection.rate[1], 3), " (", round(a40_30k$detection.rate[2], 2), ")"),
%'                          paste0(round(a40_40k$detection.rate[1], 3), " (", round(a40_40k$detection.rate[2], 2), ")")),
%'                        c(paste0(round(a40_10k$false.alarm[1], 3), " (", round(a40_10k$false.alarm[2], 2), ")"),
%'                          paste0(round(a40_20k$false.alarm[1], 3), " (", round(a40_20k$false.alarm[2], 2), ")"),
%'                          paste0(round(a40_30k$false.alarm[1], 3), " (", round(a40_30k$false.alarm[2], 2), ")"),
%'                          paste0(round(a40_40k$false.alarm[1], 3), " (", round(a40_40k$false.alarm[2], 2), ")"))))
%' 
%' colnames(tab) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' rownames(tab) = c("threshold", "detection rate", "false alarm rate")
%' xtab        = xtable(tab, caption = "Threshold, detection rate and false alarm rate at 40\\% intrusions", label = "evaluation:tab4", digits = 3)
%' align(xtab) = "r|llll"
%' print(xtab, booktabs = TRUE, caption.placement = "top")
%' @
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' % %The choice of the input parameter, $k$, heavily depends on the application domain and prior knowledge about data. Without knowing the actual sizes, or the local densities, of possible clusters in the data, $k$ can only be set arbitrarily. According to the authors of the LOF algorithm, $k$ must be large enough to avoid any statistical fluctuations in the computed scores. Also $k$ must be less than the size of the entire dataset to avoid including all the instances as the neighbourhood. What can be deducted from the examples given in the literature is that the choice of $k$ is related to the size of data that the algorithm is applied to. Therefore we have decided to set $k$ to be proportional to the size of a testset such that $k$ is appropriately high for a large dataset and vice versa.
%' % 
%' % We have tested the performance of LOF on four different values of $k$, namely 500, 1,000, 1,500 and 2,000, that is 10\%, 20\%, 30\% and 40\% of the testset size of 5,000. The amount of intrusion instances, as denoted by \emph{p.attack}, in the testset is also varied to assess whether the adequacy of $k$ depends on the number of intrusion instances in the testset. Tables \ref{tab:phase1-detection}, \ref{tab:phase1-false} and \ref{tab:phase1-threshold} present the means of the detection and false alarm rates and thresholds from the 100 repeated experiments. The corresponding standard deviations are represented in brackets.
%' % 
%' % <<phase1-data, cache=TRUE, echo = FALSE, results='asis'>>=
%' % load(file = "phase1_evaluation.RData")
%' % @
%' % 
%' % <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' % library(xtable)
%' % detection.rate = data.frame(rbind(
%' %     c(paste0(round(a10_10k$detection.rate[1], 3), " (", round(a10_10k$detection.rate[2], 2), ")"),
%' %       paste0(round(a10_20k$detection.rate[1], 3), " (", round(a10_20k$detection.rate[2], 2), ")"),
%' %       paste0(round(a10_30k$detection.rate[1], 3), " (", round(a10_30k$detection.rate[2], 2), ")"),
%' %       paste0(round(a10_40k$detection.rate[1], 3), " (", round(a10_40k$detection.rate[2], 2), ")")),
%' %     
%' %     c(paste0(round(a20_10k$detection.rate[1], 3), " (", round(a20_10k$detection.rate[2], 2), ")"),
%' %       paste0(round(a20_20k$detection.rate[1], 3), " (", round(a20_20k$detection.rate[2], 2), ")"),
%' %       paste0(round(a20_30k$detection.rate[1], 3), " (", round(a20_30k$detection.rate[2], 2), ")"),
%' %       paste0(round(a20_40k$detection.rate[1], 3), " (", round(a20_40k$detection.rate[2], 2), ")")),
%' %     
%' %     c(paste0(round(a30_10k$detection.rate[1], 3), " (", round(a30_10k$detection.rate[2], 2), ")"),
%' %       paste0(round(a30_20k$detection.rate[1], 3), " (", round(a30_20k$detection.rate[2], 2), ")"),
%' %       paste0(round(a30_30k$detection.rate[1], 3), " (", round(a30_30k$detection.rate[2], 2), ")"),
%' %       paste0(round(a30_40k$detection.rate[1], 3), " (", round(a30_40k$detection.rate[2], 2), ")")),
%' %     
%' %     c(paste0(round(a40_10k$detection.rate[1], 3), " (", round(a40_10k$detection.rate[2], 2), ")"),
%' %       paste0(round(a40_20k$detection.rate[1], 3), " (", round(a40_20k$detection.rate[2], 2), ")"),
%' %       paste0(round(a40_30k$detection.rate[1], 3), " (", round(a40_30k$detection.rate[2], 2), ")"),
%' %       paste0(round(a40_40k$detection.rate[1], 3), " (", round(a40_40k$detection.rate[2], 2), ")"))
%' % ))
%' % 
%' % colnames(detection.rate) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' % rownames(detection.rate) = c("p.attack = 10%", "p.attack = 20%", "p.attack = 30%", "p.attack = 40%")
%' % 
%' % xtab        = xtable(detection.rate, caption = "Detection rate", label = "tab:phase1-detection")
%' % align(xtab) = "r|llll"
%' % print(xtab, booktabs = TRUE)
%' % @
%' % 
%' % <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' % library(xtable)
%' % false.alarm = data.frame(rbind(
%' %     c(paste0(round(a10_10k$false.alarm[1], 3), " (", round(a10_10k$false.alarm[2], 2), ")"),
%' %       paste0(round(a10_20k$false.alarm[1], 3), " (", round(a10_20k$false.alarm[2], 2), ")"),
%' %       paste0(round(a10_30k$false.alarm[1], 3), " (", round(a10_30k$false.alarm[2], 2), ")"),
%' %       paste0(round(a10_40k$false.alarm[1], 3), " (", round(a10_40k$false.alarm[2], 2), ")")),
%' %     
%' %     c(paste0(round(a20_10k$false.alarm[1], 3), " (", round(a20_10k$false.alarm[2], 2), ")"),
%' %       paste0(round(a20_20k$false.alarm[1], 3), " (", round(a20_20k$false.alarm[2], 2), ")"),
%' %       paste0(round(a20_30k$false.alarm[1], 3), " (", round(a20_30k$false.alarm[2], 2), ")"),
%' %       paste0(round(a20_40k$false.alarm[1], 3), " (", round(a20_40k$false.alarm[2], 2), ")")),
%' %     
%' %     c(paste0(round(a30_10k$false.alarm[1], 3), " (", round(a30_10k$false.alarm[2], 2), ")"),
%' %       paste0(round(a30_20k$false.alarm[1], 3), " (", round(a30_20k$false.alarm[2], 2), ")"),
%' %       paste0(round(a30_30k$false.alarm[1], 3), " (", round(a30_30k$false.alarm[2], 2), ")"),
%' %       paste0(round(a30_40k$false.alarm[1], 3), " (", round(a30_40k$false.alarm[2], 2), ")")),
%' %     
%' %     c(paste0(round(a40_10k$false.alarm[1], 3), " (", round(a40_10k$false.alarm[2], 2), ")"),
%' %       paste0(round(a40_20k$false.alarm[1], 3), " (", round(a40_20k$false.alarm[2], 2), ")"),
%' %       paste0(round(a40_30k$false.alarm[1], 3), " (", round(a40_30k$false.alarm[2], 2), ")"),
%' %       paste0(round(a40_40k$false.alarm[1], 3), " (", round(a40_40k$false.alarm[2], 2), ")"))
%' % ))
%' % 
%' % colnames(false.alarm) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' % rownames(false.alarm) = c("p.attack = 10%", "p.attack = 20%", "p.attack = 30%", "p.attack = 40%")
%' % 
%' % xtab        = xtable(false.alarm, caption = "False alarm rate", label = "tab:phase1-false")
%' % align(xtab) = "r|llll"
%' % print(xtab, booktabs = TRUE)
%' % @
%' % 
%' % <<cache=TRUE, dependson="phase1-data", echo = FALSE, results='asis'>>=
%' % library(xtable)
%' % threshold = data.frame(rbind(
%' %     c(paste0(round(a10_10k$threshold[1], 3), " (", round(a10_10k$threshold[2], 2), ")"),
%' %       paste0(round(a10_20k$threshold[1], 3), " (", round(a10_20k$threshold[2], 2), ")"),
%' %       paste0(round(a10_30k$threshold[1], 3), " (", round(a10_30k$threshold[2], 2), ")"),
%' %       paste0(round(a10_40k$threshold[1], 3), " (", round(a10_40k$threshold[2], 2), ")")),
%' %     
%' %     c(paste0(round(a20_10k$threshold[1], 3), " (", round(a20_10k$threshold[2], 2), ")"),
%' %       paste0(round(a20_20k$threshold[1], 3), " (", round(a20_20k$threshold[2], 2), ")"),
%' %       paste0(round(a20_30k$threshold[1], 3), " (", round(a20_30k$threshold[2], 2), ")"),
%' %       paste0(round(a20_40k$threshold[1], 3), " (", round(a20_40k$threshold[2], 2), ")")),
%' %     
%' %     c(paste0(round(a30_10k$threshold[1], 3), " (", round(a30_10k$threshold[2], 2), ")"),
%' %       paste0(round(a30_20k$threshold[1], 3), " (", round(a30_20k$threshold[2], 2), ")"),
%' %       paste0(round(a30_30k$threshold[1], 3), " (", round(a30_30k$threshold[2], 2), ")"),
%' %       paste0(round(a30_40k$threshold[1], 3), " (", round(a30_40k$threshold[2], 2), ")")),
%' %     
%' %     c(paste0(round(a40_10k$threshold[1], 3), " (", round(a40_10k$threshold[2], 2), ")"),
%' %       paste0(round(a40_20k$threshold[1], 3), " (", round(a40_20k$threshold[2], 2), ")"),
%' %       paste0(round(a40_30k$threshold[1], 3), " (", round(a40_30k$threshold[2], 2), ")"),
%' %       paste0(round(a40_40k$threshold[1], 3), " (", round(a40_40k$threshold[2], 2), ")"))
%' % ))
%' % 
%' % colnames(threshold) = c("k = 10%", "k = 20%", "k = 30%", "k = 40%")
%' % rownames(threshold) = c("p.attack = 10%", "p.attack = 20%", "p.attack = 30%", "p.attack = 40%")
%' % 
%' % xtab        = xtable(threshold, caption = "Threshold", label = "tab:phase1-threshold")
%' % align(xtab) = "r|llll"
%' % print(xtab, booktabs = TRUE)
%' % @
%' % 
%' % From the tables, a few observations can be made:
%' % 
%' % \begin{itemize}
%' % 
%' % \item As $k$ increases, the detection and false positive rates as well as the thresholds are more consistent, as shown by the decreases in their respective standard deviations. This suggests that $k$ should be adequately large to produce more stable results.
%' % 
%' % \item As the number of intrusion instances, denoted by \emph{p.attack}, increases in the testset, the false alarm rates at $k = 10\%$ seems to increase. This mildly suggests that the LOF algorithm for this particular instance has performed well when the ratio between the numbers of intrusion and non-intrusion instances are further apart. However, the other rates vary widely between the different values of p.attack to support this claim.
%' % 
%' % \item The thresholds fluctuate widely for the different testsets. This is expected as our threshold is set to be adaptive to the density curve of the computed scores.
%' % 
%' % \item The performance seems to be at best for the case where p.attack = 10\% and $k = 40\%$, as indicated by the highest detection rate and the lowest false alarm rate. The density curves of the 100 sets of LOF scores can be seen in Figure \ref{fig:phase1-eval-density}.
%' % 
%' % \end{itemize}
%' % 
%' % 
%' % <<phase1-eval-density, echo=FALSE, fig.align='center', fig.cap='The density curves of the 100 sets of the LOF scores computed for the testset of 4,500 non-intrusion and 500 intrusion instances at k = 2,000.', out.width="0.6\\linewidth">>=
%' % a10_40k_scores = read.csv("phase1_scores-10a-40k.csv")
%' % a10_40k_scores = as.matrix(a10_40k_scores)
%' % 
%' % library(scales)
%' % plot(density(a10_40k_scores[1, ]), ylim = c(0, 4), col = alpha("black", 0.1), xlab = "LOF scores", main = "Density curves for 100 sets of LOF scores")
%' % for (i in 2:99) {
%' %     lines(density(a10_40k_scores[i, ]), col = alpha("black", 0.1))
%' % }
%' % @
%' % 
%' % %The input parameter, $k$, is set to be proportional to the size of the testset. We have tested the performance of LOF using $k$ values at 10\%, 20\%, 30\% and 40\% of the testset size, that is 500, 1,000, 1,500 and 2,000 respectively. The initial $k$ value of 500 may seem rather large but our intention is to avoid any statistical inconsistency by setting it too low, as mentioned by the authors of the LOF algorithm. Four different types of testsets are used, where the number of intrusion instances is varied between 2,000, 1,500, 1,000 and 500.
%' % 
%' % % As discussed by the authors of the LOF algorithm, a set of LOF scores can fluctuate non-monotonically depending on the choice of the input parameter, $k$. What they have found was that the variation, measured by the standard deviation, in the scores computed for a random uniform sample stabilises for $k$ greater than 10. From this, they suggested a heuristic that $k$ should be greater than 10 to avoid any statistical inconsistency in the computed LOF scores. On the other hand, if $k$ is set to be too large, non-outliers can end up being detected as outliers because the size of the neighbourhood increases to potentially include non-outliers from other clusters as $k$ increases. The authors' suggestion is to compute LOF scores over a range of $k$ and take the maximum of the scores to report on the most outlying scores.
%' % % 
%' % % The difficulty to proceed with their heuristic is the computation time and memory resource consumption, both of which depend on the size of $k$ range. If $k$ is set to be over a wide range, the LOF algorithm is going to occur that much more compared to over a narrow range.
%' % % 
%' % % Hence, the size 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' 
%' % uncomment the subsection below:
%' % -----------------
%' % SUB-SECTION: 3.2
%' % -----------------
%' \subsection{Phase 2}
%' \label{evaluation:sec3.2}
%' 
%' Since the aim of the DBSCAN clustering technique used in Phase 2 is to categorise instances belonging to the same attack type into each cluster, it is of the most interest to evaluate whether the members of each cluster belong to one particular type of attacks. Hence the externel criterion of clustering quality, \emph{purity}, is measured, that is given by
%' \[ purity = \frac{1}{N} \sum_{i=1}^{k} max_j |c_i \cap t_j |, \]
%' where $N$ is the number of objects, $k$ is the number of clusters, $c_i$ is a cluster in $C$ and $t_j$ is the class $j$. Because purity is an average measure aggregated over all clusters, the presence of relatively large clusters can result in an increased purity. To avoid the bias, clustering purity is computed at an individual cluster level for Phase 2 by
%' \[ purity_i = \frac{1}{n_i} max_j |c_i \cap t_j |, \]
%' for the $i$th cluster.
%' 
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 10\% intrusions}
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 20\% intrusions}
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 30\% intrusions}
%' 
%' %----------------------------------------------------------
%' \subsubsection{At 40\% intrusions}

% As an example, the most frequent class, its count, the size and the purity computed for the first iteration of the repeated experiment is presented in Table \ref{tab:phase2-first1}. The classical purity measure is $\frac{1}{1719}(349+957+261+28) = 0.9279$, which is slightly greater than our approach of $\frac{1}{4}(0.751+0.994+1+0.933) = 0.9195$. The standard deviation of our purity measure is about 0.1163, which gives us a measure of consistency between the individual purity.
% 
% <<phase2-data, echo=FALSE, cache=TRUE>>=
% load(file = "phase2_evaluation.RData")
% @
% 
% <<cache=TRUE, dependson="phase2-data", echo=FALSE, results='asis'>>=
% library(xtable)
% tab = rbind(phase2_results$max_class[1, ], c(349,957,261,28), phase2_results$size[1, ], round(phase2_results$purity[1, ], 3))
% rownames(tab) = c("$class", "$count", "$size", "$purity")
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first1")
% align(xtab) = "r|llll"
% print(xtab, booktabs = TRUE)
% @
% 
% Table \ref{tab:phase2-first2} presents the Euclidean distance to the centroid of the non-intrusion instances from the centroid of each cluster for the same iteration. Since the second cluster is the closest to the centroid of the non-intrusion instances, it is removed and the remaining three clusters will form the set of detected intrusions. The third cluster, which is purely of the non-intrusion instances, is counterintuitively distant from the non-intrusion centroid. It is more distant than the first cluster of the Smurf attacks and less distant than the fourth cluster of the Neptune attacks. It could be possible that the non-intrusion instances in this cluster are rather behaving as if they are anomalies. A simple example would be that a user who has forgotten his or her password could show similar behaviour as a dictionary attack by multiple attempts at logging into a system.
% 
% <<cache=TRUE, dependson="phase2-data", echo=FALSE, results='asis'>>=
% library(xtable)
% tab = matrix(round(phase2_results$distances[1, ], 3), nrow = 1)
% rownames(tab) = "$distance"
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first2")
% align(xtab) = "r|llll"
% print(xtab, booktabs = TRUE)
% @
% 
% Finally, the mean and the standard deviation of 100 sets of purity measures are presented in Table \ref{tab:phase2-first3}.
% 
% <<cache=TRUE, dependson="phase2-data", echo=FALSE, results='asis'>>=
% library(xtable)
% tab = matrix(c(round(mean(phase2_results$purity), 4), round(sd(phase2_results$purity), 4)), nrow = 1)
% rownames(tab) = "$overall.purity"
% colnames(tab) = c("mean", "sd")
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first3", digits = 4)
% align(xtab) = "r|ll"
% print(xtab, booktabs = TRUE)
% @

% 
% 
% %----------------------------------------------------------
% \section{System Evaluation}
% \label{sec:evaluation-system}
% 
% Table \ref{tab:phase2-first3} is the result of the overall system evaluation. There are 61 occasions where the clusters of non-intrusion instances are correctly identified by being the closest to the non-intrusion centroid to result in the detection rates that are all above 96\%. For the remaining 39 occasions, the clusters are incorrectly identified to result in the detection rates below 81\%. For these 39 cases, the clusters of intrusions are identified as being the closest to the non-intrusion centroid while the actual clusters of non-intrusion instances that we want to identify are further away. The reason is because there are multiple clusters of non-intrusion instances present in the dataset. These non-intrusion instances behave abnormally relative to the majority of the other non-intrusion instances to the extent that they are detected as possible anomalies.
% 
% <<cache=TRUE, dependson="phase2-data", echo=FALSE, results='asis'>>=
% library(xtable)
% tab = matrix(c(round(mean(phase2_results$detection.rate), 4), round(sd(phase2_results$detection.rate), 4),
%                round(mean(phase2_results$false.alarm.rate), 4), round(sd(phase2_results$false.alarm.rate), 4)), nrow = 2, byrow = TRUE)
% rownames(tab) = c("detection rate", "false alarm rate")
% colnames(tab) = c("mean", "sd")
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first3", digits = 4)
% align(xtab) = "r|ll"
% print(xtab, booktabs = TRUE)
% @
% 
% The same experiment is conducted on the test dataset of KDD'99 where there are 14 unique attack types. The result is presented in Table \ref{tab:phase2-first4}. The similar situation mentioned previously is also present for the results using the test dataset.
% 
% <<echo=FALSE, results='asis'>>=
% library(xtable)
% load(file = "system_testset.RData")
% tab = t(test_testdata$table)
% tab[1, 1] = as.numeric(tab[1, 1])
% tab[2, 1] = as.numeric(tab[2, 1])
% tab[1, 2] = as.numeric(gsub("[(]|[)]", "", tab[1, 2]))
% tab[2, 2] = as.numeric(gsub("[(]|[)]", "", tab[2, 2]))
% xtab = xtable(tab, caption = "First iteration of the repeated experiments", label = "tab:phase2-first3", digits = 4)
% align(xtab) = "r|ll"
% print(xtab, booktabs = TRUE)
% @
% 
% % \subsection{Experimental Results}
% % 
% % <<load_results, echo=FALSE>>=
% % load(file = "evaluation.results.RData")
% % @
% % 
% % 
% % \subsubsection{Phase 1}
% % 
% % \begin{itemize}
% % \item Detection rate from LOF
% % \item False alarm rate from LOF
% % \item Computation time
% % \end{itemize}
% % 
% % \subsubsection{Phase 2}
% % 
% % \begin{itemize}
% % \item Average purity of clusters and s.d.
% % \item Rate of correctly identifying "normal" clusters
% % \item Six different distance metrics
% % \item Computation time
% % \end{itemize}
% % 
% % \subsubsection{Overall}
% % 
% % <<echo=FALSE, results=tex>>=
% % library(xtable)
% % xtable(k30.euc$table,  caption = "Euclidean")
% % xtable(k30.weuc$table, caption = "Weighted Euclidean")
% % xtable(k30.man$table,  caption = "Manhattan")
% % xtable(k30.che$table,  caption = "Chebyshev")
% % xtable(k30.min$table,  caption = "Minkoski")
% % xtable(k30.mah$table,  caption = "Mahalanobis")
% % @





% Comment below out when building the whole doc:
\bibliographystyle{ieeetr}
\bibliography{../Main/bibliography}
